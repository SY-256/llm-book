{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 第8章 文埋め込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文埋め込みモデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 教師なしSimCSEの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# 乱数シードの設定\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset jawiki-sentences (/root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "unsup_train_dataset = load_dataset(\n",
    "    \"llm-book/jawiki-sentences\", split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 24387500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットの形式と事例数を確認\n",
    "print(unsup_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 アンパサンド(&, 英語: ampersand)は、並立助詞「...と...」を意味する記号である。\n",
      "1 ラテン語で「...と...」を表す接続詞 \"et\" の合字を起源とする。\n",
      "2 現代のフォントでも、Trebuchet MS など一部のフォントでは、\"et\" の合字であることが容易にわかる字形を使用している。\n",
      "3 英語で教育を行う学校でアルファベットを復唱する場合、その文字自体が単語となる文字(\"A\", \"I\", かつては \"O\" も)については、伝統的にラテン語の per se(それ自体)を用いて \"A per se A\" のように唱えられていた。\n",
      "4 また、アルファベットの最後に、27番目の文字のように \"&\" を加えることも広く行われていた。\n",
      "5 \"&\" はラテン語で et と読まれていたが、後に英語で and と読まれるようになった。\n",
      "6 結果として、アルファベットの復唱の最後は \"X, Y, Z, and per se and\" という形になった。\n",
      "7 この最後のフレーズが繰り返されるうちに \"ampersand\" と訛っていき、この言葉は1837年までには英語の一般的な語法となった。\n",
      "8 アンドレ=マリ・アンペールがこの記号を自身の著作で使い、これが広く読まれたため、この記号が \"Ampère's and\" と呼ばれるようになったという誤った語源俗説がある。\n",
      "9 アンパサンドの起源は1世紀の古ローマ筆記体にまで遡ることができる。\n",
      "10 古ローマ筆記体では、E と T はしばしば合字として繋げて書かれていた(左図「アンパサンドの変遷」の字形1)。それに続く、流麗さを増した新ローマ筆記体では、様々な合字が極めて頻繁に使われるようになった。\n",
      "11 字形2と3は4世紀中頃における et の合字の例である。\n",
      "12 その後、9世紀のカロリング小文字体に至るラテン文字の変遷の過程で、合字の使用は一般には廃れていった。\n",
      "13 しかし、et の合字は使われ続け、次第に元の文字がわかりにくい字形に変化していった(字形4から6)。\n",
      "14 現代のイタリック体のアンパサンドは、ルネサンス期に発展した筆記体での et の合字に遡る。\n",
      "15 1455年のヨーロッパにおける印刷技術の発明以降、印刷業者はイタリック体とローマ筆記体のアンパサンドの両方を多用するようになった。\n",
      "16 アンパサンドのルーツはローマ時代に遡るため、ラテンアルファベットを使用する多くの言語でアンパサンドが使用されるようになった。\n",
      "17 アンパサンドはしばしばラテンアルファベットの最後の文字とされることがあった。\n",
      "18 例えば1011年のByrhtferthの文字表がその例である。\n",
      "19 同様に、\"&\" は英語アルファベットの27番目の文字とされ、アメリカ合衆国やその他の地域でも、子供達はアンパサンドはアルファベットの最後の文字だと教えられていた。\n",
      "20 1863年の M. B. Moore の著書 The Dixie Primer, for the Little Folks にその一例を見ることができる。\n",
      "21 ジョージ・エリオットは、1859年に発表した小説「アダム・ビード(英語版)」の中で、Jacob Storey に次のセリフを語らせている。\n",
      "22 \"He thought it [Z] had only been put to finish off th' alphabet like; though ampusand would ha' done as well, for what he could see.\" よく知られた童謡の Apple Pie ABC は \"X, Y, Z, and ampersand, All wished for a piece in hand\" という歌詞で締めくくられる。\n",
      "23 アンパサンドは、ティロ式記号の et (\"⁊\", Unicode U+204A) とは別のものである。\n",
      "24 ティロ式記号の et は、アンパサンドと意味は同じだが数字の「7」に似た形の記号である。\n",
      "25 両者はともに古代から使用され、中世を通してラテン語の et を表すために使用された。\n",
      "26 しかし、アンパサンドとティロ式記号の et はそれぞれ独立に発明されたものである。\n",
      "27 ラテン文字から発展した古アイルランド語の文字では、アイルランド語の agus(「...と...」)を表すためにティロ式記号の et が使用されていた。\n",
      "28 今日はゲール文字の一部として主に装飾的な目的で使用されている。\n",
      "29 この文字はアイルランドにおけるキリスト教時代初期に修道院の影響によって書き文字に加わった可能性がある。\n",
      "30 日常的な手書きの場合、欧米では小文字の ε(エプシロン)を大きくしたもの(あるいは数字の \"3\" の鏡文字)に縦線を加えた形の単純化されたアンパサンドがしばしば使われる。\n",
      "31 また、エプシロンの上下に縦線または点を付けたものもしばしば使われる。\n",
      "32 くだけた用法として、プラス記号(\"+\", この記号もまた et の合字である)がアンパサンドの代わりに使われることもある。\n",
      "33 また、プラス記号に輪を重ねたような、無声歯茎側面摩擦音を示す発音記号「[ɬ]」のようなものが使われることもある。\n",
      "34 ティロの速記には「et」を表すための「⁊」(U+204A Tironian sign et)がある。\n",
      "35 この文字はドイツのフラクトゥールで使われたほか、ゲール文字でも使用される。\n",
      "36 ギリシア文字では「......と」を意味するκαιを表すための合字として「ϗ」(U+03D7 Greek kai symbol)が使われることがある。\n",
      "37 プログラミング言語では、C など多数の言語で AND 演算子として用いられる。\n",
      "38 PHPでは、変数宣言記号($)の直前に記述することで、参照渡しを行うことができる。\n",
      "39 BASIC 系列の言語では文字列の連結演算子として使用される。\n",
      "40 \"foo\" & \"bar\" は \"foobar\" を返す。\n",
      "41 また、主にマイクロソフト系では整数の十六進表記に &h を用い、&h0F (十進で15)のように表現する。\n",
      "42 SGML、XML、HTMLでは、アンパサンドを使ってSGML実体を参照する。\n",
      "43 \n",
      "44 言語(げんご)は、狭義には「声による記号の体系」をいう。\n",
      "45 広辞苑や大辞泉には次のように解説されている。\n",
      "46 『日本大百科全書』では、「言語」という語は多義である、と解説され、大脳の言語中枢(英語版)に蓄えられた《語彙と文法規則の体系》を指すこともあり、その体系を用いる能力としてとらえることもある、と解説され、一方では、抽象的に「すべての人間が共有する言語能力」を指すこともあり、「個々の個別言語」を指すこともある、と解説されている。\n",
      "47 広義の言語には、verbalなものとnon-verbalなもの(各種記号、アイコン、図形、ボディーランゲージ等)の両方を含み、日常のコミュニケーションでは狭義の言語表現に身振り、手振り、図示、擬音等も加えて表現されることもある。\n",
      "48 言語は、人間が用いる意志伝達手段であり、社会集団内で形成習得され、意志を相互に伝達することや、抽象的な思考を可能にし、結果として人間の社会的活動や文化的活動を支えている。\n",
      "49 言語には、文化の特徴が織り込まれており、共同体で用いられている言語の習得をすることによって、その共同体での社会的学習、および人格の形成をしていくことになる。\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットの中身を確認する\n",
    "for i, text in enumerate(unsup_train_dataset[:50][\"text\"]):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-ecc223a34f1ffe88.arrow\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットから空白行を事例を削除\n",
    "unsup_train_dataset = unsup_train_dataset.filter(\n",
    "    lambda example: example[\"text\"].strip() != \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-e9e6caaca5ef995f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-9484a91dba79bc01.arrow\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットをシャッフルし、最初の100万事例を取り出す\n",
    "unsup_train_dataset = unsup_train_dataset.shuffle().select(\n",
    "    range(1000000)\n",
    ")\n",
    "# パフォーマンスの低下を防ぐために、シャッフルされた状態の訓練セットを\n",
    "# ディスクに書き込む\n",
    "unsup_train_dataset = unsup_train_dataset.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 前処理後の訓練セットの形式と事例数を確認\n",
    "print(unsup_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2005年の時点で、10,000人ものウズベキスタン人が韓国での労働に従事しており、その大部分が高麗人である。\n",
      "1 小学5年生(11歳)の時から芸能活動を開始。\n",
      "2 i ħ d d t | ψ ( t ) ⟩ = L ^ | ψ ( t ) ⟩ {\\displaystyle i\\hbar {\\frac {d}{dt}}|\\psi (t)\\rangle ={\\hat {L}}|\\psi (t)\\rangle }\n",
      "3 安土宗論(あづちしゅうろん)は、1579年(天正7年)、安土城下の浄厳院で行われた浄土宗と法華宗の宗論。\n",
      "4 1927年 オーストラリア選手権(1927ねんオーストラリアせんしゅけん、1927 Australian Championships)に関する記事。\n",
      "5 さらにマップ上で最大8つまでしか建築できず(司令官アビリティの”解体”か設置したプレイヤー自らが出向いて解体する必要がある)\n",
      "6 特に誉淳が1827年から作成した『古瓦譜』は畿内で600点以上の拓本を蒐集し、瓦当文様に着目したうえで編年を試みている。\n",
      "7 マルクス主義者を広言し、メキシコ共産党の敵であり味方であった。\n",
      "8 ICHILLIN'(アイチリン、朝: 아이칠린)は、韓国の7人組女性アイドルグループ。\n",
      "9 マークVIは1983年にモデルサイクルを終了し、1984年のマークVII(英語版)はフルサイズセグメントから撤退し、マークシリーズは異なるセグメントに移行した。\n"
     ]
    }
   ],
   "source": [
    "# 前処理後の訓練セットの内容を確認\n",
    "for i, text in enumerate(unsup_train_dataset[:10][\"text\"]):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset jglue (/root/.cache/huggingface/datasets/llm-book___jglue/JSTS/1.1.0/b394a8dbefe82fb1dc2724c1eb79bb1ea3062df2037f91a69a27c089f3ff685f)\n",
      "Reusing dataset jglue (/root/.cache/huggingface/datasets/llm-book___jglue/JSTS/1.1.0/b394a8dbefe82fb1dc2724c1eb79bb1ea3062df2037f91a69a27c089f3ff685f)\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Hubのllm-book/JGLUEのリポジトリから\n",
    "# JSTSデータセットの訓練セットと検証セットを読み込み、\n",
    "# それぞれをSimCSEの検証セットとテストセットとして使用する\n",
    "valid_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JSTS\", split=\"train\"\n",
    ")\n",
    "test_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JSTS\", split=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トークナイザとcollate関数の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "def unsup_train_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"教師なしSimCSEの訓練セットのミニバッチを作成\"\"\"\n",
    "    # ミニバッチに含まれる文にトークナイザを適用する\n",
    "    tokenized_texts = tokenizer(\n",
    "        [example[\"text\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # 文と文の類似度行列における正例ペアの位置を示すTensorを作成する\n",
    "    # 行列のi行目の事例（文）に対してi列目の事例（文）との組が正例ペアとなる\n",
    "    labels = torch.arange(len(examples))\n",
    "\n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_texts,\n",
    "        \"tokenized_texts_2\": tokenized_texts,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"SimCSEの検証・テストセットのミニバッチを作成\"\"\"\n",
    "    # ミニバッチの文ペアに含まれる文（文1と文2）のそれぞれに\n",
    "    # トークナイザを適用する\n",
    "    tokenized_texts_1 = tokenizer(\n",
    "        [example[\"sentence1\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized_texts_2 = tokenizer(\n",
    "        [example[\"sentence2\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # 文1と文2の類似度行列における正例ペアの位置を示すTensorを作成する\n",
    "    # 行列のi行目の事例（文1）に対して\n",
    "    # i列目の事例（文2）との組が正例ペアとなる\n",
    "    labels = torch.arange(len(examples))\n",
    "\n",
    "    # データセットに付与された類似度スコアのTensorを作成する\n",
    "    label_scores = torch.tensor(\n",
    "        [example[\"label\"] for example in examples]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_texts_1,\n",
    "        \"tokenized_texts_2\": tokenized_texts_2,\n",
    "        \"labels\": labels,\n",
    "        \"label_scores\": label_scores,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデル準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:803: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from transformers.utils import ModelOutput\n",
    "\n",
    "class SimCSEModel(nn.Module):\n",
    "    \"\"\"SimCSEのモデル\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name: str,\n",
    "        mlp_only_train: bool = False,\n",
    "        temperature: float = 0.05,\n",
    "    ):\n",
    "        \"\"\"モデルの初期化\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # モデル名からエンコーダを初期化する\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        # パラメータをメモリ上に隣接した形で配置\n",
    "        # これを実行しない場合、モデルの保存でエラーになることがある\n",
    "        # for param in model.parameters():\n",
    "        #     param.data = param.data.contiguous()\n",
    "        # MLP層の次元数\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        # MLP層の線形層\n",
    "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        # MLP層の活性化関数\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # MLP層による変換を訓練時にのみ適用するよう設定するフラグ\n",
    "        self.mlp_only_train = mlp_only_train\n",
    "        # 交差エントロピー損失の計算時に使用する温度\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def encode_texts(self, tokenized_texts: BatchEncoding) -> Tensor:\n",
    "        \"\"\"エンコーダを用いて文をベクトルに変換\"\"\"\n",
    "        # トークナイズされた文をエンコーダに入力する\n",
    "        encoded_texts = self.encoder(**tokenized_texts)\n",
    "        # モデルの最終層の出力（last_hidden_state）の\n",
    "        # [CLS]トークン（0番目の位置のトークン）のベクトルを取り出す\n",
    "        encoded_texts = encoded_texts.last_hidden_state[:, 0]\n",
    "\n",
    "        # self.mlp_only_trainのフラグがTrueに設定されていて\n",
    "        # かつ訓練時でない場合、MLP層の変換を適用せずにベクトルを返す\n",
    "        if self.mlp_only_train and not self.training:\n",
    "            return encoded_texts\n",
    "\n",
    "        # MLP層によるベクトルの変換を行う\n",
    "        encoded_texts = self.dense(encoded_texts)\n",
    "        encoded_texts = self.activation(encoded_texts)\n",
    "\n",
    "        return encoded_texts\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokenized_texts_1: BatchEncoding,\n",
    "        tokenized_texts_2: BatchEncoding,\n",
    "        labels: Tensor,\n",
    "        label_scores: Tensor | None = None,\n",
    "    ) -> ModelOutput:\n",
    "        \"\"\"モデルの前向き計算を定義\"\"\"\n",
    "        # 文ペアをベクトルに変換する\n",
    "        encoded_texts_1 = self.encode_texts(tokenized_texts_1)\n",
    "        encoded_texts_2 = self.encode_texts(tokenized_texts_2)\n",
    "\n",
    "        # 文ペアの類似度行列を作成する\n",
    "        sim_matrix = F.cosine_similarity(\n",
    "            encoded_texts_1.unsqueeze(1),\n",
    "            encoded_texts_2.unsqueeze(0),\n",
    "            dim=2,\n",
    "        )\n",
    "\n",
    "        # 交差エントロピー損失を求める\n",
    "        loss = F.cross_entropy(sim_matrix / self.temperature, labels)\n",
    "\n",
    "        # 性能評価に使用するため、正例ペアに対するスコアを類似度行列から取り出す\n",
    "        positive_mask = F.one_hot(labels, sim_matrix.size(1)).bool()\n",
    "        positive_scores = torch.masked_select(\n",
    "            sim_matrix, positive_mask\n",
    "        )\n",
    "\n",
    "        return ModelOutput(loss=loss, scores=positive_scores)\n",
    "\n",
    "# 教師なしSimCSEのモデルを初期化する\n",
    "unsup_model = SimCSEModel(base_model_name, mlp_only_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainerの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    モデルが予測したスコアと評価用データのスコアの\n",
    "    スピアマンの順位相関係数を計算\n",
    "    \"\"\"\n",
    "    scores = p.predictions\n",
    "    labels, label_scores = p.label_ids\n",
    "\n",
    "    spearman = spearmanr(scores, label_scores).correlation\n",
    "\n",
    "    return {\"spearman\": spearman}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 教師なしSimCSEの訓練のハイパーパラメータを設定する\n",
    "unsup_training_args = TrainingArguments(\n",
    "    output_dir=\"../model/outputs_unsup_simcse\",  # 結果の保存先フォルダ\n",
    "    per_device_train_batch_size=256,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=256,  # 評価時のバッチサイズ\n",
    "    learning_rate=3e-5,  # 学習率\n",
    "    num_train_epochs=1,  # 訓練エポック数\n",
    "    evaluation_strategy=\"steps\",  # 検証セットによる評価のタイミング\n",
    "    eval_steps=250,  # 検証セットによる評価を行う訓練ステップ数の間隔\n",
    "    logging_steps=250,  # ロギングを行う訓練ステップ数の間隔\n",
    "    save_steps=250,  # チェックポイントを保存する訓練ステップ数の間隔\n",
    "    save_total_limit=1,  # 保存するチェックポイントの最大数\n",
    "    fp16=True,  # 自動混合精度演算の有効化\n",
    "    load_best_model_at_end=True,  # 最良のモデルを訓練終了後に読み込むか\n",
    "    metric_for_best_model=\"spearman\",  # 最良のモデルを決定する評価指標\n",
    "    remove_unused_columns=False,  # データセットの不要フィールドを削除するか\n",
    "    report_to=\"none\",  # 外部ツールへのログを無効化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer\n",
    "\n",
    "class SimCSETrainer(Trainer):\n",
    "    \"\"\"SimCSEの訓練に使用するTrainer\"\"\"\n",
    "\n",
    "    def get_eval_dataloader(\n",
    "        self, eval_dataset: Dataset | None = None\n",
    "    ) -> DataLoader:\n",
    "        \"\"\"\n",
    "        検証・テストセットのDataLoaderでeval_collate_fnを使うように\n",
    "        Trainerのget_eval_dataloaderをオーバーライド\n",
    "        \"\"\"\n",
    "        if eval_dataset is None:\n",
    "            eval_dataset = self.eval_dataset\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=64,\n",
    "            collate_fn=eval_collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "# 教師なしSimCSEのTrainerを初期化する\n",
    "unsup_trainer = SimCSETrainer(\n",
    "    model=unsup_model,\n",
    "    args=unsup_training_args,\n",
    "    data_collator=unsup_train_collate_fn,\n",
    "    train_dataset=unsup_train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  訓練の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3907' max='3907' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3907/3907 27:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.331667</td>\n",
       "      <td>0.752163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.402906</td>\n",
       "      <td>0.755473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.385067</td>\n",
       "      <td>0.753259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.366536</td>\n",
       "      <td>0.757948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.350700</td>\n",
       "      <td>0.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.320323</td>\n",
       "      <td>0.760896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.332596</td>\n",
       "      <td>0.759427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.334367</td>\n",
       "      <td>0.757139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.300360</td>\n",
       "      <td>0.758956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.297671</td>\n",
       "      <td>0.758454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.306671</td>\n",
       "      <td>0.753173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.296501</td>\n",
       "      <td>0.756182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.296679</td>\n",
       "      <td>0.756792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.294138</td>\n",
       "      <td>0.756875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.301019</td>\n",
       "      <td>0.756191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3907, training_loss=0.00033567894401759603, metrics={'train_runtime': 1629.5441, 'train_samples_per_second': 613.669, 'train_steps_per_second': 2.398, 'total_flos': 0.0, 'train_loss': 0.00033567894401759603, 'epoch': 1.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 教師なしSimCSEの訓練を行う\n",
    "unsup_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 性能評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3203227519989014,\n",
       " 'eval_spearman': 0.7608961210671525,\n",
       " 'eval_runtime': 10.878,\n",
       " 'eval_samples_per_second': 1144.601,\n",
       " 'eval_steps_per_second': 4.504,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証セットで教師なしSimCSEのモデル評価を行う\n",
    "unsup_trainer.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1774518489837646,\n",
       " 'eval_spearman': 0.7878933714566898,\n",
       " 'eval_runtime': 1.3524,\n",
       " 'eval_samples_per_second': 1077.307,\n",
       " 'eval_steps_per_second': 4.436,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テストセットで教師なしSimCSEのモデル評価を行う\n",
    "unsup_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/outputs_unsup_simcse/encoder/tokenizer_config.json',\n",
       " '../model/outputs_unsup_simcse/encoder/special_tokens_map.json',\n",
       " '../model/outputs_unsup_simcse/encoder/vocab.txt',\n",
       " '../model/outputs_unsup_simcse/encoder/added_tokens.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### トークナイザの保存とモデルの保存\n",
    "encoder_path = \"../model/outputs_unsup_simcse/encoder\"\n",
    "unsup_model.encoder.save_pretrained(encoder_path)\n",
    "tokenizer.save_pretrained(encoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 教師ありSimCSEの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数シードの設定を行う\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset jsnli (/root/.cache/huggingface/datasets/llm-book___jsnli/default/1.0.0/b59ac9cb188ddb68dc451bae1b33ac9ebe501a5d2c41f17c5ec06ad0621186d7)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# JSNLIの訓練セットを読み込む\n",
    "jsnli_dataset = load_dataset(\"llm-book/jsnli\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label'],\n",
      "    num_rows: 533005\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# JSNLIの訓練セットの形式と事例数を確認\n",
    "print(jsnli_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hypothesis': '男 は 魔法 の ショー の ため に ナイフ を 投げる 行為 を 練習 して い ます 。',\n",
      " 'label': 'neutral',\n",
      " 'premise': 'ガレージ で 、 壁 に ナイフ を 投げる 男 。'}\n",
      "{'hypothesis': '女性 が 畑 で 踊って い ます 。',\n",
      " 'label': 'contradiction',\n",
      " 'premise': '茶色 の ドレス を 着た 女性 が ベンチ に 座って い ます 。'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# JSNLIの訓練セットの内容を確認\n",
    "pprint(jsnli_dataset[0])\n",
    "pprint(jsnli_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from typing import Iterator\n",
    "\n",
    "# JSNLI訓練セットから、前提文とラベルごとに仮説文をまとめたdictを作成する\n",
    "premise2hypotheses = {}\n",
    "\n",
    "primises = jsnli_dataset[\"premise\"] # 前提文\n",
    "hypotheses = jsnli_dataset[\"hypothesis\"] # 仮説文\n",
    "labels = jsnli_dataset[\"label\"] # ラベル\n",
    "\n",
    "for premise, hypothesis, label in zip(primises, hypotheses, labels):\n",
    "    if premise not in premise2hypotheses:\n",
    "        premise2hypotheses[premise] = {\n",
    "            \"entailment\": [],\n",
    "            \"neutral\": [],\n",
    "            \"contradiction\": []\n",
    "        }\n",
    "        \n",
    "    premise2hypotheses[premise][label].append(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sup_train_example():\n",
    "    \"\"\"\n",
    "    教師ありSimCSEの訓練セットを生成する関数\n",
    "    Returns:\n",
    "        dict: 'premise', 'entailment_hypothesis', 'contradiction_hypothesis'をキーとする辞書\n",
    "    \"\"\"\n",
    "    # 結果を格納する辞書を初期化\n",
    "    dataset_dict = {\n",
    "        \"premise\": [],\n",
    "        \"entailment_hypothesis\": [],\n",
    "        \"contradiction_hypothesis\": []\n",
    "    }\n",
    "    \n",
    "    # JSNLIのデータから三つ組を生成\n",
    "    for premise, hypotheses in premise2hypotheses.items():\n",
    "        # 「矛盾」ラベルの仮説文が存在しない場合はスキップ\n",
    "        if len(hypotheses[\"contradiction\"]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 「含意」ラベルの仮説文それぞれに対して処理\n",
    "        for entailment_hypothesis in hypotheses[\"entailment\"]:\n",
    "            # 「矛盾」ラベルの仮説文をランダムに選択\n",
    "            contradiction_hypothesis = random.choice(hypotheses[\"contradiction\"])\n",
    "            \n",
    "            # 各要素をリストに追加\n",
    "            dataset_dict[\"premise\"].append(premise)\n",
    "            dataset_dict[\"entailment_hypothesis\"].append(entailment_hypothesis)\n",
    "            dataset_dict[\"contradiction_hypothesis\"].append(contradiction_hypothesis)\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# データセットの作成\n",
    "try:\n",
    "    dataset_dict = generate_sup_train_example()\n",
    "    sup_train_dataset = Dataset.from_dict(dataset_dict)\n",
    "except Exception as e:\n",
    "    print(f\"エラーが発生しました: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'entailment_hypothesis', 'contradiction_hypothesis'],\n",
      "    num_rows: 173438\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットの形式と事例数を確認\n",
    "print(sup_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contradiction_hypothesis': '男 が 台所 の テーブル で 本 を 読んで い ます 。',\n",
      " 'entailment_hypothesis': 'ガレージ に 男 が い ます 。',\n",
      " 'premise': 'ガレージ で 、 壁 に ナイフ を 投げる 男 。'}\n",
      "{'contradiction_hypothesis': '黒人 は デスクトップ コンピューター を 使用 し ます 。',\n",
      " 'entailment_hypothesis': '人 は 椅子 に 座って い ます 。',\n",
      " 'premise': 'ラップ トップ コンピューター を 使用 して 机 に 座って いる 若い 白人 男 。'}\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットの内容を確認\n",
    "pprint(sup_train_dataset[0])\n",
    "pprint(sup_train_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collate関数の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "def sup_train_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"訓練セットのミニバッチを作成\"\"\"\n",
    "    premises = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for example in examples:\n",
    "        premises.append(example[\"premise\"])\n",
    "        \n",
    "        entailment_hypotheis = example[\"entailment_hypothesis\"]\n",
    "        contradiction_hypothesis = example[\"contradiction_hypothesis\"]\n",
    "        \n",
    "        hypotheses.extend(\n",
    "            [entailment_hypotheis, contradiction_hypothesis]\n",
    "        )\n",
    "    # ミニバッチに含まれる前提文と仮説文にトークナイザを適用\n",
    "    tokenized_premises = tokenizer(\n",
    "        premises,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized_hypotheses = tokenizer(\n",
    "        hypotheses,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # 前提文と仮説文の類似度行列における正例ペアの位置を示すTensorを作成\n",
    "    # 行列にi行目の事例（前提文）に対して\n",
    "    # 2*i列目の要素（仮説文）が正例ペアとなる\n",
    "    labels = torch.arange(0, 2 * len(premises), 2)\n",
    "    \n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_premises,\n",
    "        \"tokenized_texts_2\": tokenized_hypotheses,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教師ありSimCSEモデルを初期化\n",
    "sup_model = SimCSEModel(base_model_name, mlp_only_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainerの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 教師ありSimCSEに訓練のハイパーパラメータを設定\n",
    "sup_training_args = TrainingArguments(\n",
    "    output_dir=\"../model/outputs_sup_simcse\",\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    logging_steps=250,\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"spearman\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",  # 外部ツールへのログを無効化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教師ありSimCSEのTrainerを初期化\n",
    "sup_trainer = SimCSETrainer(\n",
    "    model=sup_model,\n",
    "    args=sup_training_args,\n",
    "    data_collator=sup_train_collate_fn,\n",
    "    train_dataset=sup_train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4065' max='4065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4065/4065 22:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.438000</td>\n",
       "      <td>2.782808</td>\n",
       "      <td>0.791591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>2.710751</td>\n",
       "      <td>0.780901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.017300</td>\n",
       "      <td>2.759371</td>\n",
       "      <td>0.786594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.967000</td>\n",
       "      <td>2.818005</td>\n",
       "      <td>0.788974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.926100</td>\n",
       "      <td>2.781399</td>\n",
       "      <td>0.798985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.828600</td>\n",
       "      <td>2.828457</td>\n",
       "      <td>0.792996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>2.907998</td>\n",
       "      <td>0.801895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>2.885943</td>\n",
       "      <td>0.800020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.743200</td>\n",
       "      <td>2.904130</td>\n",
       "      <td>0.793232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.724700</td>\n",
       "      <td>2.885217</td>\n",
       "      <td>0.796730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>2.924343</td>\n",
       "      <td>0.800334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.620400</td>\n",
       "      <td>2.915778</td>\n",
       "      <td>0.800752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>2.910745</td>\n",
       "      <td>0.799712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>2.924684</td>\n",
       "      <td>0.801591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.601500</td>\n",
       "      <td>2.912793</td>\n",
       "      <td>0.799567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.595800</td>\n",
       "      <td>2.912041</td>\n",
       "      <td>0.800678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4065, training_loss=0.8097005076953846, metrics={'train_runtime': 1370.6597, 'train_samples_per_second': 379.608, 'train_steps_per_second': 2.966, 'total_flos': 0.0, 'train_loss': 0.8097005076953846, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 教師ありSimCSEの訓練を行う\n",
    "sup_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 性能評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.9079980850219727,\n",
       " 'eval_spearman': 0.8018950489894365,\n",
       " 'eval_runtime': 10.6323,\n",
       " 'eval_samples_per_second': 1171.049,\n",
       " 'eval_steps_per_second': 9.217,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証セットで教師ありSimCSEのモデル評価を行う\n",
    "sup_trainer.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.628929376602173,\n",
       " 'eval_spearman': 0.8184395733197469,\n",
       " 'eval_runtime': 1.3935,\n",
       " 'eval_samples_per_second': 1045.534,\n",
       " 'eval_steps_per_second': 8.611,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テストセットで教師ありSimCSEのモデル評価を行う\n",
    "sup_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最近傍探索ライブラリFaissを使った検索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faissを利用した最近傍探索の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu==1.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset jawiki-paragraphs (/root/.cache/huggingface/datasets/llm-book___jawiki-paragraphs/default/1.0.0/0f2d7acd99ad7ae0615fd07442dbd1654d37c5d60a39fc720efe28acff3f86f8)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "paragraph_dataset = load_dataset(\n",
    "    \"llm-book/jawiki-paragraphs\", split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'pageid', 'revid', 'paragraph_index', 'title', 'section', 'text', 'html_tag'],\n",
      "    num_rows: 9668476\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 段落データ形式と事例数を確認\n",
    "print(paragraph_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'html_tag': 'p',\n",
      " 'id': '5-89167474-0',\n",
      " 'pageid': 5,\n",
      " 'paragraph_index': 0,\n",
      " 'revid': 89167474,\n",
      " 'section': '__LEAD__',\n",
      " 'text': 'アンパサンド(&, 英語: '\n",
      "         'ampersand)は、並立助詞「...と...」を意味する記号である。ラテン語で「...と...」を表す接続詞 \"et\" '\n",
      "         'の合字を起源とする。現代のフォントでも、Trebuchet MS など一部のフォントでは、\"et\" '\n",
      "         'の合字であることが容易にわかる字形を使用している。',\n",
      " 'title': 'アンパサンド'}\n",
      "{'html_tag': 'p',\n",
      " 'id': '5-89167474-1',\n",
      " 'pageid': 5,\n",
      " 'paragraph_index': 1,\n",
      " 'revid': 89167474,\n",
      " 'section': '語源',\n",
      " 'text': '英語で教育を行う学校でアルファベットを復唱する場合、その文字自体が単語となる文字(\"A\", \"I\", かつては \"O\" '\n",
      "         'も)については、伝統的にラテン語の per se(それ自体)を用いて \"A per se A\" '\n",
      "         'のように唱えられていた。また、アルファベットの最後に、27番目の文字のように \"&\" を加えることも広く行われていた。\"&\" '\n",
      "         'はラテン語で et と読まれていたが、後に英語で and と読まれるようになった。結果として、アルファベットの復唱の最後は \"X, Y, '\n",
      "         'Z, and per se and\" という形になった。この最後のフレーズが繰り返されるうちに \"ampersand\" '\n",
      "         'と訛っていき、この言葉は1837年までには英語の一般的な語法となった。',\n",
      " 'title': 'アンパサンド'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 段落データの内容を確認\n",
    "pprint(paragraph_dataset[0])\n",
    "pprint(paragraph_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9669/9669 [01:29<00:00, 107.46ba/s]\n"
     ]
    }
   ],
   "source": [
    "# 段落データのうち、各記事の最初の段落のみを使う\n",
    "paragraph_dataset = paragraph_dataset.filter(\n",
    "    lambda example: example[\"paragraph_index\"] == 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トークナイザとモデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting mecab-python3\n",
      "  Downloading mecab_python3-1.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mecab-python3\n",
      "Successfully installed mecab-python3-1.0.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.10/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1489, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='24.3.1'),)\n"
     ]
    }
   ],
   "source": [
    "# !pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers[ja,sentencepice] in /usr/local/lib/python3.10/dist-packages (4.21.2)\n",
      "\u001b[33mWARNING: transformers 4.21.2 does not provide the extra 'sentencepice'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (4.65.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (3.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (2023.6.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepice]) (2.31.0)\n",
      "Collecting fugashi>=1.0\n",
      "  Downloading fugashi-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.7/671.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting unidic>=1.0.2\n",
      "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ipadic<2.0,>=1.0.0\n",
      "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting unidic-lite>=1.0.7\n",
      "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[ja,sentencepice]) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[ja,sentencepice]) (2023.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging>=20.0->transformers[ja,sentencepice]) (3.0.9)\n",
      "Collecting wasabi<1.0.0,>=0.6.0\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting plac<2.0.0,>=1.1.3\n",
      "  Downloading plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja,sentencepice]) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja,sentencepice]) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja,sentencepice]) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja,sentencepice]) (1.26.16)\n",
      "Building wheels for collected packages: ipadic, unidic, unidic-lite\n",
      "  Building wheel for ipadic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556721 sha256=463b39e4a7d2e5a1ee1e993f15b0e8a75aab651ef3a4095e62cd9946f240fff7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2vj8u2b7/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
      "  Building wheel for unidic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7425 sha256=57ad393d8dfdfb5afff7d3c562756db80d26982fbb4067ba3f4033bb4030523e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2vj8u2b7/wheels/7a/72/72/1f3d654c345ea69d5d51b531c90daf7ba14cc555eaf2c64ab0\n",
      "  Building wheel for unidic-lite (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658834 sha256=aef714456077ef9a83dee574cef916c106cd3660fe9fbe09ab26a368ea05ea22\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2vj8u2b7/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
      "Successfully built ipadic unidic unidic-lite\n",
      "Installing collected packages: wasabi, unidic-lite, plac, ipadic, fugashi, unidic\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 1.1.2\n",
      "    Uninstalling wasabi-1.1.2:\n",
      "      Successfully uninstalled wasabi-1.1.2\n",
      "Successfully installed fugashi-1.4.0 ipadic-1.0.0 plac-1.4.3 unidic-1.1.0 unidic-lite-1.0.8 wasabi-0.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.10/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1489, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='24.3.1'),)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers[ja,sentencepice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 634/634 [00:00<00:00, 3.78MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 424M/424M [00:10<00:00, 44.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"llm-book/bert-base-japanese-v3-unsup-simcse-jawiki\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoder = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込んだモデルをGPUのメモリに移動\n",
    "device = \"cuda:0\"\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルによる埋め込みの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def embed_texts(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"SimCSEのモデルを用いてテキストの埋め込みを計算\"\"\"\n",
    "    # テキストにトークナイザを適用\n",
    "    tokenized_texts = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    \n",
    "    # トークナイズされたテキストをベクトルに変換\n",
    "    with torch.inference_mode():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            encoded_texts = encoder(\n",
    "                **tokenized_texts\n",
    "            ).last_hidden_state[:, 0]\n",
    "            \n",
    "    # ベクトルをNuPyのarrayに変換\n",
    "    emb = encoded_texts.cpu().numpy().astype(np.float32)\n",
    "    # ベクトルのノルムが1になるように正規化\n",
    "    emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1340/1340 [21:01<00:00,  1.06ba/s]\n"
     ]
    }
   ],
   "source": [
    "# 段落データのすべての事例に埋め込みを付与する\n",
    "paragraph_dataset = paragraph_dataset.map(\n",
    "    lambda examples: {\n",
    "        \"embeddings\": list(embed_texts(examples[\"text\"]))\n",
    "    },\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'pageid', 'revid', 'paragraph_index', 'title', 'section', 'text', 'html_tag', 'embeddings'],\n",
      "    num_rows: 1339236\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 埋め込みを付与した段落データの形式と事例数を確認\n",
    "print(paragraph_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embeddings': [0.042540159076452255,\n",
      "                -0.04196695238351822,\n",
      "                -0.032314036041498184,\n",
      "                0.018254535272717476,\n",
      "                -0.06702621281147003,\n",
      "                -0.06091688573360443,\n",
      "                -0.05339366942644119,\n",
      "                0.00584664149209857,\n",
      "                0.005609563086181879,\n",
      "                0.0004761741147376597,\n",
      "                0.0544019490480423,\n",
      "                -0.03015177696943283,\n",
      "                -0.01541396789252758,\n",
      "                -0.09760468453168869,\n",
      "                0.031523678451776505,\n",
      "                0.0070890034548938274,\n",
      "                0.004207753110677004,\n",
      "                -0.01840118132531643,\n",
      "                -0.07030576467514038,\n",
      "                0.00973212718963623,\n",
      "                0.006157499272376299,\n",
      "                -0.03276313096284866,\n",
      "                -0.00840569194406271,\n",
      "                -0.023154262453317642,\n",
      "                0.051235586404800415,\n",
      "                0.0434039942920208,\n",
      "                -0.046278174966573715,\n",
      "                0.059877779334783554,\n",
      "                -0.020364778116345406,\n",
      "                0.04288491979241371,\n",
      "                0.016641128808259964,\n",
      "                0.016299232840538025,\n",
      "                -0.02672434411942959,\n",
      "                -0.23037508130073547,\n",
      "                -0.04771876707673073,\n",
      "                -0.041735898703336716,\n",
      "                0.026584578678011894,\n",
      "                -0.004833120387047529,\n",
      "                0.03280454874038696,\n",
      "                0.008692031726241112,\n",
      "                0.026281075552105904,\n",
      "                0.008603501133620739,\n",
      "                0.01185057032853365,\n",
      "                0.036140076816082,\n",
      "                -0.0204145647585392,\n",
      "                -0.007465626113116741,\n",
      "                0.062059883028268814,\n",
      "                0.06716842949390411,\n",
      "                0.010287214070558548,\n",
      "                0.017544733360409737,\n",
      "                0.008576548658311367,\n",
      "                0.24090181291103363,\n",
      "                -0.007460836321115494,\n",
      "                0.026462329551577568,\n",
      "                0.05773117020726204,\n",
      "                0.020801538601517677,\n",
      "                9.024007158586755e-05,\n",
      "                0.004082648083567619,\n",
      "                0.022419705986976624,\n",
      "                -0.025337573140859604,\n",
      "                -0.0012932008830830455,\n",
      "                -0.00905726756900549,\n",
      "                0.025043809786438942,\n",
      "                0.002930764341726899,\n",
      "                0.03718065097928047,\n",
      "                -0.02739681489765644,\n",
      "                -0.07144065946340561,\n",
      "                0.05721418932080269,\n",
      "                -0.09308648854494095,\n",
      "                0.032024823129177094,\n",
      "                -0.015171934850513935,\n",
      "                -0.001922608120366931,\n",
      "                0.006132249720394611,\n",
      "                0.05010312795639038,\n",
      "                -0.05577431246638298,\n",
      "                -0.036590781062841415,\n",
      "                0.004664589650928974,\n",
      "                -0.015770766884088516,\n",
      "                0.011320318095386028,\n",
      "                -0.04417886584997177,\n",
      "                0.04446115344762802,\n",
      "                -0.05728046968579292,\n",
      "                -0.02512170933187008,\n",
      "                0.00010270514030708,\n",
      "                -0.0643891990184784,\n",
      "                0.017626937478780746,\n",
      "                -0.056373950093984604,\n",
      "                0.017277875915169716,\n",
      "                0.007070938125252724,\n",
      "                -0.037370987236499786,\n",
      "                0.021627984941005707,\n",
      "                0.013350320979952812,\n",
      "                0.0627049058675766,\n",
      "                0.0012486559571698308,\n",
      "                -0.06406562030315399,\n",
      "                -0.02438768744468689,\n",
      "                0.06499079614877701,\n",
      "                -0.013452861458063126,\n",
      "                -0.01652398891746998,\n",
      "                -0.012981169857084751,\n",
      "                -0.06914354115724564,\n",
      "                0.04866036772727966,\n",
      "                -0.04113989323377609,\n",
      "                0.014882386662065983,\n",
      "                -0.006760261487215757,\n",
      "                -0.0024583141785115004,\n",
      "                -0.05835260823369026,\n",
      "                0.0764366164803505,\n",
      "                -0.013113034889101982,\n",
      "                0.0064517357386648655,\n",
      "                -0.004054510500282049,\n",
      "                0.022705087438225746,\n",
      "                0.00130589643958956,\n",
      "                0.04765632003545761,\n",
      "                0.011330176144838333,\n",
      "                -0.017515264451503754,\n",
      "                -0.03595444932579994,\n",
      "                0.010056952014565468,\n",
      "                -0.05186023935675621,\n",
      "                -0.026217250153422356,\n",
      "                -0.015386904589831829,\n",
      "                0.026784498244524002,\n",
      "                0.021116530522704124,\n",
      "                0.028124535456299782,\n",
      "                0.0003387818287592381,\n",
      "                -0.05653264746069908,\n",
      "                0.023717600852251053,\n",
      "                0.040606699883937836,\n",
      "                -0.012987330555915833,\n",
      "                0.017380109056830406,\n",
      "                0.01698986254632473,\n",
      "                -0.04050646722316742,\n",
      "                0.06733410060405731,\n",
      "                -0.017750302329659462,\n",
      "                -0.024987835437059402,\n",
      "                -0.034051522612571716,\n",
      "                0.03133416920900345,\n",
      "                -0.021355129778385162,\n",
      "                0.05992012098431587,\n",
      "                -0.022111164405941963,\n",
      "                0.0014494899660348892,\n",
      "                0.01844825968146324,\n",
      "                -0.056883540004491806,\n",
      "                -0.0699291005730629,\n",
      "                -0.02995873987674713,\n",
      "                -0.00968884490430355,\n",
      "                -0.024245543405413628,\n",
      "                -0.043731823563575745,\n",
      "                -0.04915335401892662,\n",
      "                0.03337789699435234,\n",
      "                0.03428194671869278,\n",
      "                -0.022460561245679855,\n",
      "                -0.012695708312094212,\n",
      "                0.057590026408433914,\n",
      "                0.023606890812516212,\n",
      "                -0.08523456752300262,\n",
      "                -0.015011265873908997,\n",
      "                0.019639793783426285,\n",
      "                0.054789721965789795,\n",
      "                -0.04390912875533104,\n",
      "                -0.011557498015463352,\n",
      "                0.007640243507921696,\n",
      "                -0.00903007946908474,\n",
      "                -0.008442888036370277,\n",
      "                0.007301672361791134,\n",
      "                0.03722250461578369,\n",
      "                -0.024697281420230865,\n",
      "                0.026772288605570793,\n",
      "                -0.01160980761051178,\n",
      "                -0.005527100060135126,\n",
      "                -0.023254944011569023,\n",
      "                -0.019672321155667305,\n",
      "                0.04083985462784767,\n",
      "                -0.022480012848973274,\n",
      "                -0.0014800591161474586,\n",
      "                -0.020065918564796448,\n",
      "                -0.021317943930625916,\n",
      "                0.0964040607213974,\n",
      "                0.041126567870378494,\n",
      "                0.046738818287849426,\n",
      "                0.05795814096927643,\n",
      "                0.024722905829548836,\n",
      "                0.03213697671890259,\n",
      "                -0.039190858602523804,\n",
      "                -0.013017687946557999,\n",
      "                0.0324007011950016,\n",
      "                0.008898039348423481,\n",
      "                -0.04187396913766861,\n",
      "                0.011144150979816914,\n",
      "                -0.07008207589387894,\n",
      "                -0.007209671661257744,\n",
      "                0.040143225342035294,\n",
      "                0.008102566935122013,\n",
      "                0.0329863540828228,\n",
      "                -0.0024545216001570225,\n",
      "                0.007923739030957222,\n",
      "                -0.05220025032758713,\n",
      "                -0.008946295827627182,\n",
      "                0.003465465735644102,\n",
      "                -0.012969347648322582,\n",
      "                -0.036199551075696945,\n",
      "                0.004500904586166143,\n",
      "                -0.036174945533275604,\n",
      "                -0.0038527173455804586,\n",
      "                -0.0426744669675827,\n",
      "                -0.010328895412385464,\n",
      "                0.017991017550230026,\n",
      "                0.015834352001547813,\n",
      "                -0.06596431881189346,\n",
      "                -0.03800814226269722,\n",
      "                -0.05578521266579628,\n",
      "                -0.021106792613863945,\n",
      "                0.02356788143515587,\n",
      "                -0.015794597566127777,\n",
      "                0.004155648406594992,\n",
      "                0.02147161029279232,\n",
      "                0.016567137092351913,\n",
      "                -0.03116142936050892,\n",
      "                -0.015015864744782448,\n",
      "                0.00386902061291039,\n",
      "                -0.018701378256082535,\n",
      "                0.04944441467523575,\n",
      "                -0.024762248620390892,\n",
      "                0.0002054251090157777,\n",
      "                -0.07038494199514389,\n",
      "                -0.05456056073307991,\n",
      "                0.0325070358812809,\n",
      "                0.03351812809705734,\n",
      "                0.014695254154503345,\n",
      "                0.00500953383743763,\n",
      "                -0.03396925330162048,\n",
      "                0.024180971086025238,\n",
      "                0.016184862703084946,\n",
      "                0.02678351104259491,\n",
      "                0.004764530807733536,\n",
      "                -0.026743724942207336,\n",
      "                -0.008947256952524185,\n",
      "                -0.007029969245195389,\n",
      "                0.03305850178003311,\n",
      "                -0.0063833133317530155,\n",
      "                0.011202500201761723,\n",
      "                -0.016981080174446106,\n",
      "                0.013703846372663975,\n",
      "                -0.04332580044865608,\n",
      "                -0.005697146523743868,\n",
      "                0.02808438241481781,\n",
      "                0.005758828017860651,\n",
      "                -0.0367637574672699,\n",
      "                -0.0012568276142701507,\n",
      "                -0.04923186078667641,\n",
      "                -0.02611635997891426,\n",
      "                -0.014137279242277145,\n",
      "                -0.04123341292142868,\n",
      "                0.04124460741877556,\n",
      "                -0.022705236449837685,\n",
      "                0.05965059623122215,\n",
      "                -0.03479090332984924,\n",
      "                -0.011191115714609623,\n",
      "                0.04318763688206673,\n",
      "                -0.014676064252853394,\n",
      "                0.028514273464679718,\n",
      "                -0.04710642248392105,\n",
      "                -0.030521174892783165,\n",
      "                -0.021076006814837456,\n",
      "                -0.029443159699440002,\n",
      "                0.0237666554749012,\n",
      "                0.07126199454069138,\n",
      "                -0.058837853372097015,\n",
      "                -0.0412045493721962,\n",
      "                -0.052679985761642456,\n",
      "                -0.060660067945718765,\n",
      "                0.0514436699450016,\n",
      "                0.007330047897994518,\n",
      "                -0.04797064885497093,\n",
      "                0.013167074881494045,\n",
      "                -0.05175577849149704,\n",
      "                -0.008066194131970406,\n",
      "                0.01966707408428192,\n",
      "                -0.014517571777105331,\n",
      "                -0.0479314811527729,\n",
      "                -0.03634883463382721,\n",
      "                0.027336277067661285,\n",
      "                -0.013093679212033749,\n",
      "                0.022754231467843056,\n",
      "                -0.07912877947092056,\n",
      "                -0.00041337963193655014,\n",
      "                -0.01738922856748104,\n",
      "                0.010096064768731594,\n",
      "                -0.09137891232967377,\n",
      "                -0.03360529989004135,\n",
      "                0.008252881467342377,\n",
      "                0.002329067559912801,\n",
      "                -0.019779404625296593,\n",
      "                0.022803280502557755,\n",
      "                0.006952999159693718,\n",
      "                -0.016818616539239883,\n",
      "                -0.04433990269899368,\n",
      "                0.011819398030638695,\n",
      "                0.016454679891467094,\n",
      "                0.03321840986609459,\n",
      "                -0.012235443107783794,\n",
      "                -0.00829965341836214,\n",
      "                0.007010059431195259,\n",
      "                3.644631942734122e-05,\n",
      "                0.04477927088737488,\n",
      "                -0.04561259225010872,\n",
      "                -0.040887825191020966,\n",
      "                0.006954985670745373,\n",
      "                -0.08563264459371567,\n",
      "                0.05881951004266739,\n",
      "                0.012297743000090122,\n",
      "                -0.03135101497173309,\n",
      "                0.006375560071319342,\n",
      "                0.00018991882097907364,\n",
      "                0.07241058349609375,\n",
      "                -0.02877739816904068,\n",
      "                -0.014290321618318558,\n",
      "                -0.018785865977406502,\n",
      "                0.002494240179657936,\n",
      "                -0.06907522678375244,\n",
      "                -0.029216084629297256,\n",
      "                0.011934129521250725,\n",
      "                0.0007296685944311321,\n",
      "                -0.05039326474070549,\n",
      "                0.021474577486515045,\n",
      "                -0.0008128737099468708,\n",
      "                0.010904442518949509,\n",
      "                -0.08591504395008087,\n",
      "                -0.03731812909245491,\n",
      "                0.03751133754849434,\n",
      "                0.029803458601236343,\n",
      "                -0.03501354157924652,\n",
      "                -0.03768623620271683,\n",
      "                -0.04172416403889656,\n",
      "                -0.07944969832897186,\n",
      "                0.026660986244678497,\n",
      "                0.01132760290056467,\n",
      "                0.013039636425673962,\n",
      "                -0.03228478133678436,\n",
      "                -0.01682828553020954,\n",
      "                -0.025426756590604782,\n",
      "                -0.03717350214719772,\n",
      "                0.0038196404930204153,\n",
      "                0.02791176736354828,\n",
      "                -0.04345746338367462,\n",
      "                0.0013184179551899433,\n",
      "                -0.02902195043861866,\n",
      "                0.0002528324257582426,\n",
      "                0.02520105615258217,\n",
      "                0.061485566198825836,\n",
      "                -0.020349597558379173,\n",
      "                0.053845688700675964,\n",
      "                0.059734929352998734,\n",
      "                0.06740780174732208,\n",
      "                -0.03432319685816765,\n",
      "                -0.014572111889719963,\n",
      "                0.033966921269893646,\n",
      "                0.02249004691839218,\n",
      "                -0.0015251029981300235,\n",
      "                0.0028972786385565996,\n",
      "                -0.06578769534826279,\n",
      "                0.015040186233818531,\n",
      "                -0.00039785506669431925,\n",
      "                -0.016590021550655365,\n",
      "                -0.05548323318362236,\n",
      "                0.023930689319968224,\n",
      "                0.05279238149523735,\n",
      "                0.003391997655853629,\n",
      "                -0.015694444999098778,\n",
      "                0.04054875671863556,\n",
      "                0.04003767669200897,\n",
      "                0.006569531746208668,\n",
      "                -0.06467505544424057,\n",
      "                0.009961578994989395,\n",
      "                -0.04929076135158539,\n",
      "                -0.03551280498504639,\n",
      "                -0.012908241711556911,\n",
      "                0.016105955466628075,\n",
      "                0.047395009547472,\n",
      "                -0.022329634055495262,\n",
      "                -0.0025415895506739616,\n",
      "                0.0240423996001482,\n",
      "                -0.030718475580215454,\n",
      "                -0.038401857018470764,\n",
      "                -0.03320624306797981,\n",
      "                -0.031491804867982864,\n",
      "                -0.011735652573406696,\n",
      "                0.018982058390975,\n",
      "                0.010830450803041458,\n",
      "                0.017371274530887604,\n",
      "                -0.03658872842788696,\n",
      "                0.03283131867647171,\n",
      "                0.005337994545698166,\n",
      "                -0.0036708954721689224,\n",
      "                -0.03797680139541626,\n",
      "                -0.01759413629770279,\n",
      "                0.010849637910723686,\n",
      "                0.0023688485380262136,\n",
      "                -0.019363030791282654,\n",
      "                -0.005663009826093912,\n",
      "                -0.038939859718084335,\n",
      "                0.016082512214779854,\n",
      "                -0.002547689015045762,\n",
      "                0.008325728587806225,\n",
      "                0.007390604354441166,\n",
      "                -0.012269668281078339,\n",
      "                0.009069341234862804,\n",
      "                -0.012798656709492207,\n",
      "                0.010342017747461796,\n",
      "                -0.03950528800487518,\n",
      "                -0.024255085736513138,\n",
      "                -0.008244502358138561,\n",
      "                -0.02046162076294422,\n",
      "                0.012549471110105515,\n",
      "                -0.00036139358417131007,\n",
      "                0.013225859962403774,\n",
      "                0.02483857050538063,\n",
      "                0.030151184648275375,\n",
      "                0.015702830627560616,\n",
      "                0.049219757318496704,\n",
      "                -0.0400761216878891,\n",
      "                -0.020726734772324562,\n",
      "                0.004473079927265644,\n",
      "                -0.035874947905540466,\n",
      "                0.0039020958356559277,\n",
      "                0.0014614424435421824,\n",
      "                -0.013247155584394932,\n",
      "                0.005045789293944836,\n",
      "                -0.01900075562298298,\n",
      "                0.002496435772627592,\n",
      "                0.0989246815443039,\n",
      "                0.03094380535185337,\n",
      "                -0.007828583009541035,\n",
      "                0.028138011693954468,\n",
      "                -0.010845056734979153,\n",
      "                0.0014105149311944842,\n",
      "                0.0015272167511284351,\n",
      "                0.00886779185384512,\n",
      "                -0.034181322902441025,\n",
      "                0.032454099506139755,\n",
      "                -0.0036283666267991066,\n",
      "                -0.051581718027591705,\n",
      "                -0.03370273485779762,\n",
      "                -0.0014410456642508507,\n",
      "                -0.03633030876517296,\n",
      "                -0.007852067239582539,\n",
      "                -0.01832183264195919,\n",
      "                0.03174200281500816,\n",
      "                0.01371869444847107,\n",
      "                0.028478354215621948,\n",
      "                0.004741273354738951,\n",
      "                -0.020717041566967964,\n",
      "                0.026383867487311363,\n",
      "                -0.0048964545130729675,\n",
      "                -0.03385871276259422,\n",
      "                0.016341758891940117,\n",
      "                0.001669026562012732,\n",
      "                0.0367942750453949,\n",
      "                0.020776156336069107,\n",
      "                0.06335952877998352,\n",
      "                -0.007480324245989323,\n",
      "                -0.010307063348591328,\n",
      "                0.030228067189455032,\n",
      "                -0.08898860961198807,\n",
      "                0.12131591141223907,\n",
      "                0.07001496106386185,\n",
      "                -0.024438519030809402,\n",
      "                -0.0016724559245631099,\n",
      "                -0.056960202753543854,\n",
      "                -0.008062862791121006,\n",
      "                0.013614777475595474,\n",
      "                -0.06264910846948624,\n",
      "                -0.058946676552295685,\n",
      "                -0.05235336720943451,\n",
      "                -0.08395804464817047,\n",
      "                -0.0678526908159256,\n",
      "                -0.005571565590798855,\n",
      "                -0.04398970678448677,\n",
      "                0.03774905577301979,\n",
      "                -0.022055620327591896,\n",
      "                -0.019786832854151726,\n",
      "                0.01462237536907196,\n",
      "                -0.005122958216816187,\n",
      "                0.049840960651636124,\n",
      "                -0.028754673898220062,\n",
      "                0.029741330072283745,\n",
      "                0.017435098066926003,\n",
      "                0.055580463260412216,\n",
      "                0.008729156106710434,\n",
      "                -0.022368917241692543,\n",
      "                -0.023207692429423332,\n",
      "                0.014860105700790882,\n",
      "                -0.042951010167598724,\n",
      "                0.02179393731057644,\n",
      "                0.049821604043245316,\n",
      "                -0.004267735406756401,\n",
      "                0.007884656079113483,\n",
      "                0.005941973999142647,\n",
      "                0.010932724922895432,\n",
      "                0.018064584583044052,\n",
      "                0.03470801189541817,\n",
      "                0.025568069890141487,\n",
      "                0.008204551413655281,\n",
      "                0.023617250844836235,\n",
      "                -0.003310084342956543,\n",
      "                0.016535190865397453,\n",
      "                0.0076090069487690926,\n",
      "                0.02971961721777916,\n",
      "                0.08723320066928864,\n",
      "                -0.04095161706209183,\n",
      "                -0.024238022044301033,\n",
      "                -0.0028092768043279648,\n",
      "                0.031076474115252495,\n",
      "                -0.01991058886051178,\n",
      "                0.0003116297593805939,\n",
      "                -0.034705281257629395,\n",
      "                -0.024978742003440857,\n",
      "                -0.033381007611751556,\n",
      "                0.023188330233097076,\n",
      "                -0.05547437071800232,\n",
      "                0.008105896413326263,\n",
      "                -0.018175166100263596,\n",
      "                -0.025666844099760056,\n",
      "                0.02591213583946228,\n",
      "                0.041252896189689636,\n",
      "                0.02095908112823963,\n",
      "                -0.036489471793174744,\n",
      "                0.045534055680036545,\n",
      "                0.008307071402668953,\n",
      "                0.03367548808455467,\n",
      "                -0.006842798087745905,\n",
      "                0.06395308673381805,\n",
      "                -0.07297595590353012,\n",
      "                0.002876404905691743,\n",
      "                0.03039156273007393,\n",
      "                0.09134627133607864,\n",
      "                0.04427489638328552,\n",
      "                -0.006622613873332739,\n",
      "                -0.005612237378954887,\n",
      "                0.019424844533205032,\n",
      "                -0.020372720435261726,\n",
      "                0.03956928476691246,\n",
      "                -0.04534859582781792,\n",
      "                0.037949088960886,\n",
      "                0.016620205715298653,\n",
      "                -0.015874341130256653,\n",
      "                -0.057326361536979675,\n",
      "                -0.022031918168067932,\n",
      "                -0.025301214307546616,\n",
      "                0.025278722867369652,\n",
      "                0.015378876589238644,\n",
      "                0.05117941275238991,\n",
      "                0.006878442130982876,\n",
      "                -0.019063983112573624,\n",
      "                0.023693734779953957,\n",
      "                0.009539982303977013,\n",
      "                0.01827102340757847,\n",
      "                -0.010863392613828182,\n",
      "                -0.06045226752758026,\n",
      "                0.018247714266180992,\n",
      "                0.015016371384263039,\n",
      "                0.0011455384083092213,\n",
      "                -0.011806670576334,\n",
      "                0.018979676067829132,\n",
      "                -0.004833598155528307,\n",
      "                -0.012291577644646168,\n",
      "                0.019255083054304123,\n",
      "                -0.03265067934989929,\n",
      "                -0.023077690973877907,\n",
      "                -0.006736865732818842,\n",
      "                -0.002071920782327652,\n",
      "                0.026391485705971718,\n",
      "                0.03913000598549843,\n",
      "                0.040459126234054565,\n",
      "                0.006062899250537157,\n",
      "                -0.04942271485924721,\n",
      "                -0.03932031989097595,\n",
      "                -0.023836493492126465,\n",
      "                0.018874680623412132,\n",
      "                -0.04973466321825981,\n",
      "                -0.007242764346301556,\n",
      "                0.009182188659906387,\n",
      "                0.02162923291325569,\n",
      "                0.018296189606189728,\n",
      "                -0.010609588585793972,\n",
      "                0.01213030330836773,\n",
      "                -0.0025192457251250744,\n",
      "                -0.0005845357081852853,\n",
      "                -0.043092887848615646,\n",
      "                0.009210416115820408,\n",
      "                0.04267199710011482,\n",
      "                0.009820367209613323,\n",
      "                0.03534364700317383,\n",
      "                -0.03549111634492874,\n",
      "                -0.024688642472028732,\n",
      "                -0.04016024246811867,\n",
      "                -0.003982850816100836,\n",
      "                0.03931316360831261,\n",
      "                -0.002870261901989579,\n",
      "                -0.005231664981693029,\n",
      "                -0.02176145650446415,\n",
      "                0.030668731778860092,\n",
      "                -0.05012432485818863,\n",
      "                -0.01341717317700386,\n",
      "                0.027700427919626236,\n",
      "                0.010148715227842331,\n",
      "                -0.027252987027168274,\n",
      "                -0.032459806650877,\n",
      "                0.01866304874420166,\n",
      "                0.0689491480588913,\n",
      "                -0.02375674806535244,\n",
      "                0.07240501791238785,\n",
      "                -0.02295684814453125,\n",
      "                -0.006002945359796286,\n",
      "                -0.027831390500068665,\n",
      "                -0.03641098365187645,\n",
      "                -0.07204163819551468,\n",
      "                0.04712032899260521,\n",
      "                -0.009406487457454205,\n",
      "                -0.00023602708824910223,\n",
      "                -0.012416522018611431,\n",
      "                -0.040951959788799286,\n",
      "                0.03166799992322922,\n",
      "                -0.03181497007608414,\n",
      "                0.03471081703901291,\n",
      "                -0.04045380279421806,\n",
      "                0.034989550709724426,\n",
      "                -0.0463961698114872,\n",
      "                0.04023255407810211,\n",
      "                -0.012163184583187103,\n",
      "                0.0035560657270252705,\n",
      "                0.023570531979203224,\n",
      "                -0.011833256110548973,\n",
      "                0.04062855616211891,\n",
      "                0.029315365478396416,\n",
      "                -0.039095208048820496,\n",
      "                0.02784857712686062,\n",
      "                -0.009391025640070438,\n",
      "                -0.007784043904393911,\n",
      "                0.04386991634964943,\n",
      "                0.07762081921100616,\n",
      "                0.046007540076971054,\n",
      "                -0.04043368250131607,\n",
      "                -0.03293861821293831,\n",
      "                -0.019148526713252068,\n",
      "                -0.01991402916610241,\n",
      "                -0.006853508297353983,\n",
      "                -0.02484016679227352,\n",
      "                0.03961578384041786,\n",
      "                0.013065341860055923,\n",
      "                0.03789978846907616,\n",
      "                0.014171255752444267,\n",
      "                -0.001745388493873179,\n",
      "                -0.00198668846860528,\n",
      "                -0.012120606377720833,\n",
      "                -0.002579884137958288,\n",
      "                0.01973351649940014,\n",
      "                -0.03202984854578972,\n",
      "                -0.09200670570135117,\n",
      "                0.04463724419474602,\n",
      "                -0.034317925572395325,\n",
      "                0.034287769347429276,\n",
      "                -0.0018015556270256639,\n",
      "                0.0079501923173666,\n",
      "                0.013661442324519157,\n",
      "                -0.021732693538069725,\n",
      "                -0.008870079182088375,\n",
      "                0.025600090622901917,\n",
      "                0.036471523344516754,\n",
      "                0.04787340387701988,\n",
      "                -0.022407030686736107,\n",
      "                -0.02805890329182148,\n",
      "                -0.06351083517074585,\n",
      "                0.0151875801384449,\n",
      "                0.05776624009013176,\n",
      "                -0.023469366133213043,\n",
      "                0.020084155723452568,\n",
      "                0.025584841147065163,\n",
      "                -0.01753566600382328,\n",
      "                -0.031787388026714325,\n",
      "                0.047396961599588394,\n",
      "                0.024702535942196846,\n",
      "                0.04364173859357834,\n",
      "                -0.012793718837201595,\n",
      "                0.01809663698077202,\n",
      "                -0.042126528918743134,\n",
      "                0.0063933697529137135,\n",
      "                0.013333641923964024,\n",
      "                -0.005193754099309444,\n",
      "                0.04415041208267212,\n",
      "                0.0154829490929842,\n",
      "                -0.010818137787282467,\n",
      "                0.011382438242435455,\n",
      "                0.04251408204436302,\n",
      "                -0.01032558735460043,\n",
      "                -0.027847381308674812,\n",
      "                0.0465427041053772,\n",
      "                0.07020127028226852,\n",
      "                0.010166937485337257,\n",
      "                -0.005678914021700621,\n",
      "                0.01905481517314911,\n",
      "                -0.014080354012548923,\n",
      "                -0.015620879828929901,\n",
      "                -0.005855165421962738,\n",
      "                -0.0008229411323554814,\n",
      "                0.010492355562746525,\n",
      "                0.006110855843871832,\n",
      "                0.06693283468484879,\n",
      "                -0.01925591379404068,\n",
      "                -0.0570887066423893,\n",
      "                0.02182072028517723,\n",
      "                0.008830415084958076,\n",
      "                -0.05359598249197006,\n",
      "                -0.032630957663059235,\n",
      "                -0.021444708108901978,\n",
      "                0.07387198507785797,\n",
      "                -0.0016343139577656984,\n",
      "                0.02302449196577072,\n",
      "                0.01624281145632267,\n",
      "                -0.02946361154317856,\n",
      "                -0.05032894387841225,\n",
      "                -0.0021166843362152576,\n",
      "                0.002801751485094428,\n",
      "                0.004384131636470556,\n",
      "                -0.019827380776405334,\n",
      "                0.0627407431602478,\n",
      "                0.022216318175196648,\n",
      "                0.0016002350021153688,\n",
      "                -0.019655441865324974,\n",
      "                -0.0113168153911829,\n",
      "                0.01304564531892538,\n",
      "                -0.008409669622778893,\n",
      "                0.04467420652508736,\n",
      "                0.0262879841029644,\n",
      "                -0.008478502742946148,\n",
      "                -0.05514432489871979,\n",
      "                -0.06173084303736687,\n",
      "                -0.01055558305233717,\n",
      "                -0.02139444276690483,\n",
      "                0.05992216244339943,\n",
      "                -0.0052911159582436085,\n",
      "                0.04471858963370323,\n",
      "                -0.034054793417453766,\n",
      "                -0.0049622924998402596,\n",
      "                -0.04096011444926262,\n",
      "                0.023729531094431877,\n",
      "                0.017050880938768387,\n",
      "                0.037212345749139786,\n",
      "                -0.018911069259047508,\n",
      "                0.04432971775531769,\n",
      "                0.031155088916420937,\n",
      "                -0.10485612601041794,\n",
      "                -0.05281565338373184,\n",
      "                -0.02112666890025139,\n",
      "                0.02676091156899929,\n",
      "                0.016139158979058266,\n",
      "                -0.0004660341946873814,\n",
      "                -0.021888520568609238,\n",
      "                0.028859885409474373,\n",
      "                0.04655569791793823,\n",
      "                -0.01676889695227146,\n",
      "                -0.003774089040234685,\n",
      "                0.008635259233415127,\n",
      "                -0.013462945818901062,\n",
      "                0.006973509676754475,\n",
      "                -0.023599833250045776,\n",
      "                -0.0010374520206823945,\n",
      "                0.0012236456386744976],\n",
      " 'html_tag': 'p',\n",
      " 'id': '5-89167474-0',\n",
      " 'pageid': 5,\n",
      " 'paragraph_index': 0,\n",
      " 'revid': 89167474,\n",
      " 'section': '__LEAD__',\n",
      " 'text': 'アンパサンド(&, 英語: '\n",
      "         'ampersand)は、並立助詞「...と...」を意味する記号である。ラテン語で「...と...」を表す接続詞 \"et\" '\n",
      "         'の合字を起源とする。現代のフォントでも、Trebuchet MS など一部のフォントでは、\"et\" '\n",
      "         'の合字であることが容易にわかる字形を使用している。',\n",
      " 'title': 'アンパサンド'}\n"
     ]
    }
   ],
   "source": [
    "# 埋め込みを計算した段落データの内容を確認\n",
    "pprint(paragraph_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込みを付与した段落データをディスクに保存\n",
    "paragraph_dataset.save_to_disk(\n",
    "    \"../model/outputs_unsup_simcse/embedded_paragraphs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faissによる最近傍探索を試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(emb_dim)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 段落データの\"embeddings\"フィールドのベクトルからFaissインデックスを構築する\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mparagraph_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_faiss_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:4465\u001b[0m, in \u001b[0;36mDataset.add_faiss_index\u001b[0;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose, dtype)\u001b[0m\n\u001b[1;32m   4415\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[1;32m   4416\u001b[0m \u001b[38;5;124;03mBy default the index is done over the vectors of the specified column.\u001b[39;00m\n\u001b[1;32m   4417\u001b[0m \u001b[38;5;124;03mYou can specify :obj:`device` if you want to run it on GPU (:obj:`device` must be the GPU index).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4462\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   4463\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatted_as(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[column], dtype\u001b[38;5;241m=\u001b[39mdtype):\n\u001b[0;32m-> 4465\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_faiss_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstring_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaiss_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaiss_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/search.py:474\u001b[0m, in \u001b[0;36mIndexableMixin.add_faiss_index\u001b[0;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03mThe index is created using the vectors of the specified column.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03mYou can specify `device` if you want to run it on GPU (`device` must be the GPU index, see more below).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    faiss_verbose (:obj:`bool`, defaults to False): Enable the verbosity of the Faiss index.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    473\u001b[0m index_name \u001b[38;5;241m=\u001b[39m index_name \u001b[38;5;28;01mif\u001b[39;00m index_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[0;32m--> 474\u001b[0m faiss_index \u001b[38;5;241m=\u001b[39m \u001b[43mFaissIndex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_index\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m faiss_index\u001b[38;5;241m.\u001b[39madd_vectors(\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m, column\u001b[38;5;241m=\u001b[39mcolumn, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, train_size\u001b[38;5;241m=\u001b[39mtrain_size, faiss_verbose\u001b[38;5;241m=\u001b[39mfaiss_verbose\n\u001b[1;32m    479\u001b[0m )\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexes[index_name] \u001b[38;5;241m=\u001b[39m faiss_index\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/search.py:245\u001b[0m, in \u001b[0;36mFaissIndex.__init__\u001b[0;34m(self, device, string_factory, metric_type, custom_index)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaiss_index \u001b[38;5;241m=\u001b[39m custom_index\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_faiss:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available."
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# ベクトルの次元数をエンコーダの設定値から取り出す\n",
    "emb_dim = encoder.config.hidden_size\n",
    "# ベクトルの次元数を指定して空のFaissインデックスを作成する\n",
    "index = faiss.IndexFlatIP(emb_dim)\n",
    "# 段落データの\"embeddings\"フィールドのベクトルからFaissインデックスを構築する\n",
    "paragraph_dataset.add_faiss_index(\"embeddings\", custom_index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
