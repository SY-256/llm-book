{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ç¬¬8ç«  æ–‡åŸ‹ã‚è¾¼ã¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–‡åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ•™å¸«ãªã—SimCSEã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®è¨­å®š\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset jawiki-sentences (/root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "unsup_train_dataset = load_dataset(\n",
    "    \"llm-book/jawiki-sentences\", split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 24387500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´ã‚»ãƒƒãƒˆã®å½¢å¼ã¨äº‹ä¾‹æ•°ã‚’ç¢ºèª\n",
    "print(unsup_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰(&, è‹±èª: ampersand)ã¯ã€ä¸¦ç«‹åŠ©è©ã€Œ...ã¨...ã€ã‚’æ„å‘³ã™ã‚‹è¨˜å·ã§ã‚ã‚‹ã€‚\n",
      "1 ãƒ©ãƒ†ãƒ³èªã§ã€Œ...ã¨...ã€ã‚’è¡¨ã™æ¥ç¶šè© \"et\" ã®åˆå­—ã‚’èµ·æºã¨ã™ã‚‹ã€‚\n",
      "2 ç¾ä»£ã®ãƒ•ã‚©ãƒ³ãƒˆã§ã‚‚ã€Trebuchet MS ãªã©ä¸€éƒ¨ã®ãƒ•ã‚©ãƒ³ãƒˆã§ã¯ã€\"et\" ã®åˆå­—ã§ã‚ã‚‹ã“ã¨ãŒå®¹æ˜“ã«ã‚ã‹ã‚‹å­—å½¢ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã€‚\n",
      "3 è‹±èªã§æ•™è‚²ã‚’è¡Œã†å­¦æ ¡ã§ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‚’å¾©å”±ã™ã‚‹å ´åˆã€ãã®æ–‡å­—è‡ªä½“ãŒå˜èªã¨ãªã‚‹æ–‡å­—(\"A\", \"I\", ã‹ã¤ã¦ã¯ \"O\" ã‚‚)ã«ã¤ã„ã¦ã¯ã€ä¼çµ±çš„ã«ãƒ©ãƒ†ãƒ³èªã® per se(ãã‚Œè‡ªä½“)ã‚’ç”¨ã„ã¦ \"A per se A\" ã®ã‚ˆã†ã«å”±ãˆã‚‰ã‚Œã¦ã„ãŸã€‚\n",
      "4 ã¾ãŸã€ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®æœ€å¾Œã«ã€27ç•ªç›®ã®æ–‡å­—ã®ã‚ˆã†ã« \"&\" ã‚’åŠ ãˆã‚‹ã“ã¨ã‚‚åºƒãè¡Œã‚ã‚Œã¦ã„ãŸã€‚\n",
      "5 \"&\" ã¯ãƒ©ãƒ†ãƒ³èªã§ et ã¨èª­ã¾ã‚Œã¦ã„ãŸãŒã€å¾Œã«è‹±èªã§ and ã¨èª­ã¾ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚\n",
      "6 çµæœã¨ã—ã¦ã€ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®å¾©å”±ã®æœ€å¾Œã¯ \"X, Y, Z, and per se and\" ã¨ã„ã†å½¢ã«ãªã£ãŸã€‚\n",
      "7 ã“ã®æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ã‚ºãŒç¹°ã‚Šè¿”ã•ã‚Œã‚‹ã†ã¡ã« \"ampersand\" ã¨è¨›ã£ã¦ã„ãã€ã“ã®è¨€è‘‰ã¯1837å¹´ã¾ã§ã«ã¯è‹±èªã®ä¸€èˆ¬çš„ãªèªæ³•ã¨ãªã£ãŸã€‚\n",
      "8 ã‚¢ãƒ³ãƒ‰ãƒ¬=ãƒãƒªãƒ»ã‚¢ãƒ³ãƒšãƒ¼ãƒ«ãŒã“ã®è¨˜å·ã‚’è‡ªèº«ã®è‘—ä½œã§ä½¿ã„ã€ã“ã‚ŒãŒåºƒãèª­ã¾ã‚ŒãŸãŸã‚ã€ã“ã®è¨˜å·ãŒ \"AmpÃ¨re's and\" ã¨å‘¼ã°ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã¨ã„ã†èª¤ã£ãŸèªæºä¿—èª¬ãŒã‚ã‚‹ã€‚\n",
      "9 ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã®èµ·æºã¯1ä¸–ç´€ã®å¤ãƒ­ãƒ¼ãƒç­†è¨˜ä½“ã«ã¾ã§é¡ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
      "10 å¤ãƒ­ãƒ¼ãƒç­†è¨˜ä½“ã§ã¯ã€E ã¨ T ã¯ã—ã°ã—ã°åˆå­—ã¨ã—ã¦ç¹‹ã’ã¦æ›¸ã‹ã‚Œã¦ã„ãŸ(å·¦å›³ã€Œã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã®å¤‰é·ã€ã®å­—å½¢1)ã€‚ãã‚Œã«ç¶šãã€æµéº—ã•ã‚’å¢—ã—ãŸæ–°ãƒ­ãƒ¼ãƒç­†è¨˜ä½“ã§ã¯ã€æ§˜ã€…ãªåˆå­—ãŒæ¥µã‚ã¦é »ç¹ã«ä½¿ã‚ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚\n",
      "11 å­—å½¢2ã¨3ã¯4ä¸–ç´€ä¸­é ƒã«ãŠã‘ã‚‹ et ã®åˆå­—ã®ä¾‹ã§ã‚ã‚‹ã€‚\n",
      "12 ãã®å¾Œã€9ä¸–ç´€ã®ã‚«ãƒ­ãƒªãƒ³ã‚°å°æ–‡å­—ä½“ã«è‡³ã‚‹ãƒ©ãƒ†ãƒ³æ–‡å­—ã®å¤‰é·ã®éç¨‹ã§ã€åˆå­—ã®ä½¿ç”¨ã¯ä¸€èˆ¬ã«ã¯å»ƒã‚Œã¦ã„ã£ãŸã€‚\n",
      "13 ã—ã‹ã—ã€et ã®åˆå­—ã¯ä½¿ã‚ã‚Œç¶šã‘ã€æ¬¡ç¬¬ã«å…ƒã®æ–‡å­—ãŒã‚ã‹ã‚Šã«ãã„å­—å½¢ã«å¤‰åŒ–ã—ã¦ã„ã£ãŸ(å­—å½¢4ã‹ã‚‰6)ã€‚\n",
      "14 ç¾ä»£ã®ã‚¤ã‚¿ãƒªãƒƒã‚¯ä½“ã®ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã¯ã€ãƒ«ãƒã‚µãƒ³ã‚¹æœŸã«ç™ºå±•ã—ãŸç­†è¨˜ä½“ã§ã® et ã®åˆå­—ã«é¡ã‚‹ã€‚\n",
      "15 1455å¹´ã®ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã«ãŠã‘ã‚‹å°åˆ·æŠ€è¡“ã®ç™ºæ˜ä»¥é™ã€å°åˆ·æ¥­è€…ã¯ã‚¤ã‚¿ãƒªãƒƒã‚¯ä½“ã¨ãƒ­ãƒ¼ãƒç­†è¨˜ä½“ã®ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã®ä¸¡æ–¹ã‚’å¤šç”¨ã™ã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚\n",
      "16 ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã®ãƒ«ãƒ¼ãƒ„ã¯ãƒ­ãƒ¼ãƒæ™‚ä»£ã«é¡ã‚‹ãŸã‚ã€ãƒ©ãƒ†ãƒ³ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹å¤šãã®è¨€èªã§ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ãŒä½¿ç”¨ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚\n",
      "17 ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã¯ã—ã°ã—ã°ãƒ©ãƒ†ãƒ³ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®æœ€å¾Œã®æ–‡å­—ã¨ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã£ãŸã€‚\n",
      "18 ä¾‹ãˆã°1011å¹´ã®Byrhtferthã®æ–‡å­—è¡¨ãŒãã®ä¾‹ã§ã‚ã‚‹ã€‚\n",
      "19 åŒæ§˜ã«ã€\"&\" ã¯è‹±èªã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®27ç•ªç›®ã®æ–‡å­—ã¨ã•ã‚Œã€ã‚¢ãƒ¡ãƒªã‚«åˆè¡†å›½ã‚„ãã®ä»–ã®åœ°åŸŸã§ã‚‚ã€å­ä¾›é”ã¯ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã¯ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®æœ€å¾Œã®æ–‡å­—ã ã¨æ•™ãˆã‚‰ã‚Œã¦ã„ãŸã€‚\n",
      "20 1863å¹´ã® M. B. Moore ã®è‘—æ›¸ The Dixie Primer, for the Little Folks ã«ãã®ä¸€ä¾‹ã‚’è¦‹ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚\n",
      "21 ã‚¸ãƒ§ãƒ¼ã‚¸ãƒ»ã‚¨ãƒªã‚ªãƒƒãƒˆã¯ã€1859å¹´ã«ç™ºè¡¨ã—ãŸå°èª¬ã€Œã‚¢ãƒ€ãƒ ãƒ»ãƒ“ãƒ¼ãƒ‰(è‹±èªç‰ˆ)ã€ã®ä¸­ã§ã€Jacob Storey ã«æ¬¡ã®ã‚»ãƒªãƒ•ã‚’èªã‚‰ã›ã¦ã„ã‚‹ã€‚\n",
      "22 \"He thought it [Z] had only been put to finish off th' alphabet like; though ampusand would ha' done as well, for what he could see.\" ã‚ˆãçŸ¥ã‚‰ã‚ŒãŸç«¥è¬¡ã® Apple Pie ABC ã¯ \"X, Y, Z, and ampersand, All wished for a piece in hand\" ã¨ã„ã†æ­Œè©ã§ç· ã‚ããã‚‰ã‚Œã‚‹ã€‚\n",
      "23 ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã¯ã€ãƒ†ã‚£ãƒ­å¼è¨˜å·ã® et (\"âŠ\", Unicode U+204A) ã¨ã¯åˆ¥ã®ã‚‚ã®ã§ã‚ã‚‹ã€‚\n",
      "24 ãƒ†ã‚£ãƒ­å¼è¨˜å·ã® et ã¯ã€ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã¨æ„å‘³ã¯åŒã˜ã ãŒæ•°å­—ã®ã€Œ7ã€ã«ä¼¼ãŸå½¢ã®è¨˜å·ã§ã‚ã‚‹ã€‚\n",
      "25 ä¸¡è€…ã¯ã¨ã‚‚ã«å¤ä»£ã‹ã‚‰ä½¿ç”¨ã•ã‚Œã€ä¸­ä¸–ã‚’é€šã—ã¦ãƒ©ãƒ†ãƒ³èªã® et ã‚’è¡¨ã™ãŸã‚ã«ä½¿ç”¨ã•ã‚ŒãŸã€‚\n",
      "26 ã—ã‹ã—ã€ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã¨ãƒ†ã‚£ãƒ­å¼è¨˜å·ã® et ã¯ãã‚Œãã‚Œç‹¬ç«‹ã«ç™ºæ˜ã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚‹ã€‚\n",
      "27 ãƒ©ãƒ†ãƒ³æ–‡å­—ã‹ã‚‰ç™ºå±•ã—ãŸå¤ã‚¢ã‚¤ãƒ«ãƒ©ãƒ³ãƒ‰èªã®æ–‡å­—ã§ã¯ã€ã‚¢ã‚¤ãƒ«ãƒ©ãƒ³ãƒ‰èªã® agus(ã€Œ...ã¨...ã€)ã‚’è¡¨ã™ãŸã‚ã«ãƒ†ã‚£ãƒ­å¼è¨˜å·ã® et ãŒä½¿ç”¨ã•ã‚Œã¦ã„ãŸã€‚\n",
      "28 ä»Šæ—¥ã¯ã‚²ãƒ¼ãƒ«æ–‡å­—ã®ä¸€éƒ¨ã¨ã—ã¦ä¸»ã«è£…é£¾çš„ãªç›®çš„ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
      "29 ã“ã®æ–‡å­—ã¯ã‚¢ã‚¤ãƒ«ãƒ©ãƒ³ãƒ‰ã«ãŠã‘ã‚‹ã‚­ãƒªã‚¹ãƒˆæ•™æ™‚ä»£åˆæœŸã«ä¿®é“é™¢ã®å½±éŸ¿ã«ã‚ˆã£ã¦æ›¸ãæ–‡å­—ã«åŠ ã‚ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚‹ã€‚\n",
      "30 æ—¥å¸¸çš„ãªæ‰‹æ›¸ãã®å ´åˆã€æ¬§ç±³ã§ã¯å°æ–‡å­—ã® Îµ(ã‚¨ãƒ—ã‚·ãƒ­ãƒ³)ã‚’å¤§ããã—ãŸã‚‚ã®(ã‚ã‚‹ã„ã¯æ•°å­—ã® \"3\" ã®é¡æ–‡å­—)ã«ç¸¦ç·šã‚’åŠ ãˆãŸå½¢ã®å˜ç´”åŒ–ã•ã‚ŒãŸã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ãŒã—ã°ã—ã°ä½¿ã‚ã‚Œã‚‹ã€‚\n",
      "31 ã¾ãŸã€ã‚¨ãƒ—ã‚·ãƒ­ãƒ³ã®ä¸Šä¸‹ã«ç¸¦ç·šã¾ãŸã¯ç‚¹ã‚’ä»˜ã‘ãŸã‚‚ã®ã‚‚ã—ã°ã—ã°ä½¿ã‚ã‚Œã‚‹ã€‚\n",
      "32 ãã ã‘ãŸç”¨æ³•ã¨ã—ã¦ã€ãƒ—ãƒ©ã‚¹è¨˜å·(\"+\", ã“ã®è¨˜å·ã‚‚ã¾ãŸ et ã®åˆå­—ã§ã‚ã‚‹)ãŒã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã®ä»£ã‚ã‚Šã«ä½¿ã‚ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚\n",
      "33 ã¾ãŸã€ãƒ—ãƒ©ã‚¹è¨˜å·ã«è¼ªã‚’é‡ã­ãŸã‚ˆã†ãªã€ç„¡å£°æ­¯èŒå´é¢æ‘©æ“¦éŸ³ã‚’ç¤ºã™ç™ºéŸ³è¨˜å·ã€Œ[É¬]ã€ã®ã‚ˆã†ãªã‚‚ã®ãŒä½¿ã‚ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚\n",
      "34 ãƒ†ã‚£ãƒ­ã®é€Ÿè¨˜ã«ã¯ã€Œetã€ã‚’è¡¨ã™ãŸã‚ã®ã€ŒâŠã€(U+204A Tironian sign et)ãŒã‚ã‚‹ã€‚\n",
      "35 ã“ã®æ–‡å­—ã¯ãƒ‰ã‚¤ãƒ„ã®ãƒ•ãƒ©ã‚¯ãƒˆã‚¥ãƒ¼ãƒ«ã§ä½¿ã‚ã‚ŒãŸã»ã‹ã€ã‚²ãƒ¼ãƒ«æ–‡å­—ã§ã‚‚ä½¿ç”¨ã•ã‚Œã‚‹ã€‚\n",
      "36 ã‚®ãƒªã‚·ã‚¢æ–‡å­—ã§ã¯ã€Œ......ã¨ã€ã‚’æ„å‘³ã™ã‚‹ÎºÎ±Î¹ã‚’è¡¨ã™ãŸã‚ã®åˆå­—ã¨ã—ã¦ã€ŒÏ—ã€(U+03D7 Greek kai symbol)ãŒä½¿ã‚ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚\n",
      "37 ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã¯ã€C ãªã©å¤šæ•°ã®è¨€èªã§ AND æ¼”ç®—å­ã¨ã—ã¦ç”¨ã„ã‚‰ã‚Œã‚‹ã€‚\n",
      "38 PHPã§ã¯ã€å¤‰æ•°å®£è¨€è¨˜å·($)ã®ç›´å‰ã«è¨˜è¿°ã™ã‚‹ã“ã¨ã§ã€å‚ç…§æ¸¡ã—ã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚\n",
      "39 BASIC ç³»åˆ—ã®è¨€èªã§ã¯æ–‡å­—åˆ—ã®é€£çµæ¼”ç®—å­ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã€‚\n",
      "40 \"foo\" & \"bar\" ã¯ \"foobar\" ã‚’è¿”ã™ã€‚\n",
      "41 ã¾ãŸã€ä¸»ã«ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆç³»ã§ã¯æ•´æ•°ã®åå…­é€²è¡¨è¨˜ã« &h ã‚’ç”¨ã„ã€&h0F (åé€²ã§15)ã®ã‚ˆã†ã«è¡¨ç¾ã™ã‚‹ã€‚\n",
      "42 SGMLã€XMLã€HTMLã§ã¯ã€ã‚¢ãƒ³ãƒ‘ã‚µãƒ³ãƒ‰ã‚’ä½¿ã£ã¦SGMLå®Ÿä½“ã‚’å‚ç…§ã™ã‚‹ã€‚\n",
      "43 \n",
      "44 è¨€èª(ã’ã‚“ã”)ã¯ã€ç‹­ç¾©ã«ã¯ã€Œå£°ã«ã‚ˆã‚‹è¨˜å·ã®ä½“ç³»ã€ã‚’ã„ã†ã€‚\n",
      "45 åºƒè¾è‹‘ã‚„å¤§è¾æ³‰ã«ã¯æ¬¡ã®ã‚ˆã†ã«è§£èª¬ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
      "46 ã€æ—¥æœ¬å¤§ç™¾ç§‘å…¨æ›¸ã€ã§ã¯ã€ã€Œè¨€èªã€ã¨ã„ã†èªã¯å¤šç¾©ã§ã‚ã‚‹ã€ã¨è§£èª¬ã•ã‚Œã€å¤§è„³ã®è¨€èªä¸­æ¢(è‹±èªç‰ˆ)ã«è“„ãˆã‚‰ã‚ŒãŸã€Šèªå½™ã¨æ–‡æ³•è¦å‰‡ã®ä½“ç³»ã€‹ã‚’æŒ‡ã™ã“ã¨ã‚‚ã‚ã‚Šã€ãã®ä½“ç³»ã‚’ç”¨ã„ã‚‹èƒ½åŠ›ã¨ã—ã¦ã¨ã‚‰ãˆã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€ã¨è§£èª¬ã•ã‚Œã€ä¸€æ–¹ã§ã¯ã€æŠ½è±¡çš„ã«ã€Œã™ã¹ã¦ã®äººé–“ãŒå…±æœ‰ã™ã‚‹è¨€èªèƒ½åŠ›ã€ã‚’æŒ‡ã™ã“ã¨ã‚‚ã‚ã‚Šã€ã€Œå€‹ã€…ã®å€‹åˆ¥è¨€èªã€ã‚’æŒ‡ã™ã“ã¨ã‚‚ã‚ã‚‹ã€ã¨è§£èª¬ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
      "47 åºƒç¾©ã®è¨€èªã«ã¯ã€verbalãªã‚‚ã®ã¨non-verbalãªã‚‚ã®(å„ç¨®è¨˜å·ã€ã‚¢ã‚¤ã‚³ãƒ³ã€å›³å½¢ã€ãƒœãƒ‡ã‚£ãƒ¼ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ç­‰)ã®ä¸¡æ–¹ã‚’å«ã¿ã€æ—¥å¸¸ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ç‹­ç¾©ã®è¨€èªè¡¨ç¾ã«èº«æŒ¯ã‚Šã€æ‰‹æŒ¯ã‚Šã€å›³ç¤ºã€æ“¬éŸ³ç­‰ã‚‚åŠ ãˆã¦è¡¨ç¾ã•ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚\n",
      "48 è¨€èªã¯ã€äººé–“ãŒç”¨ã„ã‚‹æ„å¿—ä¼é”æ‰‹æ®µã§ã‚ã‚Šã€ç¤¾ä¼šé›†å›£å†…ã§å½¢æˆç¿’å¾—ã•ã‚Œã€æ„å¿—ã‚’ç›¸äº’ã«ä¼é”ã™ã‚‹ã“ã¨ã‚„ã€æŠ½è±¡çš„ãªæ€è€ƒã‚’å¯èƒ½ã«ã—ã€çµæœã¨ã—ã¦äººé–“ã®ç¤¾ä¼šçš„æ´»å‹•ã‚„æ–‡åŒ–çš„æ´»å‹•ã‚’æ”¯ãˆã¦ã„ã‚‹ã€‚\n",
      "49 è¨€èªã«ã¯ã€æ–‡åŒ–ã®ç‰¹å¾´ãŒç¹”ã‚Šè¾¼ã¾ã‚Œã¦ãŠã‚Šã€å…±åŒä½“ã§ç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹è¨€èªã®ç¿’å¾—ã‚’ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ãã®å…±åŒä½“ã§ã®ç¤¾ä¼šçš„å­¦ç¿’ã€ãŠã‚ˆã³äººæ ¼ã®å½¢æˆã‚’ã—ã¦ã„ãã“ã¨ã«ãªã‚‹ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´ã‚»ãƒƒãƒˆã®ä¸­èº«ã‚’ç¢ºèªã™ã‚‹\n",
    "for i, text in enumerate(unsup_train_dataset[:50][\"text\"]):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-ecc223a34f1ffe88.arrow\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´ã‚»ãƒƒãƒˆã‹ã‚‰ç©ºç™½è¡Œã‚’äº‹ä¾‹ã‚’å‰Šé™¤\n",
    "unsup_train_dataset = unsup_train_dataset.filter(\n",
    "    lambda example: example[\"text\"].strip() != \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-e9e6caaca5ef995f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-9484a91dba79bc01.arrow\n"
     ]
    }
   ],
   "source": [
    "# è¨“ç·´ã‚»ãƒƒãƒˆã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã€æœ€åˆã®100ä¸‡äº‹ä¾‹ã‚’å–ã‚Šå‡ºã™\n",
    "unsup_train_dataset = unsup_train_dataset.shuffle().select(\n",
    "    range(1000000)\n",
    ")\n",
    "# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ä¸‹ã‚’é˜²ããŸã‚ã«ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã•ã‚ŒãŸçŠ¶æ…‹ã®è¨“ç·´ã‚»ãƒƒãƒˆã‚’\n",
    "# ãƒ‡ã‚£ã‚¹ã‚¯ã«æ›¸ãè¾¼ã‚€\n",
    "unsup_train_dataset = unsup_train_dataset.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# å‰å‡¦ç†å¾Œã®è¨“ç·´ã‚»ãƒƒãƒˆã®å½¢å¼ã¨äº‹ä¾‹æ•°ã‚’ç¢ºèª\n",
    "print(unsup_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2005å¹´ã®æ™‚ç‚¹ã§ã€10,000äººã‚‚ã®ã‚¦ã‚ºãƒ™ã‚­ã‚¹ã‚¿ãƒ³äººãŒéŸ“å›½ã§ã®åŠ´åƒã«å¾“äº‹ã—ã¦ãŠã‚Šã€ãã®å¤§éƒ¨åˆ†ãŒé«˜éº—äººã§ã‚ã‚‹ã€‚\n",
      "1 å°å­¦5å¹´ç”Ÿ(11æ­³)ã®æ™‚ã‹ã‚‰èŠ¸èƒ½æ´»å‹•ã‚’é–‹å§‹ã€‚\n",
      "2 i Ä§ d d t | Ïˆ ( t ) âŸ© = L ^ | Ïˆ ( t ) âŸ© {\\displaystyle i\\hbar {\\frac {d}{dt}}|\\psi (t)\\rangle ={\\hat {L}}|\\psi (t)\\rangle }\n",
      "3 å®‰åœŸå®—è«–(ã‚ã¥ã¡ã—ã‚…ã†ã‚ã‚“)ã¯ã€1579å¹´(å¤©æ­£7å¹´)ã€å®‰åœŸåŸä¸‹ã®æµ„å³é™¢ã§è¡Œã‚ã‚ŒãŸæµ„åœŸå®—ã¨æ³•è¯å®—ã®å®—è«–ã€‚\n",
      "4 1927å¹´ ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢é¸æ‰‹æ¨©(1927ã­ã‚“ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢ã›ã‚“ã—ã‚…ã‘ã‚“ã€1927 Australian Championships)ã«é–¢ã™ã‚‹è¨˜äº‹ã€‚\n",
      "5 ã•ã‚‰ã«ãƒãƒƒãƒ—ä¸Šã§æœ€å¤§8ã¤ã¾ã§ã—ã‹å»ºç¯‰ã§ããš(å¸ä»¤å®˜ã‚¢ãƒ“ãƒªãƒ†ã‚£ã®â€è§£ä½“â€ã‹è¨­ç½®ã—ãŸãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼è‡ªã‚‰ãŒå‡ºå‘ã„ã¦è§£ä½“ã™ã‚‹å¿…è¦ãŒã‚ã‚‹)\n",
      "6 ç‰¹ã«èª‰æ·³ãŒ1827å¹´ã‹ã‚‰ä½œæˆã—ãŸã€å¤ç“¦è­œã€ã¯ç•¿å†…ã§600ç‚¹ä»¥ä¸Šã®æ‹“æœ¬ã‚’è’é›†ã—ã€ç“¦å½“æ–‡æ§˜ã«ç€ç›®ã—ãŸã†ãˆã§ç·¨å¹´ã‚’è©¦ã¿ã¦ã„ã‚‹ã€‚\n",
      "7 ãƒãƒ«ã‚¯ã‚¹ä¸»ç¾©è€…ã‚’åºƒè¨€ã—ã€ãƒ¡ã‚­ã‚·ã‚³å…±ç”£å…šã®æ•µã§ã‚ã‚Šå‘³æ–¹ã§ã‚ã£ãŸã€‚\n",
      "8 ICHILLIN'(ã‚¢ã‚¤ãƒãƒªãƒ³ã€æœ: ì•„ì´ì¹ ë¦°)ã¯ã€éŸ“å›½ã®7äººçµ„å¥³æ€§ã‚¢ã‚¤ãƒ‰ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã€‚\n",
      "9 ãƒãƒ¼ã‚¯VIã¯1983å¹´ã«ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚¯ãƒ«ã‚’çµ‚äº†ã—ã€1984å¹´ã®ãƒãƒ¼ã‚¯VII(è‹±èªç‰ˆ)ã¯ãƒ•ãƒ«ã‚µã‚¤ã‚ºã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ’¤é€€ã—ã€ãƒãƒ¼ã‚¯ã‚·ãƒªãƒ¼ã‚ºã¯ç•°ãªã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ç§»è¡Œã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "# å‰å‡¦ç†å¾Œã®è¨“ç·´ã‚»ãƒƒãƒˆã®å†…å®¹ã‚’ç¢ºèª\n",
    "for i, text in enumerate(unsup_train_dataset[:10][\"text\"]):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset jglue (/root/.cache/huggingface/datasets/llm-book___jglue/JSTS/1.1.0/b394a8dbefe82fb1dc2724c1eb79bb1ea3062df2037f91a69a27c089f3ff685f)\n",
      "Reusing dataset jglue (/root/.cache/huggingface/datasets/llm-book___jglue/JSTS/1.1.0/b394a8dbefe82fb1dc2724c1eb79bb1ea3062df2037f91a69a27c089f3ff685f)\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Hubã®llm-book/JGLUEã®ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰\n",
    "# JSTSãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨“ç·´ã‚»ãƒƒãƒˆã¨æ¤œè¨¼ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€\n",
    "# ãã‚Œãã‚Œã‚’SimCSEã®æ¤œè¨¼ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨ã™ã‚‹\n",
    "valid_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JSTS\", split=\"train\"\n",
    ")\n",
    "test_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JSTS\", split=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨collateé–¢æ•°ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "def unsup_train_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"æ•™å¸«ãªã—SimCSEã®è¨“ç·´ã‚»ãƒƒãƒˆã®ãƒŸãƒ‹ãƒãƒƒãƒã‚’ä½œæˆ\"\"\"\n",
    "    # ãƒŸãƒ‹ãƒãƒƒãƒã«å«ã¾ã‚Œã‚‹æ–‡ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’é©ç”¨ã™ã‚‹\n",
    "    tokenized_texts = tokenizer(\n",
    "        [example[\"text\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # æ–‡ã¨æ–‡ã®é¡ä¼¼åº¦è¡Œåˆ—ã«ãŠã‘ã‚‹æ­£ä¾‹ãƒšã‚¢ã®ä½ç½®ã‚’ç¤ºã™Tensorã‚’ä½œæˆã™ã‚‹\n",
    "    # è¡Œåˆ—ã®iè¡Œç›®ã®äº‹ä¾‹ï¼ˆæ–‡ï¼‰ã«å¯¾ã—ã¦iåˆ—ç›®ã®äº‹ä¾‹ï¼ˆæ–‡ï¼‰ã¨ã®çµ„ãŒæ­£ä¾‹ãƒšã‚¢ã¨ãªã‚‹\n",
    "    labels = torch.arange(len(examples))\n",
    "\n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_texts,\n",
    "        \"tokenized_texts_2\": tokenized_texts,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"SimCSEã®æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ãƒŸãƒ‹ãƒãƒƒãƒã‚’ä½œæˆ\"\"\"\n",
    "    # ãƒŸãƒ‹ãƒãƒƒãƒã®æ–‡ãƒšã‚¢ã«å«ã¾ã‚Œã‚‹æ–‡ï¼ˆæ–‡1ã¨æ–‡2ï¼‰ã®ãã‚Œãã‚Œã«\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’é©ç”¨ã™ã‚‹\n",
    "    tokenized_texts_1 = tokenizer(\n",
    "        [example[\"sentence1\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized_texts_2 = tokenizer(\n",
    "        [example[\"sentence2\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # æ–‡1ã¨æ–‡2ã®é¡ä¼¼åº¦è¡Œåˆ—ã«ãŠã‘ã‚‹æ­£ä¾‹ãƒšã‚¢ã®ä½ç½®ã‚’ç¤ºã™Tensorã‚’ä½œæˆã™ã‚‹\n",
    "    # è¡Œåˆ—ã®iè¡Œç›®ã®äº‹ä¾‹ï¼ˆæ–‡1ï¼‰ã«å¯¾ã—ã¦\n",
    "    # iåˆ—ç›®ã®äº‹ä¾‹ï¼ˆæ–‡2ï¼‰ã¨ã®çµ„ãŒæ­£ä¾‹ãƒšã‚¢ã¨ãªã‚‹\n",
    "    labels = torch.arange(len(examples))\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ä»˜ä¸ã•ã‚ŒãŸé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã®Tensorã‚’ä½œæˆã™ã‚‹\n",
    "    label_scores = torch.tensor(\n",
    "        [example[\"label\"] for example in examples]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_texts_1,\n",
    "        \"tokenized_texts_2\": tokenized_texts_2,\n",
    "        \"labels\": labels,\n",
    "        \"label_scores\": label_scores,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ¢ãƒ‡ãƒ«æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from transformers.utils import ModelOutput\n",
    "\n",
    "class SimCSEModel(nn.Module):\n",
    "    \"\"\"SimCSEã®ãƒ¢ãƒ‡ãƒ«\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name: str,\n",
    "        mlp_only_train: bool = False,\n",
    "        temperature: float = 0.05,\n",
    "    ):\n",
    "        \"\"\"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªä¸Šã«éš£æ¥ã—ãŸå½¢ã§é…ç½®\n",
    "        # ã“ã‚Œã‚’å®Ÿè¡Œã—ãªã„å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹\n",
    "        # for param in model.parameters():\n",
    "        #     param.data = param.data.contiguous()\n",
    "        # MLPå±¤ã®æ¬¡å…ƒæ•°\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        # MLPå±¤ã®ç·šå½¢å±¤\n",
    "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        # MLPå±¤ã®æ´»æ€§åŒ–é–¢æ•°\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # MLPå±¤ã«ã‚ˆã‚‹å¤‰æ›ã‚’è¨“ç·´æ™‚ã«ã®ã¿é©ç”¨ã™ã‚‹ã‚ˆã†è¨­å®šã™ã‚‹ãƒ•ãƒ©ã‚°\n",
    "        self.mlp_only_train = mlp_only_train\n",
    "        # äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã®è¨ˆç®—æ™‚ã«ä½¿ç”¨ã™ã‚‹æ¸©åº¦\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def encode_texts(self, tokenized_texts: BatchEncoding) -> Tensor:\n",
    "        \"\"\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ç”¨ã„ã¦æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›\"\"\"\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã•ã‚ŒãŸæ–‡ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«å…¥åŠ›ã™ã‚‹\n",
    "        encoded_texts = self.encoder(**tokenized_texts)\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚å±¤ã®å‡ºåŠ›ï¼ˆlast_hidden_stateï¼‰ã®\n",
    "        # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ0ç•ªç›®ã®ä½ç½®ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–ã‚Šå‡ºã™\n",
    "        encoded_texts = encoded_texts.last_hidden_state[:, 0]\n",
    "\n",
    "        # self.mlp_only_trainã®ãƒ•ãƒ©ã‚°ãŒTrueã«è¨­å®šã•ã‚Œã¦ã„ã¦\n",
    "        # ã‹ã¤è¨“ç·´æ™‚ã§ãªã„å ´åˆã€MLPå±¤ã®å¤‰æ›ã‚’é©ç”¨ã›ãšã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™\n",
    "        if self.mlp_only_train and not self.training:\n",
    "            return encoded_texts\n",
    "\n",
    "        # MLPå±¤ã«ã‚ˆã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã®å¤‰æ›ã‚’è¡Œã†\n",
    "        encoded_texts = self.dense(encoded_texts)\n",
    "        encoded_texts = self.activation(encoded_texts)\n",
    "\n",
    "        return encoded_texts\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokenized_texts_1: BatchEncoding,\n",
    "        tokenized_texts_2: BatchEncoding,\n",
    "        labels: Tensor,\n",
    "        label_scores: Tensor | None = None,\n",
    "    ) -> ModelOutput:\n",
    "        \"\"\"ãƒ¢ãƒ‡ãƒ«ã®å‰å‘ãè¨ˆç®—ã‚’å®šç¾©\"\"\"\n",
    "        # æ–‡ãƒšã‚¢ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹\n",
    "        encoded_texts_1 = self.encode_texts(tokenized_texts_1)\n",
    "        encoded_texts_2 = self.encode_texts(tokenized_texts_2)\n",
    "\n",
    "        # æ–‡ãƒšã‚¢ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’ä½œæˆã™ã‚‹\n",
    "        sim_matrix = F.cosine_similarity(\n",
    "            encoded_texts_1.unsqueeze(1),\n",
    "            encoded_texts_2.unsqueeze(0),\n",
    "            dim=2,\n",
    "        )\n",
    "\n",
    "        # äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’æ±‚ã‚ã‚‹\n",
    "        loss = F.cross_entropy(sim_matrix / self.temperature, labels)\n",
    "\n",
    "        # æ€§èƒ½è©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹ãŸã‚ã€æ­£ä¾‹ãƒšã‚¢ã«å¯¾ã™ã‚‹ã‚¹ã‚³ã‚¢ã‚’é¡ä¼¼åº¦è¡Œåˆ—ã‹ã‚‰å–ã‚Šå‡ºã™\n",
    "        positive_mask = F.one_hot(labels, sim_matrix.size(1)).bool()\n",
    "        positive_scores = torch.masked_select(\n",
    "            sim_matrix, positive_mask\n",
    "        )\n",
    "\n",
    "        return ModelOutput(loss=loss, scores=positive_scores)\n",
    "\n",
    "# æ•™å¸«ãªã—SimCSEã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "unsup_model = SimCSEModel(base_model_name, mlp_only_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainerã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã—ãŸã‚¹ã‚³ã‚¢ã¨è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚³ã‚¢ã®\n",
    "    ã‚¹ãƒ”ã‚¢ãƒãƒ³ã®é †ä½ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—\n",
    "    \"\"\"\n",
    "    scores = p.predictions\n",
    "    labels, label_scores = p.label_ids\n",
    "\n",
    "    spearman = spearmanr(scores, label_scores).correlation\n",
    "\n",
    "    return {\"spearman\": spearman}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# æ•™å¸«ãªã—SimCSEã®è¨“ç·´ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹\n",
    "unsup_training_args = TrainingArguments(\n",
    "    output_dir=\"../model/outputs_unsup_simcse\",  # çµæœã®ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€\n",
    "    per_device_train_batch_size=256,  # è¨“ç·´æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    per_device_eval_batch_size=256,  # è©•ä¾¡æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    learning_rate=3e-5,  # å­¦ç¿’ç‡\n",
    "    num_train_epochs=1,  # è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°\n",
    "    evaluation_strategy=\"steps\",  # æ¤œè¨¼ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹è©•ä¾¡ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°\n",
    "    eval_steps=250,  # æ¤œè¨¼ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹è©•ä¾¡ã‚’è¡Œã†è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®é–“éš”\n",
    "    logging_steps=250,  # ãƒ­ã‚®ãƒ³ã‚°ã‚’è¡Œã†è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®é–“éš”\n",
    "    save_steps=250,  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®é–“éš”\n",
    "    save_total_limit=1,  # ä¿å­˜ã™ã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®æœ€å¤§æ•°\n",
    "    fp16=True,  # è‡ªå‹•æ··åˆç²¾åº¦æ¼”ç®—ã®æœ‰åŠ¹åŒ–\n",
    "    load_best_model_at_end=True,  # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´çµ‚äº†å¾Œã«èª­ã¿è¾¼ã‚€ã‹\n",
    "    metric_for_best_model=\"spearman\",  # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºå®šã™ã‚‹è©•ä¾¡æŒ‡æ¨™\n",
    "    remove_unused_columns=False,  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸è¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‰Šé™¤ã™ã‚‹ã‹\n",
    "    report_to=\"none\",  # å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã¸ã®ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer\n",
    "\n",
    "class SimCSETrainer(Trainer):\n",
    "    \"\"\"SimCSEã®è¨“ç·´ã«ä½¿ç”¨ã™ã‚‹Trainer\"\"\"\n",
    "\n",
    "    def get_eval_dataloader(\n",
    "        self, eval_dataset: Dataset | None = None\n",
    "    ) -> DataLoader:\n",
    "        \"\"\"\n",
    "        æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®DataLoaderã§eval_collate_fnã‚’ä½¿ã†ã‚ˆã†ã«\n",
    "        Trainerã®get_eval_dataloaderã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰\n",
    "        \"\"\"\n",
    "        if eval_dataset is None:\n",
    "            eval_dataset = self.eval_dataset\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=64,\n",
    "            collate_fn=eval_collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "# æ•™å¸«ãªã—SimCSEã®Trainerã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "unsup_trainer = SimCSETrainer(\n",
    "    model=unsup_model,\n",
    "    args=unsup_training_args,\n",
    "    data_collator=unsup_train_collate_fn,\n",
    "    train_dataset=unsup_train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  è¨“ç·´ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3907' max='3907' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3907/3907 27:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.331667</td>\n",
       "      <td>0.752163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.402906</td>\n",
       "      <td>0.755473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.385067</td>\n",
       "      <td>0.753259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.366536</td>\n",
       "      <td>0.757948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.350700</td>\n",
       "      <td>0.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.320323</td>\n",
       "      <td>0.760896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.332596</td>\n",
       "      <td>0.759427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.334367</td>\n",
       "      <td>0.757139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.300360</td>\n",
       "      <td>0.758956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.297671</td>\n",
       "      <td>0.758454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.306671</td>\n",
       "      <td>0.753173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.296501</td>\n",
       "      <td>0.756182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.296679</td>\n",
       "      <td>0.756792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.294138</td>\n",
       "      <td>0.756875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.301019</td>\n",
       "      <td>0.756191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3907, training_loss=0.00033567894401759603, metrics={'train_runtime': 1629.5441, 'train_samples_per_second': 613.669, 'train_steps_per_second': 2.398, 'total_flos': 0.0, 'train_loss': 0.00033567894401759603, 'epoch': 1.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ•™å¸«ãªã—SimCSEã®è¨“ç·´ã‚’è¡Œã†\n",
    "unsup_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ€§èƒ½è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3203227519989014,\n",
       " 'eval_spearman': 0.7608961210671525,\n",
       " 'eval_runtime': 10.878,\n",
       " 'eval_samples_per_second': 1144.601,\n",
       " 'eval_steps_per_second': 4.504,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ¤œè¨¼ã‚»ãƒƒãƒˆã§æ•™å¸«ãªã—SimCSEã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚’è¡Œã†\n",
    "unsup_trainer.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1774518489837646,\n",
       " 'eval_spearman': 0.7878933714566898,\n",
       " 'eval_runtime': 1.3524,\n",
       " 'eval_samples_per_second': 1077.307,\n",
       " 'eval_steps_per_second': 4.436,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æ•™å¸«ãªã—SimCSEã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚’è¡Œã†\n",
    "unsup_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/outputs_unsup_simcse/encoder/tokenizer_config.json',\n",
       " '../model/outputs_unsup_simcse/encoder/special_tokens_map.json',\n",
       " '../model/outputs_unsup_simcse/encoder/vocab.txt',\n",
       " '../model/outputs_unsup_simcse/encoder/added_tokens.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ä¿å­˜ã¨ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "encoder_path = \"../model/outputs_unsup_simcse/encoder\"\n",
    "unsup_model.encoder.save_pretrained(encoder_path)\n",
    "tokenizer.save_pretrained(encoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ•™å¸«ã‚ã‚ŠSimCSEã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
