{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 第8章 文埋め込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文埋め込みモデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 教師なしSimCSEの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# 乱数シードの設定\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset jawiki-sentences (/root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "unsup_train_dataset = load_dataset(\n",
    "    \"llm-book/jawiki-sentences\", split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 24387500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットの形式と事例数を確認\n",
    "print(unsup_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 アンパサンド(&, 英語: ampersand)は、並立助詞「...と...」を意味する記号である。\n",
      "1 ラテン語で「...と...」を表す接続詞 \"et\" の合字を起源とする。\n",
      "2 現代のフォントでも、Trebuchet MS など一部のフォントでは、\"et\" の合字であることが容易にわかる字形を使用している。\n",
      "3 英語で教育を行う学校でアルファベットを復唱する場合、その文字自体が単語となる文字(\"A\", \"I\", かつては \"O\" も)については、伝統的にラテン語の per se(それ自体)を用いて \"A per se A\" のように唱えられていた。\n",
      "4 また、アルファベットの最後に、27番目の文字のように \"&\" を加えることも広く行われていた。\n",
      "5 \"&\" はラテン語で et と読まれていたが、後に英語で and と読まれるようになった。\n",
      "6 結果として、アルファベットの復唱の最後は \"X, Y, Z, and per se and\" という形になった。\n",
      "7 この最後のフレーズが繰り返されるうちに \"ampersand\" と訛っていき、この言葉は1837年までには英語の一般的な語法となった。\n",
      "8 アンドレ=マリ・アンペールがこの記号を自身の著作で使い、これが広く読まれたため、この記号が \"Ampère's and\" と呼ばれるようになったという誤った語源俗説がある。\n",
      "9 アンパサンドの起源は1世紀の古ローマ筆記体にまで遡ることができる。\n",
      "10 古ローマ筆記体では、E と T はしばしば合字として繋げて書かれていた(左図「アンパサンドの変遷」の字形1)。それに続く、流麗さを増した新ローマ筆記体では、様々な合字が極めて頻繁に使われるようになった。\n",
      "11 字形2と3は4世紀中頃における et の合字の例である。\n",
      "12 その後、9世紀のカロリング小文字体に至るラテン文字の変遷の過程で、合字の使用は一般には廃れていった。\n",
      "13 しかし、et の合字は使われ続け、次第に元の文字がわかりにくい字形に変化していった(字形4から6)。\n",
      "14 現代のイタリック体のアンパサンドは、ルネサンス期に発展した筆記体での et の合字に遡る。\n",
      "15 1455年のヨーロッパにおける印刷技術の発明以降、印刷業者はイタリック体とローマ筆記体のアンパサンドの両方を多用するようになった。\n",
      "16 アンパサンドのルーツはローマ時代に遡るため、ラテンアルファベットを使用する多くの言語でアンパサンドが使用されるようになった。\n",
      "17 アンパサンドはしばしばラテンアルファベットの最後の文字とされることがあった。\n",
      "18 例えば1011年のByrhtferthの文字表がその例である。\n",
      "19 同様に、\"&\" は英語アルファベットの27番目の文字とされ、アメリカ合衆国やその他の地域でも、子供達はアンパサンドはアルファベットの最後の文字だと教えられていた。\n",
      "20 1863年の M. B. Moore の著書 The Dixie Primer, for the Little Folks にその一例を見ることができる。\n",
      "21 ジョージ・エリオットは、1859年に発表した小説「アダム・ビード(英語版)」の中で、Jacob Storey に次のセリフを語らせている。\n",
      "22 \"He thought it [Z] had only been put to finish off th' alphabet like; though ampusand would ha' done as well, for what he could see.\" よく知られた童謡の Apple Pie ABC は \"X, Y, Z, and ampersand, All wished for a piece in hand\" という歌詞で締めくくられる。\n",
      "23 アンパサンドは、ティロ式記号の et (\"⁊\", Unicode U+204A) とは別のものである。\n",
      "24 ティロ式記号の et は、アンパサンドと意味は同じだが数字の「7」に似た形の記号である。\n",
      "25 両者はともに古代から使用され、中世を通してラテン語の et を表すために使用された。\n",
      "26 しかし、アンパサンドとティロ式記号の et はそれぞれ独立に発明されたものである。\n",
      "27 ラテン文字から発展した古アイルランド語の文字では、アイルランド語の agus(「...と...」)を表すためにティロ式記号の et が使用されていた。\n",
      "28 今日はゲール文字の一部として主に装飾的な目的で使用されている。\n",
      "29 この文字はアイルランドにおけるキリスト教時代初期に修道院の影響によって書き文字に加わった可能性がある。\n",
      "30 日常的な手書きの場合、欧米では小文字の ε(エプシロン)を大きくしたもの(あるいは数字の \"3\" の鏡文字)に縦線を加えた形の単純化されたアンパサンドがしばしば使われる。\n",
      "31 また、エプシロンの上下に縦線または点を付けたものもしばしば使われる。\n",
      "32 くだけた用法として、プラス記号(\"+\", この記号もまた et の合字である)がアンパサンドの代わりに使われることもある。\n",
      "33 また、プラス記号に輪を重ねたような、無声歯茎側面摩擦音を示す発音記号「[ɬ]」のようなものが使われることもある。\n",
      "34 ティロの速記には「et」を表すための「⁊」(U+204A Tironian sign et)がある。\n",
      "35 この文字はドイツのフラクトゥールで使われたほか、ゲール文字でも使用される。\n",
      "36 ギリシア文字では「......と」を意味するκαιを表すための合字として「ϗ」(U+03D7 Greek kai symbol)が使われることがある。\n",
      "37 プログラミング言語では、C など多数の言語で AND 演算子として用いられる。\n",
      "38 PHPでは、変数宣言記号($)の直前に記述することで、参照渡しを行うことができる。\n",
      "39 BASIC 系列の言語では文字列の連結演算子として使用される。\n",
      "40 \"foo\" & \"bar\" は \"foobar\" を返す。\n",
      "41 また、主にマイクロソフト系では整数の十六進表記に &h を用い、&h0F (十進で15)のように表現する。\n",
      "42 SGML、XML、HTMLでは、アンパサンドを使ってSGML実体を参照する。\n",
      "43 \n",
      "44 言語(げんご)は、狭義には「声による記号の体系」をいう。\n",
      "45 広辞苑や大辞泉には次のように解説されている。\n",
      "46 『日本大百科全書』では、「言語」という語は多義である、と解説され、大脳の言語中枢(英語版)に蓄えられた《語彙と文法規則の体系》を指すこともあり、その体系を用いる能力としてとらえることもある、と解説され、一方では、抽象的に「すべての人間が共有する言語能力」を指すこともあり、「個々の個別言語」を指すこともある、と解説されている。\n",
      "47 広義の言語には、verbalなものとnon-verbalなもの(各種記号、アイコン、図形、ボディーランゲージ等)の両方を含み、日常のコミュニケーションでは狭義の言語表現に身振り、手振り、図示、擬音等も加えて表現されることもある。\n",
      "48 言語は、人間が用いる意志伝達手段であり、社会集団内で形成習得され、意志を相互に伝達することや、抽象的な思考を可能にし、結果として人間の社会的活動や文化的活動を支えている。\n",
      "49 言語には、文化の特徴が織り込まれており、共同体で用いられている言語の習得をすることによって、その共同体での社会的学習、および人格の形成をしていくことになる。\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットの中身を確認する\n",
    "for i, text in enumerate(unsup_train_dataset[:50][\"text\"]):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-ecc223a34f1ffe88.arrow\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットから空白行を事例を削除\n",
    "unsup_train_dataset = unsup_train_dataset.filter(\n",
    "    lambda example: example[\"text\"].strip() != \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-e9e6caaca5ef995f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/llm-book___jawiki-sentences/default/1.0.0/53a30ee0f53283c9671cc04dc79a18905ce320760396d0e87085fcd63cbfa3fc/cache-9484a91dba79bc01.arrow\n"
     ]
    }
   ],
   "source": [
    "# 訓練セットをシャッフルし、最初の100万事例を取り出す\n",
    "unsup_train_dataset = unsup_train_dataset.shuffle().select(\n",
    "    range(1000000)\n",
    ")\n",
    "# パフォーマンスの低下を防ぐために、シャッフルされた状態の訓練セットを\n",
    "# ディスクに書き込む\n",
    "unsup_train_dataset = unsup_train_dataset.flatten_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 前処理後の訓練セットの形式と事例数を確認\n",
    "print(unsup_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2005年の時点で、10,000人ものウズベキスタン人が韓国での労働に従事しており、その大部分が高麗人である。\n",
      "1 小学5年生(11歳)の時から芸能活動を開始。\n",
      "2 i ħ d d t | ψ ( t ) ⟩ = L ^ | ψ ( t ) ⟩ {\\displaystyle i\\hbar {\\frac {d}{dt}}|\\psi (t)\\rangle ={\\hat {L}}|\\psi (t)\\rangle }\n",
      "3 安土宗論(あづちしゅうろん)は、1579年(天正7年)、安土城下の浄厳院で行われた浄土宗と法華宗の宗論。\n",
      "4 1927年 オーストラリア選手権(1927ねんオーストラリアせんしゅけん、1927 Australian Championships)に関する記事。\n",
      "5 さらにマップ上で最大8つまでしか建築できず(司令官アビリティの”解体”か設置したプレイヤー自らが出向いて解体する必要がある)\n",
      "6 特に誉淳が1827年から作成した『古瓦譜』は畿内で600点以上の拓本を蒐集し、瓦当文様に着目したうえで編年を試みている。\n",
      "7 マルクス主義者を広言し、メキシコ共産党の敵であり味方であった。\n",
      "8 ICHILLIN'(アイチリン、朝: 아이칠린)は、韓国の7人組女性アイドルグループ。\n",
      "9 マークVIは1983年にモデルサイクルを終了し、1984年のマークVII(英語版)はフルサイズセグメントから撤退し、マークシリーズは異なるセグメントに移行した。\n"
     ]
    }
   ],
   "source": [
    "# 前処理後の訓練セットの内容を確認\n",
    "for i, text in enumerate(unsup_train_dataset[:10][\"text\"]):\n",
    "    print(i, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset jglue (/root/.cache/huggingface/datasets/llm-book___jglue/JSTS/1.1.0/b394a8dbefe82fb1dc2724c1eb79bb1ea3062df2037f91a69a27c089f3ff685f)\n",
      "Reusing dataset jglue (/root/.cache/huggingface/datasets/llm-book___jglue/JSTS/1.1.0/b394a8dbefe82fb1dc2724c1eb79bb1ea3062df2037f91a69a27c089f3ff685f)\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Hubのllm-book/JGLUEのリポジトリから\n",
    "# JSTSデータセットの訓練セットと検証セットを読み込み、\n",
    "# それぞれをSimCSEの検証セットとテストセットとして使用する\n",
    "valid_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JSTS\", split=\"train\"\n",
    ")\n",
    "test_dataset = load_dataset(\n",
    "    \"llm-book/JGLUE\", name=\"JSTS\", split=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トークナイザとcollate関数の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "def unsup_train_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"教師なしSimCSEの訓練セットのミニバッチを作成\"\"\"\n",
    "    # ミニバッチに含まれる文にトークナイザを適用する\n",
    "    tokenized_texts = tokenizer(\n",
    "        [example[\"text\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # 文と文の類似度行列における正例ペアの位置を示すTensorを作成する\n",
    "    # 行列のi行目の事例（文）に対してi列目の事例（文）との組が正例ペアとなる\n",
    "    labels = torch.arange(len(examples))\n",
    "\n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_texts,\n",
    "        \"tokenized_texts_2\": tokenized_texts,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collate_fn(\n",
    "    examples: list[dict],\n",
    ") -> dict[str, BatchEncoding | Tensor]:\n",
    "    \"\"\"SimCSEの検証・テストセットのミニバッチを作成\"\"\"\n",
    "    # ミニバッチの文ペアに含まれる文（文1と文2）のそれぞれに\n",
    "    # トークナイザを適用する\n",
    "    tokenized_texts_1 = tokenizer(\n",
    "        [example[\"sentence1\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokenized_texts_2 = tokenizer(\n",
    "        [example[\"sentence2\"] for example in examples],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # 文1と文2の類似度行列における正例ペアの位置を示すTensorを作成する\n",
    "    # 行列のi行目の事例（文1）に対して\n",
    "    # i列目の事例（文2）との組が正例ペアとなる\n",
    "    labels = torch.arange(len(examples))\n",
    "\n",
    "    # データセットに付与された類似度スコアのTensorを作成する\n",
    "    label_scores = torch.tensor(\n",
    "        [example[\"label\"] for example in examples]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"tokenized_texts_1\": tokenized_texts_1,\n",
    "        \"tokenized_texts_2\": tokenized_texts_2,\n",
    "        \"labels\": labels,\n",
    "        \"label_scores\": label_scores,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデル準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from transformers.utils import ModelOutput\n",
    "\n",
    "class SimCSEModel(nn.Module):\n",
    "    \"\"\"SimCSEのモデル\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name: str,\n",
    "        mlp_only_train: bool = False,\n",
    "        temperature: float = 0.05,\n",
    "    ):\n",
    "        \"\"\"モデルの初期化\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # モデル名からエンコーダを初期化する\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
    "        # パラメータをメモリ上に隣接した形で配置\n",
    "        # これを実行しない場合、モデルの保存でエラーになることがある\n",
    "        # for param in model.parameters():\n",
    "        #     param.data = param.data.contiguous()\n",
    "        # MLP層の次元数\n",
    "        self.hidden_size = self.encoder.config.hidden_size\n",
    "        # MLP層の線形層\n",
    "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        # MLP層の活性化関数\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # MLP層による変換を訓練時にのみ適用するよう設定するフラグ\n",
    "        self.mlp_only_train = mlp_only_train\n",
    "        # 交差エントロピー損失の計算時に使用する温度\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def encode_texts(self, tokenized_texts: BatchEncoding) -> Tensor:\n",
    "        \"\"\"エンコーダを用いて文をベクトルに変換\"\"\"\n",
    "        # トークナイズされた文をエンコーダに入力する\n",
    "        encoded_texts = self.encoder(**tokenized_texts)\n",
    "        # モデルの最終層の出力（last_hidden_state）の\n",
    "        # [CLS]トークン（0番目の位置のトークン）のベクトルを取り出す\n",
    "        encoded_texts = encoded_texts.last_hidden_state[:, 0]\n",
    "\n",
    "        # self.mlp_only_trainのフラグがTrueに設定されていて\n",
    "        # かつ訓練時でない場合、MLP層の変換を適用せずにベクトルを返す\n",
    "        if self.mlp_only_train and not self.training:\n",
    "            return encoded_texts\n",
    "\n",
    "        # MLP層によるベクトルの変換を行う\n",
    "        encoded_texts = self.dense(encoded_texts)\n",
    "        encoded_texts = self.activation(encoded_texts)\n",
    "\n",
    "        return encoded_texts\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokenized_texts_1: BatchEncoding,\n",
    "        tokenized_texts_2: BatchEncoding,\n",
    "        labels: Tensor,\n",
    "        label_scores: Tensor | None = None,\n",
    "    ) -> ModelOutput:\n",
    "        \"\"\"モデルの前向き計算を定義\"\"\"\n",
    "        # 文ペアをベクトルに変換する\n",
    "        encoded_texts_1 = self.encode_texts(tokenized_texts_1)\n",
    "        encoded_texts_2 = self.encode_texts(tokenized_texts_2)\n",
    "\n",
    "        # 文ペアの類似度行列を作成する\n",
    "        sim_matrix = F.cosine_similarity(\n",
    "            encoded_texts_1.unsqueeze(1),\n",
    "            encoded_texts_2.unsqueeze(0),\n",
    "            dim=2,\n",
    "        )\n",
    "\n",
    "        # 交差エントロピー損失を求める\n",
    "        loss = F.cross_entropy(sim_matrix / self.temperature, labels)\n",
    "\n",
    "        # 性能評価に使用するため、正例ペアに対するスコアを類似度行列から取り出す\n",
    "        positive_mask = F.one_hot(labels, sim_matrix.size(1)).bool()\n",
    "        positive_scores = torch.masked_select(\n",
    "            sim_matrix, positive_mask\n",
    "        )\n",
    "\n",
    "        return ModelOutput(loss=loss, scores=positive_scores)\n",
    "\n",
    "# 教師なしSimCSEのモデルを初期化する\n",
    "unsup_model = SimCSEModel(base_model_name, mlp_only_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainerの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    モデルが予測したスコアと評価用データのスコアの\n",
    "    スピアマンの順位相関係数を計算\n",
    "    \"\"\"\n",
    "    scores = p.predictions\n",
    "    labels, label_scores = p.label_ids\n",
    "\n",
    "    spearman = spearmanr(scores, label_scores).correlation\n",
    "\n",
    "    return {\"spearman\": spearman}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 教師なしSimCSEの訓練のハイパーパラメータを設定する\n",
    "unsup_training_args = TrainingArguments(\n",
    "    output_dir=\"../model/outputs_unsup_simcse\",  # 結果の保存先フォルダ\n",
    "    per_device_train_batch_size=256,  # 訓練時のバッチサイズ\n",
    "    per_device_eval_batch_size=256,  # 評価時のバッチサイズ\n",
    "    learning_rate=3e-5,  # 学習率\n",
    "    num_train_epochs=1,  # 訓練エポック数\n",
    "    evaluation_strategy=\"steps\",  # 検証セットによる評価のタイミング\n",
    "    eval_steps=250,  # 検証セットによる評価を行う訓練ステップ数の間隔\n",
    "    logging_steps=250,  # ロギングを行う訓練ステップ数の間隔\n",
    "    save_steps=250,  # チェックポイントを保存する訓練ステップ数の間隔\n",
    "    save_total_limit=1,  # 保存するチェックポイントの最大数\n",
    "    fp16=True,  # 自動混合精度演算の有効化\n",
    "    load_best_model_at_end=True,  # 最良のモデルを訓練終了後に読み込むか\n",
    "    metric_for_best_model=\"spearman\",  # 最良のモデルを決定する評価指標\n",
    "    remove_unused_columns=False,  # データセットの不要フィールドを削除するか\n",
    "    report_to=\"none\",  # 外部ツールへのログを無効化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer\n",
    "\n",
    "class SimCSETrainer(Trainer):\n",
    "    \"\"\"SimCSEの訓練に使用するTrainer\"\"\"\n",
    "\n",
    "    def get_eval_dataloader(\n",
    "        self, eval_dataset: Dataset | None = None\n",
    "    ) -> DataLoader:\n",
    "        \"\"\"\n",
    "        検証・テストセットのDataLoaderでeval_collate_fnを使うように\n",
    "        Trainerのget_eval_dataloaderをオーバーライド\n",
    "        \"\"\"\n",
    "        if eval_dataset is None:\n",
    "            eval_dataset = self.eval_dataset\n",
    "\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=64,\n",
    "            collate_fn=eval_collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "# 教師なしSimCSEのTrainerを初期化する\n",
    "unsup_trainer = SimCSETrainer(\n",
    "    model=unsup_model,\n",
    "    args=unsup_training_args,\n",
    "    data_collator=unsup_train_collate_fn,\n",
    "    train_dataset=unsup_train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  訓練の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3907' max='3907' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3907/3907 27:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.331667</td>\n",
       "      <td>0.752163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>2.402906</td>\n",
       "      <td>0.755473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.385067</td>\n",
       "      <td>0.753259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.366536</td>\n",
       "      <td>0.757948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.350700</td>\n",
       "      <td>0.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>2.320323</td>\n",
       "      <td>0.760896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.332596</td>\n",
       "      <td>0.759427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.334367</td>\n",
       "      <td>0.757139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.300360</td>\n",
       "      <td>0.758956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.297671</td>\n",
       "      <td>0.758454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.306671</td>\n",
       "      <td>0.753173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>2.296501</td>\n",
       "      <td>0.756182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.296679</td>\n",
       "      <td>0.756792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.294138</td>\n",
       "      <td>0.756875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2.301019</td>\n",
       "      <td>0.756191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3907, training_loss=0.00033567894401759603, metrics={'train_runtime': 1629.5441, 'train_samples_per_second': 613.669, 'train_steps_per_second': 2.398, 'total_flos': 0.0, 'train_loss': 0.00033567894401759603, 'epoch': 1.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 教師なしSimCSEの訓練を行う\n",
    "unsup_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 性能評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3203227519989014,\n",
       " 'eval_spearman': 0.7608961210671525,\n",
       " 'eval_runtime': 10.878,\n",
       " 'eval_samples_per_second': 1144.601,\n",
       " 'eval_steps_per_second': 4.504,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 検証セットで教師なしSimCSEのモデル評価を行う\n",
    "unsup_trainer.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1774518489837646,\n",
       " 'eval_spearman': 0.7878933714566898,\n",
       " 'eval_runtime': 1.3524,\n",
       " 'eval_samples_per_second': 1077.307,\n",
       " 'eval_steps_per_second': 4.436,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テストセットで教師なしSimCSEのモデル評価を行う\n",
    "unsup_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/outputs_unsup_simcse/encoder/tokenizer_config.json',\n",
       " '../model/outputs_unsup_simcse/encoder/special_tokens_map.json',\n",
       " '../model/outputs_unsup_simcse/encoder/vocab.txt',\n",
       " '../model/outputs_unsup_simcse/encoder/added_tokens.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### トークナイザの保存とモデルの保存\n",
    "encoder_path = \"../model/outputs_unsup_simcse/encoder\"\n",
    "unsup_model.encoder.save_pretrained(encoder_path)\n",
    "tokenizer.save_pretrained(encoder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 教師ありSimCSEの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
