{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固有表現認識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 固有表現認識とは？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy-alignments seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default\n",
      "Reusing dataset ner-wikipedia-dataset (/root/.cache/huggingface/datasets/llm-book___ner-wikipedia-dataset/default/0.0.0/184bcf9be66116e777f2f534436226d47348676c93ba20cca58933f1b2b3b782)\n",
      "100%|██████████| 3/3 [00:00<00:00, 273.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# データセットを読み込む\n",
    "dataset = load_dataset(\"llm-book/ner-wikipedia-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 4274\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 534\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 535\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# データセットの形式と事例数を確認する\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'curid': '3638038',\n",
      "  'entities': [{'name': 'さくら学院', 'span': [0, 5], 'type': 'その他の組織名'},\n",
      "               {'name': 'Ciao Smiles', 'span': [6, 17], 'type': 'その他の組織名'}],\n",
      "  'text': 'さくら学院、Ciao Smilesのメンバー。'},\n",
      " {'curid': '1729527',\n",
      "  'entities': [{'name': 'レクレアティーボ・ウェルバ', 'span': [17, 30], 'type': 'その他の組織名'},\n",
      "               {'name': 'プリメーラ・ディビシオン', 'span': [32, 44], 'type': 'その他の組織名'}],\n",
      "  'text': '2008年10月5日、アウェーでのレクレアティーボ・ウェルバ戦でプリメーラ・ディビシオンでの初得点を決めた。'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 訓練セットの最初の二つの事例を表示する\n",
    "pprint(list(dataset[\"train\"])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>人名</th>\n",
       "      <td>2394</td>\n",
       "      <td>299</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>法人名</th>\n",
       "      <td>2006</td>\n",
       "      <td>231</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>地名</th>\n",
       "      <td>1769</td>\n",
       "      <td>184</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>政治的組織名</th>\n",
       "      <td>953</td>\n",
       "      <td>121</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>製品名</th>\n",
       "      <td>934</td>\n",
       "      <td>123</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>施設名</th>\n",
       "      <td>868</td>\n",
       "      <td>103</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>その他の組織名</th>\n",
       "      <td>852</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>イベント名</th>\n",
       "      <td>831</td>\n",
       "      <td>85</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>合計</th>\n",
       "      <td>10607</td>\n",
       "      <td>1245</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train  validation  test\n",
       "人名        2394         299   287\n",
       "法人名       2006         231   248\n",
       "地名        1769         184   204\n",
       "政治的組織名     953         121   106\n",
       "製品名        934         123   158\n",
       "施設名        868         103   137\n",
       "その他の組織名    852          99   100\n",
       "イベント名      831          85    93\n",
       "合計       10607        1245  1333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def count_label_occurrences(dataset: Dataset) -> dict[str, int]:\n",
    "    \"\"\"固有表現タイプの出現回数をカウント\"\"\"\n",
    "    # 各事例から固有表現タイプを抽出したlistを作成する\n",
    "    entities = [\n",
    "        e[\"type\"] for data in dataset for e in data[\"entities\"]\n",
    "    ]\n",
    "    \n",
    "    # ラベルの表現回数が多い順に並べる\n",
    "    label_counts = dict(Counter(entities).most_common())\n",
    "    return label_counts\n",
    "    \n",
    "label_counts_dict = {}\n",
    "for split in dataset: # 各分割セットを処理する\n",
    "    label_counts_dict[split] = count_label_occurrences(dataset[split])\n",
    "# DataFrame形式で表示する\n",
    "df = pd.DataFrame(label_counts_dict)\n",
    "df.loc[\"合計\"] = df.sum()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### スパンの重なる固有表現の存在を判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainにおけるスパンが重複する事例数:0\n",
      "validationにおけるスパンが重複する事例数:0\n",
      "testにおけるスパンが重複する事例数:0\n"
     ]
    }
   ],
   "source": [
    "def has_overlap(spans: list[tuple[int, int]]) -> int:\n",
    "    \"\"\"スパンの重なる固有表現の存在を判定\"\"\"\n",
    "    sorted_spans = sorted(spans, key=lambda x: x[0])\n",
    "    for i in range(1, len(sorted_spans)):\n",
    "        # 前のスパンの終了位置が現在のスパンの開始位置より大きい場合、\n",
    "        # 重なっているとする\n",
    "        if sorted_spans[i - 1][1] > sorted_spans[i][0]:\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "# 各分割セットでスパンの重なる固有表現がある事例数を数える\n",
    "overlap_count = 0\n",
    "for split in dataset: # 各分割セットを処理する\n",
    "    for data in dataset[split]: # 各事例を処理する\n",
    "        if data[\"entities\"]: # 固有表現の存在しない事例はスキップ\n",
    "            # スパンのみのlistを作成する\n",
    "            spans = [e[\"span\"] for e in data[\"entities\"]]\n",
    "            overlap_count += has_overlap(spans)\n",
    "    print(f\"{split}におけるスパンが重複する事例数:{overlap_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テキストの正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化前: ＡＢＣABCａｂｃabcｱｲｳアイウ①②③123\n",
      "正規化後: ABCABCabcabcアイウアイウ123123\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "# テキストに対してUnicode正規化を行う\n",
    "text =  \"ＡＢＣABCａｂｃabcｱｲｳアイウ①②③123\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"正規化前: {text}\")\n",
    "print(f\"正規化後: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化前: ㈱、3㌕、10℃\n",
      "正規化後: (株)、3キログラム、10°C\n"
     ]
    }
   ],
   "source": [
    "# 文字列の長さが変わる場合ある\n",
    "text = \"㈱、3㌕、10℃\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"正規化前: {text}\")\n",
    "print(f\"正規化後: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化されていない事例数: 0\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize, is_normalized\n",
    "\n",
    "count = 0\n",
    "for split in dataset: # 各分割セットを処理する\n",
    "    for data in dataset[split]: # 各事例を処理する\n",
    "        # テキストが正規化されていない事例をカウントする\n",
    "        if not is_normalized(\"NFKC\", data[\"text\"]):\n",
    "            count += 1\n",
    "print(f\"正規化されていない事例数: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テキストのトークナイゼーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サブワード単位: さくら/学院/、/C/##ia/##o/Sm/##ile/##s/の/メンバー/。\n",
      "文単位: さ/く/ら/学/院/、/C/i/a/o/ /S/m/i/l/e/s/の/メ/ン/バ/ー/。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# トークナイザを読み込み\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# トークナイゼーションを行う\n",
    "subwords = \"/\".join(tokenizer.tokenize(dataset[\"train\"][0][\"text\"]))\n",
    "characters = \"/\".join(dataset[\"train\"][0][\"text\"])\n",
    "print(f\"サブワード単位: {subwords}\")\n",
    "print(f\"文単位: {characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文字列とトークン列のアライメント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"さくら学院\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文字のlist: ['さ', 'く', 'ら', '学', '院']\n",
      "トークンのlist: ['[CLS]', 'さくら', '学院', '[SEP]']\n",
      "文字に対するトークンの位置: [[1], [1], [1], [2], [2]]\n",
      "トークンに対する文字の位置: [[], [0, 1, 2], [3, 4], []]\n"
     ]
    }
   ],
   "source": [
    "from spacy_alignments.tokenizations import get_alignments\n",
    "\n",
    "# 文字列のlistを獲得する\n",
    "characters = list(text)\n",
    "# テキストを特殊トークンを含めたトークンのlistに変換する\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "# 文字のlistとトークンのlistのアライメントをとる\n",
    "char_to_token_indices, token_to_char_indices = get_alignments(characters, tokens)\n",
    "print(f\"文字のlist: {characters}\")\n",
    "print(f\"トークンのlist: {tokens}\")\n",
    "print(f\"文字に対するトークンの位置: {char_to_token_indices}\")\n",
    "print(f\"トークンに対する文字の位置: {token_to_char_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 系列ラベリングのためのラベル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"大谷翔平は岩手県水沢市出身\"\n",
    "entities = [\n",
    "    {\"name\": \"大谷翔平\", \"span\": [0,4], \"type\": \"人名\"},\n",
    "    {\"name\": \"岩手県水沢市\", \"span\": [5,11], \"type\": \"地名\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>位置</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>トークン列</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>大谷</td>\n",
       "      <td>翔</td>\n",
       "      <td>##平</td>\n",
       "      <td>は</td>\n",
       "      <td>岩手</td>\n",
       "      <td>県</td>\n",
       "      <td>水沢</td>\n",
       "      <td>市</td>\n",
       "      <td>出身</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ラベル列</th>\n",
       "      <td>-</td>\n",
       "      <td>B-人名</td>\n",
       "      <td>I-人名</td>\n",
       "      <td>I-人名</td>\n",
       "      <td>O</td>\n",
       "      <td>B-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "位置        0     1     2     3  4     5     6     7     8   9      10\n",
       "トークン列  [CLS]    大谷     翔   ##平  は    岩手     県    水沢     市  出身  [SEP]\n",
       "ラベル列       -  B-人名  I-人名  I-人名  O  B-地名  I-地名  I-地名  I-地名   O      -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def output_tokens_and_labels(\n",
    "    text: str,\n",
    "    entities: list[dict[str, list[int] | str]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"トークンのlistとラベルのlistを出力\"\"\"\n",
    "    # 文字列のlistとトークンのlistのアライメントをとる\n",
    "    characters = list(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "    \n",
    "    # \"O\"のラベルで初期化したラベルのlistを作成する\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for entity in entities: # 各固有表現で処理する\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        start = char_to_token_indices[entity_span[0]][0]\n",
    "        end = char_to_token_indices[entity_span[1]-1][0]\n",
    "        # 固有表現の開始トークンの位置に\"B-\"のラベルを設定する\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        # 固有表現の開始トークン以外の位置に\"I-\"のラベルを設定する\n",
    "        for idx in range(start + 1, end + 1):\n",
    "            labels[idx] = f\"I-{entity_type}\"\n",
    "    # 特殊トークンの位置にはラベルを設定しない\n",
    "    labels[0] = \"-\" # 開始\n",
    "    labels[-1] = \"-\" # 終了\n",
    "    return tokens, labels\n",
    "\n",
    "# トークンとラベルのlistを出力する\n",
    "tokens, labels = output_tokens_and_labels(text, entities, tokenizer)\n",
    "# DataFrameの形式で表示する\n",
    "df = pd.DataFrame({\"トークン列\": tokens, \"ラベル列\": labels})\n",
    "df.index.name = \"位置\"\n",
    "display(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 評価指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seqevalライブラリを用いた評価スコアの算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "def create_character_labels(\n",
    "    text: str, entities: list[dict[str, list[int] | str]]\n",
    ") -> list[str]:\n",
    "    \"\"\"文字ベースでラベルのlistを作成\"\"\"\n",
    "    # \"O\"のラベルで初期化したラベルのlistを作成する\n",
    "    labels = [\"O\"] * len(text)\n",
    "    for entity in entities: # 各固有表現を処理する\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        # 固有表現の開始文字の位置に\"B-\"のラベルを設定する\n",
    "        labels[entity_span[0]] = f\"B-{entity_type}\"\n",
    "        # 固有表現の開始文字以外の位置に\"I-\"ラベルを設定する\n",
    "        for i in range(entity_span[0] + 1, entity_span[1]):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "    \n",
    "def convert_results_to_labels(\n",
    "    results: list[dict[str, Any]]\n",
    ") -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"正解データと予測データのラベルのlistを作成\"\"\"\n",
    "    true_labels, pred_labels = [], []\n",
    "    for result in results: # 各事例を処理する\n",
    "        # 文字ベースでラベルのリストを作成してlistに加える\n",
    "        true_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"entities\"])\n",
    "        )\n",
    "        pred_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"pred_entities\"])\n",
    "        )\n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          人名       1.00      1.00      1.00         1\n",
      "          地名       0.00      0.00      0.00         1\n",
      "         施設名       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.33      0.50      0.40         2\n",
      "   macro avg       0.33      0.33      0.33         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = [\n",
    "    {\n",
    "        \"text\": \"大谷翔平は岩手県水沢市出身\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"大谷翔平\", \"span\": [0,4], \"type\": \"人名\"},\n",
    "            {\"name\": \"岩手県水沢市\", \"span\": [5, 11], \"type\": \"地名\"}\n",
    "        ],\n",
    "        \"pred_entities\": [\n",
    "            {\"name\": \"大谷翔平\", \"span\": [0,4], \"type\": \"人名\"},\n",
    "            {\"name\": \"岩手県\", \"span\": [5,8], \"type\": \"地名\"},\n",
    "            {\"name\": \"水沢市\", \"span\": [8,11], \"type\": \"施設名\"}\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# 正解データと予測データのラベルのlistを作成\n",
    "true_labels, pred_labels = convert_results_to_labels(results)\n",
    "# 評価結果を取得して表示\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.3333333333333333, 'recall': 0.5, 'f1-score': 0.4}\n",
      "{'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(\n",
    "    true_labels: list[list[str]], pred_labels: list[list[str]], average: str\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"適合率、再現率、F値を算出\"\"\"\n",
    "    scores = {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average=average),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average=average),\n",
    "        \"f1-score\": f1_score(true_labels, pred_labels, average=average),\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# 適合率、再現率、F値のマイクロ平均を算出する\n",
    "print(compute_scores(true_labels, pred_labels, \"micro\"))\n",
    "# 適合率、再現率、F値のマクロ平均を算出する\n",
    "print(compute_scores(true_labels, pred_labels, \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固有表現認識モデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTのファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O',\n",
      " 1: 'B-その他の組織名',\n",
      " 2: 'I-その他の組織名',\n",
      " 3: 'B-イベント名',\n",
      " 4: 'I-イベント名',\n",
      " 5: 'B-人名',\n",
      " 6: 'I-人名',\n",
      " 7: 'B-地名',\n",
      " 8: 'I-地名',\n",
      " 9: 'B-政治的組織名',\n",
      " 10: 'I-政治的組織名',\n",
      " 11: 'B-施設名',\n",
      " 12: 'I-施設名',\n",
      " 13: 'B-法人名',\n",
      " 14: 'I-法人名',\n",
      " 15: 'B-製品名',\n",
      " 16: 'I-製品名'}\n"
     ]
    }
   ],
   "source": [
    "# ラベルとIDを対応付けるdictの作成\n",
    "import torch\n",
    "\n",
    "def create_label2id(\n",
    "    entities_list: list[list[dict[str, str | int]]]\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"ラベルとIDを紐づけるdictを作成\"\"\"\n",
    "    # \"O\"のIDには0を割り当てる\n",
    "    label2id = {\"O\": 0}\n",
    "    # 固有表現タイプのsetを獲得して並び替える\n",
    "    entity_types = set(\n",
    "        [e[\"type\"] for entities in entities_list for e in entities]\n",
    "    )\n",
    "    entity_types = sorted(entity_types)\n",
    "    for i, entity_type in enumerate(entity_types):\n",
    "        # \"B-\"のIDには奇数番号を割り当てる\n",
    "        label2id[f\"B-{entity_type}\"] = i * 2 + 1\n",
    "        # \"I-\"のIDには偶数番号を割り当てる\n",
    "        label2id[f\"I-{entity_type}\"] = i * 2 + 2\n",
    "    return label2id\n",
    "\n",
    "# ラベルとIDを紐づけるdictを作成する\n",
    "label2id = create_label2id(dataset[\"train\"][\"entities\"])\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "pprint(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4274/4274 [00:03<00:00, 1155.79ex/s]\n",
      "100%|██████████| 534/534 [00:00<00:00, 1397.92ex/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "def preprocess_data(\n",
    "    data: dict[str, Any],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    label2id: dict[int, str],\n",
    ") -> BatchEncoding:\n",
    "    \"\"\"データの前処理\"\"\"\n",
    "    # テキストのトークナイゼーションを行う\n",
    "    inputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "    \n",
    "    # 文字のlistとトークンのlistのアライメントをとる\n",
    "    characters = list(data[\"text\"])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "    \n",
    "    # \"O\"のIDのlistを作成する\n",
    "    labels = torch.zeros_like(inputs[\"input_ids\"])\n",
    "    for entity in data[\"entities\"]: # 各固有表現を処理する\n",
    "        start_token_indices = char_to_token_indices[entity[\"span\"][0]]\n",
    "        end_token_indices = char_to_token_indices[\n",
    "            entity[\"span\"][1] - 1\n",
    "        ]\n",
    "        # 文字に対するトークンが存在しなければスキップする\n",
    "        if (\n",
    "            len(start_token_indices)==0\n",
    "            or len(end_token_indices)==0\n",
    "        ):\n",
    "            continue\n",
    "        start, end = start_token_indices[0], end_token_indices[0]\n",
    "        entity_type = entity[\"type\"]\n",
    "        # 固有表現の開始トークンの位置に\"B-\"のIDを設定する\n",
    "        labels[start] = label2id[f\"B-{entity_type}\"]\n",
    "        # 固有表現の開始トークン以外の位置に\"I-\"のIDを設定する\n",
    "        if start != end:\n",
    "            labels[start + 1: end + 1] = label2id[f\"I-{entity_type}\"]\n",
    "    # 特殊トークンの位置のIDは-100とする\n",
    "    labels[torch.where(inputs[\"special_tokens_mask\"])] = -100\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "    \n",
    "# 訓練セットに対して前処理を行う\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "# 検証セットに対して前処理を行う\n",
    "validation_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:803: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "# モデルを読み込む\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, label2id=label2id, id2label=id2label\n",
    ")\n",
    "# パラメータをメモリ上に隣接する形で配置\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.contiguous()\n",
    "# collate関数にDataCollatorForTokenClassificationを用いる\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルのファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1678/359011269.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 01:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.098389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.091702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.091689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.100583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.104642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=670, training_loss=0.15430139712433316, metrics={'train_runtime': 102.5866, 'train_samples_per_second': 208.312, 'train_steps_per_second': 6.531, 'total_flos': 1070012411245680.0, 'train_loss': 0.15430139712433316, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# 乱数シードを42に固定する\n",
    "set_seed(42)\n",
    "\n",
    "# Trainerに渡す引数を初期化する\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/output_bert_ner\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Trainerを初期化する\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# 訓練する\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 固有表現の予測・抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固有表現ラベルの予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力: {'input_ids': [[0, 1], [2, 3]], 'labels': [[1, 2], [3, 4]]}\n",
      "出力: [{'input_ids': [0, 1], 'labels': [1, 2]}, {'input_ids': [2, 3], 'labels': [3, 4]}]\n"
     ]
    }
   ],
   "source": [
    "def convert_list_dict_to_dict_list(\n",
    "    list_dict: dict[str, list]\n",
    ") -> list[dict[str, list]]:\n",
    "    \"\"\"ミニバッチのデータを事例単位のlistに変換\"\"\"\n",
    "    dict_list = []\n",
    "    # dictのキーのlistを作成する\n",
    "    keys = list(list_dict.keys())\n",
    "    for idx in range(len(list_dict[keys[0]])): # 各事例で処理する\n",
    "        # dictの各キーからデータを取り出してlistに追加する\n",
    "        dict_list.append({key: list_dict[key][idx] for key in keys})\n",
    "    return dict_list\n",
    "\n",
    "# ミニバッチのデータを事例単位のlistに変換する\n",
    "list_dict = {\n",
    "    \"input_ids\": [[0, 1], [2, 3]],\n",
    "    \"labels\": [[1, 2], [3, 4]],\n",
    "}\n",
    "dict_list = convert_list_dict_to_dict_list(list_dict)\n",
    "print(f\"入力: {list_dict}\")\n",
    "print(f\"出力: {dict_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 39.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 15, 16, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 13, 14, 14, 14, 13, 0, 0, 0, 0, 0, 0, 15, 16, 15, 13, 14, 14, 14, 14, 13, 13, 13, 14, 14, 0, 0, 0, 13, 13, 14, 14, 14, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 15, 16, 16, 0, 13, 14, 14, 14, 14, 15, 0, 0, 15, 15, 15, 16, 16, 0, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "def run_prediction(\n",
    "    dataloader: DataLoader, \n",
    "    model: PreTrainedModel\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"予測スコアに基づき固有表現ラベルを予測\"\"\"\n",
    "    predictions = []\n",
    "    for batch in tqdm(dataloader): # 各ミニバッチを処理する\n",
    "        inputs = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch.items()\n",
    "            if k != \"special_tokens_mask\"\n",
    "        }\n",
    "        # 予測スコアを取得する\n",
    "        logits = model(**inputs).logits\n",
    "        # 最もスコアの高いIDを取得する\n",
    "        batch[\"pred_label_ids\"] = logits.argmax(-1)\n",
    "        batch = {k: v.cpu().tolist() for k, v in batch.items()}\n",
    "        # ミニバッチのデータを事例単位のlistに変換する\n",
    "        predictions += convert_list_dict_to_dict_list(batch)\n",
    "    return predictions\n",
    "\n",
    "# ミニバッチの作成にDataLoaderを用いる\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "# 固有表現ラベルを予測する\n",
    "predictions = run_prediction(validation_dataloader, model)\n",
    "print(predictions[0][\"pred_label_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固有表現の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curid': '1662110',\n",
      " 'entities': [{'name': '復活篇', 'span': [1, 4], 'type': '製品名'},\n",
      "              {'name': 'グリーンバニー', 'span': [6, 13], 'type': '法人名'}],\n",
      " 'pred_entities': [{'name': '復活篇', 'span': [1, 4], 'type': '製品名'},\n",
      "                   {'name': 'グリーンバニー', 'span': [6, 13], 'type': '法人名'}],\n",
      " 'text': '「復活篇」はグリーンバニーからの発売となっている。'}\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "\n",
    "def extract_entities(\n",
    "    predictions: list[dict[str, Any]],\n",
    "    dataset: list[dict[str, Any]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    id2label: dict[int, str],\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"固有表現を抽出\"\"\"\n",
    "    results = []\n",
    "    for prediction, data in zip(predictions, dataset):\n",
    "        # 文字列のlistを取得する\n",
    "        characters = list(data[\"text\"])\n",
    "        \n",
    "        # 特殊トークンを除いたトークンのlistと予測ラベルのlistを取得する\n",
    "        tokens, pred_labels = [], []\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(\n",
    "            prediction[\"input_ids\"]\n",
    "        )\n",
    "        for token, label_id in zip(\n",
    "            all_tokens, prediction[\"pred_label_ids\"]\n",
    "        ):\n",
    "            # 特殊トークン以外をlistに追加する\n",
    "            if token not in tokenizer.all_special_tokens:\n",
    "                tokens.append(token)\n",
    "                pred_labels.append(id2label[label_id])\n",
    "                \n",
    "        # 文字のlistとトークンのlistのアライメントを取る\n",
    "        _, token_to_char_indices = get_alignments(characters, tokens)\n",
    "        \n",
    "        # 予測ラベルのlistから固有表現タイプと、\n",
    "        # トークン単位の開始位置と終了位置を取得して、\n",
    "        # それらを正解データと同じ形式にする\n",
    "        pred_entities = []\n",
    "        for entity in get_entities(pred_labels):\n",
    "            entity_type, token_start, token_end = entity\n",
    "            # 文字単位の開始位置を取得する\n",
    "            char_start = token_to_char_indices[token_start][0]\n",
    "            # 文字単位の終了位置を取得する\n",
    "            char_end = token_to_char_indices[token_end][-1] + 1\n",
    "            pred_entity = {\n",
    "                \"name\": \"\".join(characters[char_start:char_end]),\n",
    "                \"span\": [char_start, char_end],\n",
    "                \"type\": entity_type,\n",
    "            }\n",
    "            pred_entities.append(pred_entity)\n",
    "        data[\"pred_entities\"] = pred_entities\n",
    "        results.append(data)\n",
    "    return results\n",
    "\n",
    "# 固有表現を抽出する\n",
    "results = extract_entities(\n",
    "    predictions, dataset[\"validation\"], tokenizer, id2label\n",
    ")\n",
    "pprint(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
