{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固有表現認識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 固有表現認識とは？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[ja,sentencepice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy-alignments seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 3.98k/3.98k [00:00<00:00, 22.9MB/s]\n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset ner-wikipedia-dataset/default to /root/.cache/huggingface/datasets/llm-book___ner-wikipedia-dataset/default/0.0.0/184bcf9be66116e777f2f534436226d47348676c93ba20cca58933f1b2b3b782...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 4.04MB [00:00, 70.3MB/s]                  \n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ner-wikipedia-dataset downloaded and prepared to /root/.cache/huggingface/datasets/llm-book___ner-wikipedia-dataset/default/0.0.0/184bcf9be66116e777f2f534436226d47348676c93ba20cca58933f1b2b3b782. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 709.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# データセットを読み込む\n",
    "dataset = load_dataset(\"llm-book/ner-wikipedia-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 4274\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 534\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 535\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# データセットの形式と事例数を確認する\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'curid': '3638038',\n",
      "  'entities': [{'name': 'さくら学院', 'span': [0, 5], 'type': 'その他の組織名'},\n",
      "               {'name': 'Ciao Smiles', 'span': [6, 17], 'type': 'その他の組織名'}],\n",
      "  'text': 'さくら学院、Ciao Smilesのメンバー。'},\n",
      " {'curid': '1729527',\n",
      "  'entities': [{'name': 'レクレアティーボ・ウェルバ', 'span': [17, 30], 'type': 'その他の組織名'},\n",
      "               {'name': 'プリメーラ・ディビシオン', 'span': [32, 44], 'type': 'その他の組織名'}],\n",
      "  'text': '2008年10月5日、アウェーでのレクレアティーボ・ウェルバ戦でプリメーラ・ディビシオンでの初得点を決めた。'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 訓練セットの最初の二つの事例を表示する\n",
    "pprint(list(dataset[\"train\"])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットの分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>人名</th>\n",
       "      <td>2394</td>\n",
       "      <td>299</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>法人名</th>\n",
       "      <td>2006</td>\n",
       "      <td>231</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>地名</th>\n",
       "      <td>1769</td>\n",
       "      <td>184</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>政治的組織名</th>\n",
       "      <td>953</td>\n",
       "      <td>121</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>製品名</th>\n",
       "      <td>934</td>\n",
       "      <td>123</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>施設名</th>\n",
       "      <td>868</td>\n",
       "      <td>103</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>その他の組織名</th>\n",
       "      <td>852</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>イベント名</th>\n",
       "      <td>831</td>\n",
       "      <td>85</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>合計</th>\n",
       "      <td>10607</td>\n",
       "      <td>1245</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train  validation  test\n",
       "人名        2394         299   287\n",
       "法人名       2006         231   248\n",
       "地名        1769         184   204\n",
       "政治的組織名     953         121   106\n",
       "製品名        934         123   158\n",
       "施設名        868         103   137\n",
       "その他の組織名    852          99   100\n",
       "イベント名      831          85    93\n",
       "合計       10607        1245  1333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def count_label_occurrences(dataset: Dataset) -> dict[str, int]:\n",
    "    \"\"\"固有表現タイプの出現回数をカウント\"\"\"\n",
    "    # 各事例から固有表現タイプを抽出したlistを作成する\n",
    "    entities = [\n",
    "        e[\"type\"] for data in dataset for e in data[\"entities\"]\n",
    "    ]\n",
    "    \n",
    "    # ラベルの表現回数が多い順に並べる\n",
    "    label_counts = dict(Counter(entities).most_common())\n",
    "    return label_counts\n",
    "    \n",
    "label_counts_dict = {}\n",
    "for split in dataset: # 各分割セットを処理する\n",
    "    label_counts_dict[split] = count_label_occurrences(dataset[split])\n",
    "# DataFrame形式で表示する\n",
    "df = pd.DataFrame(label_counts_dict)\n",
    "df.loc[\"合計\"] = df.sum()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### スパンの重なる固有表現の存在を判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainにおけるスパンが重複する事例数:0\n",
      "validationにおけるスパンが重複する事例数:0\n",
      "testにおけるスパンが重複する事例数:0\n"
     ]
    }
   ],
   "source": [
    "def has_overlap(spans: list[tuple[int, int]]) -> int:\n",
    "    \"\"\"スパンの重なる固有表現の存在を判定\"\"\"\n",
    "    sorted_spans = sorted(spans, key=lambda x: x[0])\n",
    "    for i in range(1, len(sorted_spans)):\n",
    "        # 前のスパンの終了位置が現在のスパンの開始位置より大きい場合、\n",
    "        # 重なっているとする\n",
    "        if sorted_spans[i - 1][1] > sorted_spans[i][0]:\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "# 各分割セットでスパンの重なる固有表現がある事例数を数える\n",
    "overlap_count = 0\n",
    "for split in dataset: # 各分割セットを処理する\n",
    "    for data in dataset[split]: # 各事例を処理する\n",
    "        if data[\"entities\"]: # 固有表現の存在しない事例はスキップ\n",
    "            # スパンのみのlistを作成する\n",
    "            spans = [e[\"span\"] for e in data[\"entities\"]]\n",
    "            overlap_count += has_overlap(spans)\n",
    "    print(f\"{split}におけるスパンが重複する事例数:{overlap_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テキストの正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化前: ＡＢＣABCａｂｃabcｱｲｳアイウ①②③123\n",
      "正規化後: ABCABCabcabcアイウアイウ123123\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "# テキストに対してUnicode正規化を行う\n",
    "text =  \"ＡＢＣABCａｂｃabcｱｲｳアイウ①②③123\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"正規化前: {text}\")\n",
    "print(f\"正規化後: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化前: ㈱、3㌕、10℃\n",
      "正規化後: (株)、3キログラム、10°C\n"
     ]
    }
   ],
   "source": [
    "# 文字列の長さが変わる場合ある\n",
    "text = \"㈱、3㌕、10℃\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"正規化前: {text}\")\n",
    "print(f\"正規化後: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正規化されていない事例数: 0\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize, is_normalized\n",
    "\n",
    "count = 0\n",
    "for split in dataset: # 各分割セットを処理する\n",
    "    for data in dataset[split]: # 各事例を処理する\n",
    "        # テキストが正規化されていない事例をカウントする\n",
    "        if not is_normalized(\"NFKC\", data[\"text\"]):\n",
    "            count += 1\n",
    "print(f\"正規化されていない事例数: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テキストのトークナイゼーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 251/251 [00:00<00:00, 1.69MB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 703kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サブワード単位: さくら/学院/、/C/##ia/##o/Sm/##ile/##s/の/メンバー/。\n",
      "文単位: さ/く/ら/学/院/、/C/i/a/o/ /S/m/i/l/e/s/の/メ/ン/バ/ー/。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# トークナイザを読み込み\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# トークナイゼーションを行う\n",
    "subwords = \"/\".join(tokenizer.tokenize(dataset[\"train\"][0][\"text\"]))\n",
    "characters = \"/\".join(dataset[\"train\"][0][\"text\"])\n",
    "print(f\"サブワード単位: {subwords}\")\n",
    "print(f\"文単位: {characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文字列とトークン列のアライメント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"さくら学院\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文字のlist: ['さ', 'く', 'ら', '学', '院']\n",
      "トークンのlist: ['[CLS]', 'さくら', '学院', '[SEP]']\n",
      "文字に対するトークンの位置: [[1], [1], [1], [2], [2]]\n",
      "トークンに対する文字の位置: [[], [0, 1, 2], [3, 4], []]\n"
     ]
    }
   ],
   "source": [
    "from spacy_alignments.tokenizations import get_alignments\n",
    "\n",
    "# 文字列のlistを獲得する\n",
    "characters = list(text)\n",
    "# テキストを特殊トークンを含めたトークンのlistに変換する\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "# 文字のlistとトークンのlistのアライメントをとる\n",
    "char_to_token_indices, token_to_char_indices = get_alignments(characters, tokens)\n",
    "print(f\"文字のlist: {characters}\")\n",
    "print(f\"トークンのlist: {tokens}\")\n",
    "print(f\"文字に対するトークンの位置: {char_to_token_indices}\")\n",
    "print(f\"トークンに対する文字の位置: {token_to_char_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 系列ラベリングのためのラベル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"大谷翔平は岩手県水沢市出身\"\n",
    "entities = [\n",
    "    {\"name\": \"大谷翔平\", \"span\": [0,4], \"type\": \"人名\"},\n",
    "    {\"name\": \"岩手県水沢市\", \"span\": [5,11], \"type\": \"地名\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>位置</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>トークン列</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>大谷</td>\n",
       "      <td>翔</td>\n",
       "      <td>##平</td>\n",
       "      <td>は</td>\n",
       "      <td>岩手</td>\n",
       "      <td>県</td>\n",
       "      <td>水沢</td>\n",
       "      <td>市</td>\n",
       "      <td>出身</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ラベル列</th>\n",
       "      <td>-</td>\n",
       "      <td>B-人名</td>\n",
       "      <td>I-人名</td>\n",
       "      <td>I-人名</td>\n",
       "      <td>O</td>\n",
       "      <td>B-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>I-地名</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "位置        0     1     2     3  4     5     6     7     8   9      10\n",
       "トークン列  [CLS]    大谷     翔   ##平  は    岩手     県    水沢     市  出身  [SEP]\n",
       "ラベル列       -  B-人名  I-人名  I-人名  O  B-地名  I-地名  I-地名  I-地名   O      -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def output_tokens_and_labels(\n",
    "    text: str,\n",
    "    entities: list[dict[str, list[int] | str]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"トークンのlistとラベルのlistを出力\"\"\"\n",
    "    # 文字列のlistとトークンのlistのアライメントをとる\n",
    "    characters = list(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "    \n",
    "    # \"O\"のラベルで初期化したラベルのlistを作成する\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for entity in entities: # 各固有表現で処理する\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        start = char_to_token_indices[entity_span[0]][0]\n",
    "        end = char_to_token_indices[entity_span[1]-1][0]\n",
    "        # 固有表現の開始トークンの位置に\"B-\"のラベルを設定する\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        # 固有表現の開始トークン以外の位置に\"I-\"のラベルを設定する\n",
    "        for idx in range(start + 1, end + 1):\n",
    "            labels[idx] = f\"I-{entity_type}\"\n",
    "    # 特殊トークンの位置にはラベルを設定しない\n",
    "    labels[0] = \"-\" # 開始\n",
    "    labels[-1] = \"-\" # 終了\n",
    "    return tokens, labels\n",
    "\n",
    "# トークンとラベルのlistを出力する\n",
    "tokens, labels = output_tokens_and_labels(text, entities, tokenizer)\n",
    "# DataFrameの形式で表示する\n",
    "df = pd.DataFrame({\"トークン列\": tokens, \"ラベル列\": labels})\n",
    "df.index.name = \"位置\"\n",
    "display(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 評価指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seqevalライブラリを用いた評価スコアの算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "def create_character_labels(\n",
    "    text: str, entities: list[dict[str, list[int] | str]]\n",
    ") -> list[str]:\n",
    "    \"\"\"文字ベースでラベルのlistを作成\"\"\"\n",
    "    # \"O\"のラベルで初期化したラベルのlistを作成する\n",
    "    labels = [\"O\"] * len(text)\n",
    "    for entity in entities: # 各固有表現を処理する\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        # 固有表現の開始文字の位置に\"B-\"のラベルを設定する\n",
    "        labels[entity_span[0]] = f\"B-{entity_type}\"\n",
    "        # 固有表現の開始文字以外の位置に\"I-\"ラベルを設定する\n",
    "        for i in range(entity_span[0] + 1, entity_span[1]):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "    \n",
    "def convert_results_to_labels(\n",
    "    results: list[dict[str, Any]]\n",
    ") -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"正解データと予測データのラベルのlistを作成\"\"\"\n",
    "    true_labels, pred_labels = [], []\n",
    "    for result in results: # 各事例を処理する\n",
    "        # 文字ベースでラベルのリストを作成してlistに加える\n",
    "        true_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"entities\"])\n",
    "        )\n",
    "        pred_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"pred_entities\"])\n",
    "        )\n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          人名       1.00      1.00      1.00         1\n",
      "          地名       0.00      0.00      0.00         1\n",
      "         施設名       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.33      0.50      0.40         2\n",
      "   macro avg       0.33      0.33      0.33         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = [\n",
    "    {\n",
    "        \"text\": \"大谷翔平は岩手県水沢市出身\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"大谷翔平\", \"span\": [0,4], \"type\": \"人名\"},\n",
    "            {\"name\": \"岩手県水沢市\", \"span\": [5, 11], \"type\": \"地名\"}\n",
    "        ],\n",
    "        \"pred_entities\": [\n",
    "            {\"name\": \"大谷翔平\", \"span\": [0,4], \"type\": \"人名\"},\n",
    "            {\"name\": \"岩手県\", \"span\": [5,8], \"type\": \"地名\"},\n",
    "            {\"name\": \"水沢市\", \"span\": [8,11], \"type\": \"施設名\"}\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# 正解データと予測データのラベルのlistを作成\n",
    "true_labels, pred_labels = convert_results_to_labels(results)\n",
    "# 評価結果を取得して表示\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.3333333333333333, 'recall': 0.5, 'f1-score': 0.4}\n",
      "{'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(\n",
    "    true_labels: list[list[str]], pred_labels: list[list[str]], average: str\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"適合率、再現率、F値を算出\"\"\"\n",
    "    scores = {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average=average),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average=average),\n",
    "        \"f1-score\": f1_score(true_labels, pred_labels, average=average),\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# 適合率、再現率、F値のマイクロ平均を算出する\n",
    "print(compute_scores(true_labels, pred_labels, \"micro\"))\n",
    "# 適合率、再現率、F値のマクロ平均を算出する\n",
    "print(compute_scores(true_labels, pred_labels, \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固有表現認識モデルの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTのファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O',\n",
      " 1: 'B-その他の組織名',\n",
      " 2: 'I-その他の組織名',\n",
      " 3: 'B-イベント名',\n",
      " 4: 'I-イベント名',\n",
      " 5: 'B-人名',\n",
      " 6: 'I-人名',\n",
      " 7: 'B-地名',\n",
      " 8: 'I-地名',\n",
      " 9: 'B-政治的組織名',\n",
      " 10: 'I-政治的組織名',\n",
      " 11: 'B-施設名',\n",
      " 12: 'I-施設名',\n",
      " 13: 'B-法人名',\n",
      " 14: 'I-法人名',\n",
      " 15: 'B-製品名',\n",
      " 16: 'I-製品名'}\n"
     ]
    }
   ],
   "source": [
    "# ラベルとIDを対応付けるdictの作成\n",
    "import torch\n",
    "\n",
    "def create_label2id(\n",
    "    entities_list: list[list[dict[str, str | int]]]\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"ラベルとIDを紐づけるdictを作成\"\"\"\n",
    "    # \"O\"のIDには0を割り当てる\n",
    "    label2id = {\"O\": 0}\n",
    "    # 固有表現タイプのsetを獲得して並び替える\n",
    "    entity_types = set(\n",
    "        [e[\"type\"] for entities in entities_list for e in entities]\n",
    "    )\n",
    "    entity_types = sorted(entity_types)\n",
    "    for i, entity_type in enumerate(entity_types):\n",
    "        # \"B-\"のIDには奇数番号を割り当てる\n",
    "        label2id[f\"B-{entity_type}\"] = i * 2 + 1\n",
    "        # \"I-\"のIDには偶数番号を割り当てる\n",
    "        label2id[f\"I-{entity_type}\"] = i * 2 + 2\n",
    "    return label2id\n",
    "\n",
    "# ラベルとIDを紐づけるdictを作成する\n",
    "label2id = create_label2id(dataset[\"train\"][\"entities\"])\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "pprint(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4274/4274 [00:03<00:00, 1373.51ex/s]\n",
      "100%|██████████| 534/534 [00:00<00:00, 1580.32ex/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "def preprocess_data(\n",
    "    data: dict[str, Any],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    label2id: dict[int, str],\n",
    ") -> BatchEncoding:\n",
    "    \"\"\"データの前処理\"\"\"\n",
    "    # テキストのトークナイゼーションを行う\n",
    "    inputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "    \n",
    "    # 文字のlistとトークンのlistのアライメントをとる\n",
    "    characters = list(data[\"text\"])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "    \n",
    "    # \"O\"のIDのlistを作成する\n",
    "    labels = torch.zeros_like(inputs[\"input_ids\"])\n",
    "    for entity in data[\"entities\"]: # 各固有表現を処理する\n",
    "        start_token_indices = char_to_token_indices[entity[\"span\"][0]]\n",
    "        end_token_indices = char_to_token_indices[\n",
    "            entity[\"span\"][1] - 1\n",
    "        ]\n",
    "        # 文字に対するトークンが存在しなければスキップする\n",
    "        if (\n",
    "            len(start_token_indices)==0\n",
    "            or len(end_token_indices)==0\n",
    "        ):\n",
    "            continue\n",
    "        start, end = start_token_indices[0], end_token_indices[0]\n",
    "        entity_type = entity[\"type\"]\n",
    "        # 固有表現の開始トークンの位置に\"B-\"のIDを設定する\n",
    "        labels[start] = label2id[f\"B-{entity_type}\"]\n",
    "        # 固有表現の開始トークン以外の位置に\"I-\"のIDを設定する\n",
    "        if start != end:\n",
    "            labels[start + 1: end + 1] = label2id[f\"I-{entity_type}\"]\n",
    "    # 特殊トークンの位置のIDは-100とする\n",
    "    labels[torch.where(inputs[\"special_tokens_mask\"])] = -100\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "    \n",
    "# 訓練セットに対して前処理を行う\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "# 検証セットに対して前処理を行う\n",
    "validation_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 472/472 [00:00<00:00, 3.55MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 427M/427M [00:10<00:00, 44.6MB/s] \n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-v3 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "# モデルを読み込む\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, label2id=label2id, id2label=id2label\n",
    ")\n",
    "# パラメータをメモリ上に隣接する形で配置\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.contiguous()\n",
    "# collate関数にDataCollatorForTokenClassificationを用いる\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルのファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4274\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 670\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 01:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.656300</td>\n",
       "      <td>0.107465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.084497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.090706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.096854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.100623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_ner/checkpoint-134\n",
      "Configuration saved in ../model/output_bert_ner/checkpoint-134/config.json\n",
      "Model weights saved in ../model/output_bert_ner/checkpoint-134/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_ner/checkpoint-134/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_ner/checkpoint-134/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_ner/checkpoint-268\n",
      "Configuration saved in ../model/output_bert_ner/checkpoint-268/config.json\n",
      "Model weights saved in ../model/output_bert_ner/checkpoint-268/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_ner/checkpoint-268/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_ner/checkpoint-268/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_ner/checkpoint-402\n",
      "Configuration saved in ../model/output_bert_ner/checkpoint-402/config.json\n",
      "Model weights saved in ../model/output_bert_ner/checkpoint-402/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_ner/checkpoint-402/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_ner/checkpoint-402/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_ner/checkpoint-536\n",
      "Configuration saved in ../model/output_bert_ner/checkpoint-536/config.json\n",
      "Model weights saved in ../model/output_bert_ner/checkpoint-536/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_ner/checkpoint-536/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_ner/checkpoint-536/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_ner/checkpoint-670\n",
      "Configuration saved in ../model/output_bert_ner/checkpoint-670/config.json\n",
      "Model weights saved in ../model/output_bert_ner/checkpoint-670/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_ner/checkpoint-670/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_ner/checkpoint-670/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=670, training_loss=0.15402125137955394, metrics={'train_runtime': 105.305, 'train_samples_per_second': 202.934, 'train_steps_per_second': 6.362, 'total_flos': 1054773477784752.0, 'train_loss': 0.15402125137955394, 'epoch': 5.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# 乱数シードを42に固定する\n",
    "set_seed(42)\n",
    "\n",
    "# Trainerに渡す引数を初期化する\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/output_bert_ner\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Trainerを初期化する\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# 訓練する\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 固有表現の予測・抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固有表現ラベルの予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力: {'input_ids': [[0, 1], [2, 3]], 'labels': [[1, 2], [3, 4]]}\n",
      "出力: [{'input_ids': [0, 1], 'labels': [1, 2]}, {'input_ids': [2, 3], 'labels': [3, 4]}]\n"
     ]
    }
   ],
   "source": [
    "def convert_list_dict_to_dict_list(\n",
    "    list_dict: dict[str, list]\n",
    ") -> list[dict[str, list]]:\n",
    "    \"\"\"ミニバッチのデータを事例単位のlistに変換\"\"\"\n",
    "    dict_list = []\n",
    "    # dictのキーのlistを作成する\n",
    "    keys = list(list_dict.keys())\n",
    "    for idx in range(len(list_dict[keys[0]])): # 各事例で処理する\n",
    "        # dictの各キーからデータを取り出してlistに追加する\n",
    "        dict_list.append({key: list_dict[key][idx] for key in keys})\n",
    "    return dict_list\n",
    "\n",
    "# ミニバッチのデータを事例単位のlistに変換する\n",
    "list_dict = {\n",
    "    \"input_ids\": [[0, 1], [2, 3]],\n",
    "    \"labels\": [[1, 2], [3, 4]],\n",
    "}\n",
    "dict_list = convert_list_dict_to_dict_list(list_dict)\n",
    "print(f\"入力: {list_dict}\")\n",
    "print(f\"出力: {dict_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 25.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 15, 16, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, 14, 13, 0, 0, 0, 0, 0, 15, 15, 16, 16, 13, 14, 14, 14, 16, 0, 13, 13, 14, 14, 13, 0, 0, 0, 13, 14, 14, 14, 0, 0, 13, 14, 14, 14, 0, 0, 0, 0, 0, 15, 16, 16, 0, 13, 14, 14, 14, 14, 0, 0, 0, 0, 15, 15, 16, 0, 13, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "def run_prediction(\n",
    "    dataloader: DataLoader, \n",
    "    model: PreTrainedModel\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"予測スコアに基づき固有表現ラベルを予測\"\"\"\n",
    "    predictions = []\n",
    "    for batch in tqdm(dataloader): # 各ミニバッチを処理する\n",
    "        inputs = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch.items()\n",
    "            if k != \"special_tokens_mask\"\n",
    "        }\n",
    "        # 予測スコアを取得する\n",
    "        logits = model(**inputs).logits\n",
    "        # 最もスコアの高いIDを取得する\n",
    "        batch[\"pred_label_ids\"] = logits.argmax(-1)\n",
    "        batch = {k: v.cpu().tolist() for k, v in batch.items()}\n",
    "        # ミニバッチのデータを事例単位のlistに変換する\n",
    "        predictions += convert_list_dict_to_dict_list(batch)\n",
    "    return predictions\n",
    "\n",
    "# ミニバッチの作成にDataLoaderを用いる\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "# 固有表現ラベルを予測する\n",
    "predictions = run_prediction(validation_dataloader, model)\n",
    "print(predictions[0][\"pred_label_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固有表現の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curid': '1662110',\n",
      " 'entities': [{'name': '復活篇', 'span': [1, 4], 'type': '製品名'},\n",
      "              {'name': 'グリーンバニー', 'span': [6, 13], 'type': '法人名'}],\n",
      " 'pred_entities': [{'name': '復活篇', 'span': [1, 4], 'type': '製品名'},\n",
      "                   {'name': 'グリーンバニー', 'span': [6, 13], 'type': '法人名'}],\n",
      " 'text': '「復活篇」はグリーンバニーからの発売となっている。'}\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "\n",
    "def extract_entities(\n",
    "    predictions: list[dict[str, Any]],\n",
    "    dataset: list[dict[str, Any]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    id2label: dict[int, str],\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"固有表現を抽出\"\"\"\n",
    "    results = []\n",
    "    for prediction, data in zip(predictions, dataset):\n",
    "        # 文字列のlistを取得する\n",
    "        characters = list(data[\"text\"])\n",
    "        \n",
    "        # 特殊トークンを除いたトークンのlistと予測ラベルのlistを取得する\n",
    "        tokens, pred_labels = [], []\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(\n",
    "            prediction[\"input_ids\"]\n",
    "        )\n",
    "        for token, label_id in zip(\n",
    "            all_tokens, prediction[\"pred_label_ids\"]\n",
    "        ):\n",
    "            # 特殊トークン以外をlistに追加する\n",
    "            if token not in tokenizer.all_special_tokens:\n",
    "                tokens.append(token)\n",
    "                pred_labels.append(id2label[label_id])\n",
    "                \n",
    "        # 文字のlistとトークンのlistのアライメントを取る\n",
    "        _, token_to_char_indices = get_alignments(characters, tokens)\n",
    "        \n",
    "        # 予測ラベルのlistから固有表現タイプと、\n",
    "        # トークン単位の開始位置と終了位置を取得して、\n",
    "        # それらを正解データと同じ形式にする\n",
    "        pred_entities = []\n",
    "        for entity in get_entities(pred_labels):\n",
    "            entity_type, token_start, token_end = entity\n",
    "            # 文字単位の開始位置を取得する\n",
    "            char_start = token_to_char_indices[token_start][0]\n",
    "            # 文字単位の終了位置を取得する\n",
    "            char_end = token_to_char_indices[token_end][-1] + 1\n",
    "            pred_entity = {\n",
    "                \"name\": \"\".join(characters[char_start:char_end]),\n",
    "                \"span\": [char_start, char_end],\n",
    "                \"type\": entity_type,\n",
    "            }\n",
    "            pred_entities.append(pred_entity)\n",
    "        data[\"pred_entities\"] = pred_entities\n",
    "        results.append(data)\n",
    "    return results\n",
    "\n",
    "# 固有表現を抽出する\n",
    "results = extract_entities(\n",
    "    predictions, dataset[\"validation\"], tokenizer, id2label\n",
    ")\n",
    "pprint(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検証セットを使ったモデルの選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../model/output_bert_ner/checkpoint-134/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../model/output_bert_ner/checkpoint-134\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_ner/checkpoint-134/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at ../model/output_bert_ner/checkpoint-134.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:00<00:00, 30.93it/s]\n",
      "loading configuration file ../model/output_bert_ner/checkpoint-268/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../model/output_bert_ner/checkpoint-268\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_ner/checkpoint-268/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at ../model/output_bert_ner/checkpoint-268.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:00<00:00, 23.61it/s]\n",
      "loading configuration file ../model/output_bert_ner/checkpoint-402/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../model/output_bert_ner/checkpoint-402\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_ner/checkpoint-402/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at ../model/output_bert_ner/checkpoint-402.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:00<00:00, 21.46it/s]\n",
      "loading configuration file ../model/output_bert_ner/checkpoint-536/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../model/output_bert_ner/checkpoint-536\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_ner/checkpoint-536/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at ../model/output_bert_ner/checkpoint-536.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:00<00:00, 25.85it/s]\n",
      "loading configuration file ../model/output_bert_ner/checkpoint-670/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../model/output_bert_ner/checkpoint-670\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_ner/checkpoint-670/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at ../model/output_bert_ner/checkpoint-670.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:00<00:00, 25.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "best_score = 0\n",
    "# 各チェックポイントで処理\n",
    "for checkpoint in sorted(glob(\"../model/output_bert_ner/checkpoint-*\")):\n",
    "    # モデルを読み込む\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        checkpoint\n",
    "    )\n",
    "    model.to(\"cuda:0\") # モデルをGPUに移動\n",
    "    predictions = run_prediction(validation_dataloader, model)\n",
    "    # 固有表現を抽出\n",
    "    results = extract_entities(\n",
    "        predictions, dataset[\"validation\"], tokenizer, id2label\n",
    "    )\n",
    "    # 正解データと予測データのラベルのlistを作成\n",
    "    true_labels, pred_labels = convert_results_to_labels(results)\n",
    "    # 評価スコアを算出\n",
    "    scores = compute_scores(true_labels, pred_labels, \"micro\")\n",
    "    if best_score < scores[\"f1-score\"]:\n",
    "        best_score = scores[\"f1-score\"]\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/llm-book___ner-wikipedia-dataset/default/0.0.0/184bcf9be66116e777f2f534436226d47348676c93ba20cca58933f1b2b3b782/cache-108c8ca9ee1f2f9c.arrow\n",
      "100%|██████████| 17/17 [00:00<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     その他の組織名       0.80      0.80      0.80       100\n",
      "       イベント名       0.81      0.91      0.86        93\n",
      "          人名       0.95      0.98      0.97       287\n",
      "          地名       0.85      0.88      0.86       204\n",
      "      政治的組織名       0.73      0.84      0.78       106\n",
      "         施設名       0.87      0.85      0.86       137\n",
      "         法人名       0.86      0.88      0.87       248\n",
      "         製品名       0.78      0.83      0.80       158\n",
      "\n",
      "   micro avg       0.85      0.88      0.87      1333\n",
      "   macro avg       0.83      0.87      0.85      1333\n",
      "weighted avg       0.85      0.88      0.87      1333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テストセットに対して前処理\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"test\"].column_names,\n",
    ")\n",
    "# ミニバッチ作成にDataLoaderを用いる\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "# 固有表現ラベルを予測\n",
    "predictions = run_prediction(test_dataloader, best_model)\n",
    "# 固有表現を抽出する\n",
    "results = extract_entities(\n",
    "    predictions, dataset[\"test\"], tokenizer, id2label\n",
    ")\n",
    "# 正解データと予測データのラベルのlistを作成\n",
    "true_labels, pred_labels = convert_results_to_labels(results)\n",
    "# 評価結果を出力\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エラー分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "事例16の正解「[(製品名)]兵動・野爆のキャンピング王国]」とのコラボレーション番組。\n",
      "事例16の予測「[(人名)]兵動・野爆][(製品名)]のキャンピング王国]」とのコラボレーション番組。\n",
      "\n",
      "事例18の正解[(法人名)]常盤木学園]時代の同級生に[(その他の組織名)]なでしこジャパン]の[(人名)]熊谷紗希]がいる。\n",
      "事例18の予測[(施設名)]常盤木学園]時代の同級生に[(その他の組織名)]なでしこジャパン]の[(人名)]熊谷紗希]がいる。\n",
      "\n",
      "事例19の正解テレビで狼男映画の「[(製品名)]倫敦の人狼]」を見た[(人名)]フィル・エヴァリー]は「ロンドンの狼男というタイトルで踊り騒げる曲を書いてみないか」と[(法人名)]ジヴォン]に持ちかけた。\n",
      "事例19の予測テレビで狼男映画の「[(製品名)]倫敦の人狼]」を見た[(人名)]フィル・エヴァリー]は「[(製品名)]ロンドンの狼男]というタイトルで踊り騒げる曲を書いてみないか」と[(人名)]ジヴォン]に持ちかけた。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_error_results(\n",
    "    results: list[dict[str, Any]],\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"エラー事例を発見\"\"\"\n",
    "    error_results = []\n",
    "    for idx, result in enumerate(results): # 各事例を処理\n",
    "        result[\"idx\"] = idx\n",
    "        if result[\"entities\"] != result[\"pred_entities\"]:\n",
    "            error_results.append(result)\n",
    "    return error_results\n",
    "\n",
    "def output_text_with_label(result: dict[str, Any], entity_column: str) -> str:\n",
    "    \"\"\"固有表現ラベル付きテキストを出力\"\"\"\n",
    "    text_with_label = \"\"\n",
    "    entity_count = 0\n",
    "    for i, char in enumerate(result[\"text\"]): # 各文字を処理\n",
    "        # 出力に加えていない固有表現の有無を判定\n",
    "        if entity_count < len(result[entity_column]):\n",
    "            entity = result[entity_column][entity_count]\n",
    "            # 固有表現の先頭の処理\n",
    "            if i == entity[\"span\"][0]:\n",
    "                entity_type = entity[\"type\"]\n",
    "                text_with_label += f\"[({entity_type})]\"\n",
    "            text_with_label += char\n",
    "            # 固有表現の末尾の処理\n",
    "            if i == entity[\"span\"][1] - 1:\n",
    "                text_with_label += \"]\"\n",
    "                entity_count += 1\n",
    "        else:\n",
    "            text_with_label += char\n",
    "    return text_with_label\n",
    "\n",
    "# エラー事例を発見する\n",
    "error_results = find_error_results(results)\n",
    "# 3件のエラー事例を出力する\n",
    "for result in error_results[:3]:\n",
    "    idx = result[\"idx\"]\n",
    "    true_text = output_text_with_label(result, \"entities\")\n",
    "    pred_text = output_text_with_label(result, \"pred_entities\")\n",
    "    print(f\"事例{idx}の正解{true_text}\")\n",
    "    print(f\"事例{idx}の予測{pred_text}\")\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラベル間の遷移可能性を考慮した予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遷移スコアを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transitions(\n",
    "    label2id: dict[str, int]\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"遷移スコアを定義\"\"\"\n",
    "    # \"B-\"のラベルIDのlist\n",
    "    b_ids = [v for k,v in label2id.items() if k[0]==\"B\"]\n",
    "    # \"I-\"のラベルIDのlist\n",
    "    i_ids = [v for k,v in label2id.items() if k[0]==\"I\"]\n",
    "    # \"O\"のラベルIDのlist\n",
    "    o_id = label2id[\"O\"]\n",
    "    \n",
    "    # 開始遷移スコアを定義\n",
    "    # すべてのスコアを-100で初期化\n",
    "    start_transitions = torch.full([len(label2id)], -100.0)\n",
    "    # \"B-\"のラベルへの遷移可能として0を代入する\n",
    "    start_transitions[b_ids] = 0\n",
    "    # \"O-\"のラベルへ遷移可能として0を代入する\n",
    "    start_transitions[o_id] = 0\n",
    "    \n",
    "    # ラベル間の遷移スコアを定義\n",
    "    # すべてのスコアを-100で初期化\n",
    "    transitions = torch.full([len(label2id), len(label2id)], -100.0)\n",
    "    # すべてのラベルから\"B-\"へ遷移可能として0を代入する\n",
    "    transitions[:, b_ids] = 0\n",
    "    # すべてのラベルから\"O-\"へ遷移可能として0を代入する\n",
    "    transitions[:, o_id] = 0\n",
    "    # \"B-\"から同じタイプの\"I-\"へ遷移可能として0を代入する\n",
    "    transitions[b_ids, i_ids] = 0\n",
    "    # \"I-\"から同じタイプの\"I-\"へ遷移可能として0を代入する\n",
    "    transitions[i_ids, i_ids] = 0\n",
    "    \n",
    "    # 終了遷移スコアを定義する\n",
    "    # すべてのラベルから遷移可能としてすべてのスコアを0とする\n",
    "    end_transitions = torch.zeros(len(label2id))\n",
    "    return start_transitions, transitions, end_transitions\n",
    "\n",
    "# 遷移スコアを定義する\n",
    "start_transitions, transitions, end_transitions = create_transitions(\n",
    "    label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,    0., -100.,    0., -100.,    0., -100.,    0., -100.,    0.,\n",
       "        -100.,    0., -100.,    0., -100.,    0., -100.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ビタビアルゴリズムを用いたラベル列の予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_with_viterbi(\n",
    "    emissions: torch.Tensor, # ラベルの予測スコア\n",
    "    mask: torch.Tensor, # マスク\n",
    "    start_transitions: torch.Tensor, # 開始遷移スコア\n",
    "    transitions: torch.Tensor, # ラベル間の遷移スコア\n",
    "    end_transitions: torch.Tensor, # 終了遷移スコア\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"ビタビアルゴリズムを用いて最適なラベル列を探索\"\"\"\n",
    "    # バッチサイズと系列長を取得\n",
    "    batch_size, seq_length = mask.shape\n",
    "    # 予測スコアとマスクに関して、0次元目と1次元目を入れ替える\n",
    "    emissions = emissions.transpose(1, 0)\n",
    "    mask = mask.transpose(1, 0)\n",
    "    \n",
    "    histories = [] # 最適なラベル系列を保持するための履歴list\n",
    "    # 開始遷移スコアと予測スコアを加算して、累積スコアの初期値とする\n",
    "    score = start_transitions + emissions[0]\n",
    "    for i in range(1, seq_length):\n",
    "        # 累積スコアを3次元に変換\n",
    "        broadcast_score = score.unsqueeze(2)\n",
    "        # 現在の予測スコアを3次元に変換\n",
    "        broadcast_emission = emissions[i].unsqueeze(1)\n",
    "        # 累積スコアと遷移スコアと現在の予測スコアを加算して、\n",
    "        # 現在の累積スコアを取得する\n",
    "        next_score = (\n",
    "            broadcast_score + transitions + broadcast_emission\n",
    "        )\n",
    "        # 現在の累積スコアの各ラベルの最大値とそのインデックスを取得する\n",
    "        next_score, indices = next_score.max(dim=1)\n",
    "        # マスクしない要素の場合、累積スコアを更新する\n",
    "        score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "        # スコアの高いインデックスを履歴のlistに追加する\n",
    "        histories.append(indices)\n",
    "    # 終了遷移スコアを加算して合計スコアとする\n",
    "    score += end_transitions\n",
    "    \n",
    "    # 各事例で最適なラベル列を取得する\n",
    "    best_labels_list = []\n",
    "    for i in range(batch_size):\n",
    "        # 合計スコアの中で最大のスコアとなるラベルを取得する\n",
    "        _, best_last_label = score[i].max(dim=0)\n",
    "        best_labels = [best_last_label.item()]\n",
    "        # 最後のラベルの遷移を逆方向に探索し、最適なラベル列を取得する\n",
    "        for history in reversed(histories):\n",
    "            best_last_label = history[i][best_labels[-1]]\n",
    "            best_labels.append(best_last_label.item())\n",
    "        # 順序を反転する\n",
    "        best_labels.reverse()\n",
    "        best_labels_list.append(best_labels)\n",
    "    return torch.LongTensor(best_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     その他の組織名       0.82      0.80      0.81       100\n",
      "       イベント名       0.88      0.92      0.90        93\n",
      "          人名       0.95      0.97      0.96       287\n",
      "          地名       0.87      0.89      0.88       204\n",
      "      政治的組織名       0.77      0.87      0.82       106\n",
      "         施設名       0.92      0.86      0.89       137\n",
      "         法人名       0.89      0.88      0.89       248\n",
      "         製品名       0.80      0.84      0.82       158\n",
      "\n",
      "   micro avg       0.88      0.89      0.88      1333\n",
      "   macro avg       0.86      0.88      0.87      1333\n",
      "weighted avg       0.88      0.89      0.88      1333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_prediction_viterbi(\n",
    "    dataloader: DataLoader,\n",
    "    model: PreTrainedModel,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"ビタビアルゴリズムを用いてラベルを予測\"\"\"\n",
    "    # 遷移スコアを取得する\n",
    "    start_transitions, transitions, end_transitions = create_transitions(\n",
    "        model.config.label2id\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        inputs = {\n",
    "            k: v.to(model.device)\n",
    "            for k,v in batch.items()\n",
    "            if k != \"special_tokens_mask\"\n",
    "        }\n",
    "        # [CLS]以外の予測スコアを取得する\n",
    "        logits = model(**inputs).logits.cpu()[:, 1:, :]\n",
    "        # [CLS]以外の特殊トークンのマスクを取得する\n",
    "        mask = (batch[\"special_tokens_mask\"].cpu() == 0)[:, 1:]\n",
    "        # ビタビアルゴリズムを用いて最適なIDの系列を探索する\n",
    "        pred_label_ids = decode_with_viterbi(\n",
    "            logits,\n",
    "            mask,\n",
    "            start_transitions,\n",
    "            transitions,\n",
    "            end_transitions,\n",
    "        )\n",
    "        # [CLS]のIDを0とする\n",
    "        cls_pred_label_id = torch.zeros(pred_label_ids.shape[0], 1)\n",
    "        # [CLS]のIDと探索したIDの系列を連結して予測ラベルとする\n",
    "        batch[\"pred_label_ids\"] = torch.concat(\n",
    "            [cls_pred_label_id, pred_label_ids], dim=1\n",
    "        )\n",
    "        batch = {k:v.cpu().tolist() for k,v in batch.items()}\n",
    "        # ミニバッチのデータを事例単位のlistに変換する\n",
    "        predictions += convert_list_dict_to_dict_list(batch)\n",
    "    return predictions\n",
    "\n",
    "# ビタビアルゴリズムを用いてラベルを予測する\n",
    "predictions = run_prediction_viterbi(test_dataloader, best_model)\n",
    "# 固有表現を抽出する\n",
    "results = extract_entities(\n",
    "    predictions, dataset[\"test\"], tokenizer, id2label\n",
    ")\n",
    "# 正解データと予測データのラベルのlistを作成する\n",
    "true_labels, pred_labels = convert_results_to_labels(results)\n",
    "# 評価結果を出力する\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "事例27の正解: [(政治的組織名)]李承晩政権]期から[(政治的組織名)]朴正煕政権]期の1970年前後まで、南側の[(地名)]大韓民国]よりも北側の[(地名)]朝鮮民主主義人民共和国]の方が経済的な体力では勝っていたのである。\n",
      "事例27の予測: [(人名)]李承晩政権]期から[(人名)]朴正煕政権]期の1970年前後まで、南側の[(地名)]大韓民国]よりも北側の[(地名)]朝鮮民主主義人民共和国]の方が経済的な体力では勝っていたのである。\n"
     ]
    }
   ],
   "source": [
    "idx = 27\n",
    "result = results[idx]\n",
    "true_text = output_text_with_label(result, \"entities\")\n",
    "pred_text = output_text_with_label(result, \"pred_entities\")\n",
    "print(f\"事例{idx}の正解: {true_text}\")\n",
    "print(f\"事例{idx}の予測: {pred_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRFによるラベル間の遷移可能性の学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT-CRFモデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/tohoku-nlp/bert-base-japanese-v3/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/2c9ec44dde23b6b02ec9f34997dd1595677490da33cc6f7f5931a82fea500ef1.fa452780f4f534fd5a9a500fd6dc0ab2b41b7f3a87fde31e4e439dacbbe6eea3\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/tohoku-nlp/bert-base-japanese-v3/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/201a1f23c0452ac363a0cdb3cc22b8c35e0e0919dff8808adf6fc9074a02518a.b69711e2b1dad158d9a7b14bc7bacc1155d9c924a3bcd47d0ba6db6d23ddc0f9\n",
      "Some weights of the model checkpoint at tohoku-nlp/bert-base-japanese-v3 were not used when initializing BertWithCrfTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertWithCrfTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertWithCrfTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertWithCrfTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['crf.start_transitions', 'classifier.bias', 'crf.transitions', 'classifier.weight', 'crf.end_transitions']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torchcrf import CRF\n",
    "from transformers import BertForTokenClassification, PretrainedConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class BertWithCrfTokenClassification(BertForTokenClassification):\n",
    "    \"\"\"BertForTokenClassificationにCRF層を加えたクラス\"\"\"\n",
    "    \n",
    "    def __init__(self, config:PretrainedConfig):\n",
    "        \"\"\"クラスの初期化\"\"\"\n",
    "        super().__init__(config)\n",
    "        # CRF層を追加する\n",
    "        self.crf = CRF(len(config.label2id), batch_first=True)\n",
    "        \n",
    "    def _init_weights(self, module: torch.nn.Module) -> None:\n",
    "        \"\"\"定義した遷移スコアでパラメータ初期化\"\"\"\n",
    "        super()._init_weights(module)\n",
    "        if isinstance(module, CRF):\n",
    "            st, t, et = create_transitions(self.config.label2id)\n",
    "            module.start_transitions.data = st\n",
    "            module.transitions.data = t\n",
    "            module.end_transitions.data = et\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor | None = None,\n",
    "        token_type_ids: torch.Tensor | None = None,\n",
    "        labels: torch.Tensor | None = None,    \n",
    "    ) -> TokenClassifierOutput:\n",
    "        \"\"\"モデルの前向き計算を定義\"\"\"\n",
    "        # BertForTokenClassificationのforwardメソッドを適用して、\n",
    "        # 予測スコアを算出する\n",
    "        output = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        if labels is not None:\n",
    "            logits = output.logits\n",
    "            mask = labels != -100\n",
    "            labels *= mask\n",
    "            # CRFによる損失を計算\n",
    "            output[\"loss\"] = -self.crf(\n",
    "                logits[:, 1:, :],\n",
    "                labels[:, 1:],\n",
    "                mask=mask[:, 1:],\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "        return output\n",
    "    \n",
    "# BertForTokenClassificationにCRF層を加えたクラスを定義\n",
    "model_crf = BertWithCrfTokenClassification.from_pretrained(\n",
    "    model_name, label2id=label2id, id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertWithCrfTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertWithCrfTokenClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4274\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 670\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 03:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17.101500</td>\n",
       "      <td>1.570267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.221400</td>\n",
       "      <td>1.424493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.587100</td>\n",
       "      <td>1.503783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>1.747114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>1.922975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertWithCrfTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertWithCrfTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_crf_ner/checkpoint-134\n",
      "Configuration saved in ../model/output_bert_crf_ner/checkpoint-134/config.json\n",
      "Model weights saved in ../model/output_bert_crf_ner/checkpoint-134/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_crf_ner/checkpoint-134/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_crf_ner/checkpoint-134/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertWithCrfTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertWithCrfTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_crf_ner/checkpoint-268\n",
      "Configuration saved in ../model/output_bert_crf_ner/checkpoint-268/config.json\n",
      "Model weights saved in ../model/output_bert_crf_ner/checkpoint-268/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_crf_ner/checkpoint-268/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_crf_ner/checkpoint-268/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertWithCrfTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertWithCrfTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_crf_ner/checkpoint-402\n",
      "Configuration saved in ../model/output_bert_crf_ner/checkpoint-402/config.json\n",
      "Model weights saved in ../model/output_bert_crf_ner/checkpoint-402/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_crf_ner/checkpoint-402/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_crf_ner/checkpoint-402/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertWithCrfTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertWithCrfTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_crf_ner/checkpoint-536\n",
      "Configuration saved in ../model/output_bert_crf_ner/checkpoint-536/config.json\n",
      "Model weights saved in ../model/output_bert_crf_ner/checkpoint-536/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_crf_ner/checkpoint-536/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_crf_ner/checkpoint-536/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertWithCrfTokenClassification.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertWithCrfTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 534\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../model/output_bert_crf_ner/checkpoint-670\n",
      "Configuration saved in ../model/output_bert_crf_ner/checkpoint-670/config.json\n",
      "Model weights saved in ../model/output_bert_crf_ner/checkpoint-670/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/output_bert_crf_ner/checkpoint-670/tokenizer_config.json\n",
      "Special tokens file saved in ../model/output_bert_crf_ner/checkpoint-670/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=670, training_loss=3.8808603286743164, metrics={'train_runtime': 192.622, 'train_samples_per_second': 110.943, 'train_steps_per_second': 3.478, 'total_flos': 1054777482669504.0, 'train_loss': 3.8808603286743164, 'epoch': 5.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乱数シードを再設定\n",
    "set_seed(42)\n",
    "\n",
    "# Trainerに渡す引数を初期化する\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/output_bert_crf_ner\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Trainerを初期化\n",
    "trainer = Trainer(\n",
    "    model=model_crf,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# 学習\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction_crf(\n",
    "    dataloader: DataLoader,\n",
    "    model: PreTrainedModel,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"BERT-CRFモデルを用いてラベルを予測\"\"\"\n",
    "    predictions = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        inputs = {\n",
    "            k:v.to(model.device)\n",
    "            for k,v in batch.items()\n",
    "            if k != \"special_tokens_mask\"\n",
    "        }\n",
    "        # [CLS]以外の予測スコアを取得\n",
    "        logits = model(**inputs).logits.cpu()[:, 1:, :]\n",
    "        # [CLS]以外の特殊トークンのマスクを取得する\n",
    "        mask = (batch[\"special_tokens_mask\"]==0).cpu()[:, 1:]\n",
    "        # 訓練した遷移スコアを取得する\n",
    "        start_transitions = model_crf.crf.start_transitions.cpu()\n",
    "        transitions = model_crf.crf.transitions.cpu()\n",
    "        end_transitions = model_crf.crf.end_transitions.cpu()\n",
    "        # ビタビアルゴリズムを用いて最適なIDの系列を探索する\n",
    "        pred_label_ids = decode_with_viterbi(\n",
    "            logits,\n",
    "            mask,\n",
    "            start_transitions,\n",
    "            transitions,\n",
    "            end_transitions\n",
    "        )\n",
    "        # [CLS]のIDを0とする\n",
    "        cls_pred_label_id = torch.zeros(pred_label_ids.shape[0], 1)\n",
    "        # [CLS]のIDと探索したIDの系列を連結して予測ラベルとする\n",
    "        batch[\"pred_label_ids\"] = torch.concat(\n",
    "            [cls_pred_label_id, pred_label_ids], dim=1\n",
    "        )\n",
    "        batch = {k: v.cpu().tolist() for k,v in batch.items()}\n",
    "        # ミニバッチのデータを事例単位のlistに変換\n",
    "        predictions += convert_list_dict_to_dict_list(batch)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../model/output_bert_crf_ner/checkpoint-134/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"tohoku-nlp/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertWithCrfTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_crf_ner/checkpoint-134/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertWithCrfTokenClassification.\n",
      "\n",
      "All the weights of BertWithCrfTokenClassification were initialized from the model checkpoint at ../model/output_bert_crf_ner/checkpoint-134.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertWithCrfTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:01<00:00, 11.91it/s]\n",
      "loading configuration file ../model/output_bert_crf_ner/checkpoint-268/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"tohoku-nlp/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertWithCrfTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_crf_ner/checkpoint-268/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertWithCrfTokenClassification.\n",
      "\n",
      "All the weights of BertWithCrfTokenClassification were initialized from the model checkpoint at ../model/output_bert_crf_ner/checkpoint-268.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertWithCrfTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:01<00:00, 10.79it/s]\n",
      "loading configuration file ../model/output_bert_crf_ner/checkpoint-402/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"tohoku-nlp/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertWithCrfTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_crf_ner/checkpoint-402/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertWithCrfTokenClassification.\n",
      "\n",
      "All the weights of BertWithCrfTokenClassification were initialized from the model checkpoint at ../model/output_bert_crf_ner/checkpoint-402.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertWithCrfTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:01<00:00, 10.79it/s]\n",
      "loading configuration file ../model/output_bert_crf_ner/checkpoint-536/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"tohoku-nlp/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertWithCrfTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_crf_ner/checkpoint-536/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertWithCrfTokenClassification.\n",
      "\n",
      "All the weights of BertWithCrfTokenClassification were initialized from the model checkpoint at ../model/output_bert_crf_ner/checkpoint-536.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertWithCrfTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:01<00:00,  9.85it/s]\n",
      "loading configuration file ../model/output_bert_crf_ner/checkpoint-670/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"tohoku-nlp/bert-base-japanese-v3\",\n",
      "  \"architectures\": [\n",
      "    \"BertWithCrfTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"2\": \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\",\n",
      "    \"3\": \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"4\": \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\",\n",
      "    \"5\": \"B-\\u4eba\\u540d\",\n",
      "    \"6\": \"I-\\u4eba\\u540d\",\n",
      "    \"7\": \"B-\\u5730\\u540d\",\n",
      "    \"8\": \"I-\\u5730\\u540d\",\n",
      "    \"9\": \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"10\": \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\",\n",
      "    \"11\": \"B-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"12\": \"I-\\u65bd\\u8a2d\\u540d\",\n",
      "    \"13\": \"B-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"14\": \"I-\\u6cd5\\u4eba\\u540d\",\n",
      "    \"15\": \"B-\\u88fd\\u54c1\\u540d\",\n",
      "    \"16\": \"I-\\u88fd\\u54c1\\u540d\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 1,\n",
      "    \"B-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 3,\n",
      "    \"B-\\u4eba\\u540d\": 5,\n",
      "    \"B-\\u5730\\u540d\": 7,\n",
      "    \"B-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 9,\n",
      "    \"B-\\u65bd\\u8a2d\\u540d\": 11,\n",
      "    \"B-\\u6cd5\\u4eba\\u540d\": 13,\n",
      "    \"B-\\u88fd\\u54c1\\u540d\": 15,\n",
      "    \"I-\\u305d\\u306e\\u4ed6\\u306e\\u7d44\\u7e54\\u540d\": 2,\n",
      "    \"I-\\u30a4\\u30d9\\u30f3\\u30c8\\u540d\": 4,\n",
      "    \"I-\\u4eba\\u540d\": 6,\n",
      "    \"I-\\u5730\\u540d\": 8,\n",
      "    \"I-\\u653f\\u6cbb\\u7684\\u7d44\\u7e54\\u540d\": 10,\n",
      "    \"I-\\u65bd\\u8a2d\\u540d\": 12,\n",
      "    \"I-\\u6cd5\\u4eba\\u540d\": 14,\n",
      "    \"I-\\u88fd\\u54c1\\u540d\": 16,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file ../model/output_bert_crf_ner/checkpoint-670/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertWithCrfTokenClassification.\n",
      "\n",
      "All the weights of BertWithCrfTokenClassification were initialized from the model checkpoint at ../model/output_bert_crf_ner/checkpoint-670.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertWithCrfTokenClassification for predictions without further training.\n",
      "100%|██████████| 17/17 [00:01<00:00, 10.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Checkpointからモデル選択\n",
    "best_score = 0\n",
    "for checkpoint in sorted(glob(\"../model/output_bert_crf_ner/checkpoint-*\")):\n",
    "    # モデル読み込み\n",
    "    model_crf = BertWithCrfTokenClassification.from_pretrained(\n",
    "        checkpoint\n",
    "    )\n",
    "    model_crf = model_crf.to(\"cuda:0\")\n",
    "    # 固有表現ラベルを予測\n",
    "    predictions = run_prediction_crf(validation_dataloader, model_crf)\n",
    "    # 固有表現を抽出\n",
    "    results = extract_entities(\n",
    "        predictions, dataset[\"validation\"], tokenizer, id2label\n",
    "    )\n",
    "    # 正解データと予測データのラベルのlistを作成\n",
    "    true_labels, pred_labels = convert_results_to_labels(results)\n",
    "    # 評価スコアを算出\n",
    "    socres = compute_scores(true_labels, pred_labels, \"micro\")\n",
    "    if best_score < socres[\"f1-score\"]:\n",
    "        best_score = scores[\"f1-score\"]\n",
    "        best_model_crf = model_crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     その他の組織名       0.83      0.82      0.82       100\n",
      "       イベント名       0.88      0.95      0.91        93\n",
      "          人名       0.95      0.96      0.96       287\n",
      "          地名       0.88      0.87      0.87       204\n",
      "      政治的組織名       0.83      0.89      0.86       106\n",
      "         施設名       0.90      0.87      0.88       137\n",
      "         法人名       0.89      0.87      0.88       248\n",
      "         製品名       0.80      0.85      0.82       158\n",
      "\n",
      "   micro avg       0.88      0.89      0.89      1333\n",
      "   macro avg       0.87      0.88      0.88      1333\n",
      "weighted avg       0.88      0.89      0.89      1333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 固有表現ラベルを予測する\n",
    "predictions = run_prediction_crf(test_dataloader, best_model_crf)\n",
    "# 固有表現を抽出\n",
    "results = extract_entities(\n",
    "    predictions, dataset[\"test\"], tokenizer, id2label\n",
    ")\n",
    "# 正解データと予測データのラベルのlistを作成\n",
    "true_labels, pred_labels = convert_results_to_labels(results)\n",
    "# 評価結果\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
