{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å›ºæœ‰è¡¨ç¾èªè­˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å›ºæœ‰è¡¨ç¾èªè­˜ã¨ã¯ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy-alignments seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default\n",
      "Reusing dataset ner-wikipedia-dataset (/root/.cache/huggingface/datasets/llm-book___ner-wikipedia-dataset/default/0.0.0/184bcf9be66116e777f2f534436226d47348676c93ba20cca58933f1b2b3b782)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 273.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
    "dataset = load_dataset(\"llm-book/ner-wikipedia-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 4274\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 534\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['curid', 'text', 'entities'],\n",
      "        num_rows: 535\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢å¼ã¨äº‹ä¾‹æ•°ã‚’ç¢ºèªã™ã‚‹\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'curid': '3638038',\n",
      "  'entities': [{'name': 'ã•ãã‚‰å­¦é™¢', 'span': [0, 5], 'type': 'ãã®ä»–ã®çµ„ç¹”å'},\n",
      "               {'name': 'Ciao Smiles', 'span': [6, 17], 'type': 'ãã®ä»–ã®çµ„ç¹”å'}],\n",
      "  'text': 'ã•ãã‚‰å­¦é™¢ã€Ciao Smilesã®ãƒ¡ãƒ³ãƒãƒ¼ã€‚'},\n",
      " {'curid': '1729527',\n",
      "  'entities': [{'name': 'ãƒ¬ã‚¯ãƒ¬ã‚¢ãƒ†ã‚£ãƒ¼ãƒœãƒ»ã‚¦ã‚§ãƒ«ãƒ', 'span': [17, 30], 'type': 'ãã®ä»–ã®çµ„ç¹”å'},\n",
      "               {'name': 'ãƒ—ãƒªãƒ¡ãƒ¼ãƒ©ãƒ»ãƒ‡ã‚£ãƒ“ã‚·ã‚ªãƒ³', 'span': [32, 44], 'type': 'ãã®ä»–ã®çµ„ç¹”å'}],\n",
      "  'text': '2008å¹´10æœˆ5æ—¥ã€ã‚¢ã‚¦ã‚§ãƒ¼ã§ã®ãƒ¬ã‚¯ãƒ¬ã‚¢ãƒ†ã‚£ãƒ¼ãƒœãƒ»ã‚¦ã‚§ãƒ«ãƒæˆ¦ã§ãƒ—ãƒªãƒ¡ãƒ¼ãƒ©ãƒ»ãƒ‡ã‚£ãƒ“ã‚·ã‚ªãƒ³ã§ã®åˆå¾—ç‚¹ã‚’æ±ºã‚ãŸã€‚'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# è¨“ç·´ã‚»ãƒƒãƒˆã®æœ€åˆã®äºŒã¤ã®äº‹ä¾‹ã‚’è¡¨ç¤ºã™ã‚‹\n",
    "pprint(list(dataset[\"train\"])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>äººå</th>\n",
       "      <td>2394</td>\n",
       "      <td>299</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æ³•äººå</th>\n",
       "      <td>2006</td>\n",
       "      <td>231</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>åœ°å</th>\n",
       "      <td>1769</td>\n",
       "      <td>184</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æ”¿æ²»çš„çµ„ç¹”å</th>\n",
       "      <td>953</td>\n",
       "      <td>121</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>è£½å“å</th>\n",
       "      <td>934</td>\n",
       "      <td>123</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æ–½è¨­å</th>\n",
       "      <td>868</td>\n",
       "      <td>103</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ãã®ä»–ã®çµ„ç¹”å</th>\n",
       "      <td>852</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ã‚¤ãƒ™ãƒ³ãƒˆå</th>\n",
       "      <td>831</td>\n",
       "      <td>85</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>åˆè¨ˆ</th>\n",
       "      <td>10607</td>\n",
       "      <td>1245</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train  validation  test\n",
       "äººå        2394         299   287\n",
       "æ³•äººå       2006         231   248\n",
       "åœ°å        1769         184   204\n",
       "æ”¿æ²»çš„çµ„ç¹”å     953         121   106\n",
       "è£½å“å        934         123   158\n",
       "æ–½è¨­å        868         103   137\n",
       "ãã®ä»–ã®çµ„ç¹”å    852          99   100\n",
       "ã‚¤ãƒ™ãƒ³ãƒˆå      831          85    93\n",
       "åˆè¨ˆ       10607        1245  1333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def count_label_occurrences(dataset: Dataset) -> dict[str, int]:\n",
    "    \"\"\"å›ºæœ‰è¡¨ç¾ã‚¿ã‚¤ãƒ—ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\"\"\"\n",
    "    # å„äº‹ä¾‹ã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡ºã—ãŸlistã‚’ä½œæˆã™ã‚‹\n",
    "    entities = [\n",
    "        e[\"type\"] for data in dataset for e in data[\"entities\"]\n",
    "    ]\n",
    "    \n",
    "    # ãƒ©ãƒ™ãƒ«ã®è¡¨ç¾å›æ•°ãŒå¤šã„é †ã«ä¸¦ã¹ã‚‹\n",
    "    label_counts = dict(Counter(entities).most_common())\n",
    "    return label_counts\n",
    "    \n",
    "label_counts_dict = {}\n",
    "for split in dataset: # å„åˆ†å‰²ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "    label_counts_dict[split] = count_label_occurrences(dataset[split])\n",
    "# DataFrameå½¢å¼ã§è¡¨ç¤ºã™ã‚‹\n",
    "df = pd.DataFrame(label_counts_dict)\n",
    "df.loc[\"åˆè¨ˆ\"] = df.sum()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ã‚¹ãƒ‘ãƒ³ã®é‡ãªã‚‹å›ºæœ‰è¡¨ç¾ã®å­˜åœ¨ã‚’åˆ¤å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ³ãŒé‡è¤‡ã™ã‚‹äº‹ä¾‹æ•°:0\n",
      "validationã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ³ãŒé‡è¤‡ã™ã‚‹äº‹ä¾‹æ•°:0\n",
      "testã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ³ãŒé‡è¤‡ã™ã‚‹äº‹ä¾‹æ•°:0\n"
     ]
    }
   ],
   "source": [
    "def has_overlap(spans: list[tuple[int, int]]) -> int:\n",
    "    \"\"\"ã‚¹ãƒ‘ãƒ³ã®é‡ãªã‚‹å›ºæœ‰è¡¨ç¾ã®å­˜åœ¨ã‚’åˆ¤å®š\"\"\"\n",
    "    sorted_spans = sorted(spans, key=lambda x: x[0])\n",
    "    for i in range(1, len(sorted_spans)):\n",
    "        # å‰ã®ã‚¹ãƒ‘ãƒ³ã®çµ‚äº†ä½ç½®ãŒç¾åœ¨ã®ã‚¹ãƒ‘ãƒ³ã®é–‹å§‹ä½ç½®ã‚ˆã‚Šå¤§ãã„å ´åˆã€\n",
    "        # é‡ãªã£ã¦ã„ã‚‹ã¨ã™ã‚‹\n",
    "        if sorted_spans[i - 1][1] > sorted_spans[i][0]:\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "# å„åˆ†å‰²ã‚»ãƒƒãƒˆã§ã‚¹ãƒ‘ãƒ³ã®é‡ãªã‚‹å›ºæœ‰è¡¨ç¾ãŒã‚ã‚‹äº‹ä¾‹æ•°ã‚’æ•°ãˆã‚‹\n",
    "overlap_count = 0\n",
    "for split in dataset: # å„åˆ†å‰²ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "    for data in dataset[split]: # å„äº‹ä¾‹ã‚’å‡¦ç†ã™ã‚‹\n",
    "        if data[\"entities\"]: # å›ºæœ‰è¡¨ç¾ã®å­˜åœ¨ã—ãªã„äº‹ä¾‹ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "            # ã‚¹ãƒ‘ãƒ³ã®ã¿ã®listã‚’ä½œæˆã™ã‚‹\n",
    "            spans = [e[\"span\"] for e in data[\"entities\"]]\n",
    "            overlap_count += has_overlap(spans)\n",
    "    print(f\"{split}ã«ãŠã‘ã‚‹ã‚¹ãƒ‘ãƒ³ãŒé‡è¤‡ã™ã‚‹äº‹ä¾‹æ•°:{overlap_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£è¦åŒ–å‰: ï¼¡ï¼¢ï¼£ABCï½ï½‚ï½ƒabcï½±ï½²ï½³ã‚¢ã‚¤ã‚¦â‘ â‘¡â‘¢123\n",
      "æ­£è¦åŒ–å¾Œ: ABCABCabcabcã‚¢ã‚¤ã‚¦ã‚¢ã‚¤ã‚¦123123\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦Unicodeæ­£è¦åŒ–ã‚’è¡Œã†\n",
    "text =  \"ï¼¡ï¼¢ï¼£ABCï½ï½‚ï½ƒabcï½±ï½²ï½³ã‚¢ã‚¤ã‚¦â‘ â‘¡â‘¢123\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"æ­£è¦åŒ–å‰: {text}\")\n",
    "print(f\"æ­£è¦åŒ–å¾Œ: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£è¦åŒ–å‰: ãˆ±ã€3ãŒ•ã€10â„ƒ\n",
      "æ­£è¦åŒ–å¾Œ: (æ ª)ã€3ã‚­ãƒ­ã‚°ãƒ©ãƒ ã€10Â°C\n"
     ]
    }
   ],
   "source": [
    "# æ–‡å­—åˆ—ã®é•·ã•ãŒå¤‰ã‚ã‚‹å ´åˆã‚ã‚‹\n",
    "text = \"ãˆ±ã€3ãŒ•ã€10â„ƒ\"\n",
    "normalized_text = normalize(\"NFKC\", text)\n",
    "print(f\"æ­£è¦åŒ–å‰: {text}\")\n",
    "print(f\"æ­£è¦åŒ–å¾Œ: {normalized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£è¦åŒ–ã•ã‚Œã¦ã„ãªã„äº‹ä¾‹æ•°: 0\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize, is_normalized\n",
    "\n",
    "count = 0\n",
    "for split in dataset: # å„åˆ†å‰²ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã™ã‚‹\n",
    "    for data in dataset[split]: # å„äº‹ä¾‹ã‚’å‡¦ç†ã™ã‚‹\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆãŒæ­£è¦åŒ–ã•ã‚Œã¦ã„ãªã„äº‹ä¾‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹\n",
    "        if not is_normalized(\"NFKC\", data[\"text\"]):\n",
    "            count += 1\n",
    "print(f\"æ­£è¦åŒ–ã•ã‚Œã¦ã„ãªã„äº‹ä¾‹æ•°: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½: ã•ãã‚‰/å­¦é™¢/ã€/C/##ia/##o/Sm/##ile/##s/ã®/ãƒ¡ãƒ³ãƒãƒ¼/ã€‚\n",
      "æ–‡å˜ä½: ã•/ã/ã‚‰/å­¦/é™¢/ã€/C/i/a/o/ /S/m/i/l/e/s/ã®/ãƒ¡/ãƒ³/ãƒ/ãƒ¼/ã€‚\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’èª­ã¿è¾¼ã¿\n",
    "model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†\n",
    "subwords = \"/\".join(tokenizer.tokenize(dataset[\"train\"][0][\"text\"]))\n",
    "characters = \"/\".join(dataset[\"train\"][0][\"text\"])\n",
    "print(f\"ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½: {subwords}\")\n",
    "print(f\"æ–‡å˜ä½: {characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ–‡å­—åˆ—ã¨ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ã•ãã‚‰å­¦é™¢\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡å­—ã®list: ['ã•', 'ã', 'ã‚‰', 'å­¦', 'é™¢']\n",
      "ãƒˆãƒ¼ã‚¯ãƒ³ã®list: ['[CLS]', 'ã•ãã‚‰', 'å­¦é™¢', '[SEP]']\n",
      "æ–‡å­—ã«å¯¾ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®: [[1], [1], [1], [2], [2]]\n",
      "ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹æ–‡å­—ã®ä½ç½®: [[], [0, 1, 2], [3, 4], []]\n"
     ]
    }
   ],
   "source": [
    "from spacy_alignments.tokenizations import get_alignments\n",
    "\n",
    "# æ–‡å­—åˆ—ã®listã‚’ç²å¾—ã™ã‚‹\n",
    "characters = list(text)\n",
    "# ãƒ†ã‚­ã‚¹ãƒˆã‚’ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚ãŸãƒˆãƒ¼ã‚¯ãƒ³ã®listã«å¤‰æ›ã™ã‚‹\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "# æ–‡å­—ã®listã¨ãƒˆãƒ¼ã‚¯ãƒ³ã®listã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ã¨ã‚‹\n",
    "char_to_token_indices, token_to_char_indices = get_alignments(characters, tokens)\n",
    "print(f\"æ–‡å­—ã®list: {characters}\")\n",
    "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã®list: {tokens}\")\n",
    "print(f\"æ–‡å­—ã«å¯¾ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®: {char_to_token_indices}\")\n",
    "print(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹æ–‡å­—ã®ä½ç½®: {token_to_char_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ç³»åˆ—ãƒ©ãƒ™ãƒªãƒ³ã‚°ã®ãŸã‚ã®ãƒ©ãƒ™ãƒ«ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"å¤§è°·ç¿”å¹³ã¯å²©æ‰‹çœŒæ°´æ²¢å¸‚å‡ºèº«\"\n",
    "entities = [\n",
    "    {\"name\": \"å¤§è°·ç¿”å¹³\", \"span\": [0,4], \"type\": \"äººå\"},\n",
    "    {\"name\": \"å²©æ‰‹çœŒæ°´æ²¢å¸‚\", \"span\": [5,11], \"type\": \"åœ°å\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ä½ç½®</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ãƒˆãƒ¼ã‚¯ãƒ³åˆ—</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>å¤§è°·</td>\n",
       "      <td>ç¿”</td>\n",
       "      <td>##å¹³</td>\n",
       "      <td>ã¯</td>\n",
       "      <td>å²©æ‰‹</td>\n",
       "      <td>çœŒ</td>\n",
       "      <td>æ°´æ²¢</td>\n",
       "      <td>å¸‚</td>\n",
       "      <td>å‡ºèº«</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ãƒ©ãƒ™ãƒ«åˆ—</th>\n",
       "      <td>-</td>\n",
       "      <td>B-äººå</td>\n",
       "      <td>I-äººå</td>\n",
       "      <td>I-äººå</td>\n",
       "      <td>O</td>\n",
       "      <td>B-åœ°å</td>\n",
       "      <td>I-åœ°å</td>\n",
       "      <td>I-åœ°å</td>\n",
       "      <td>I-åœ°å</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ä½ç½®        0     1     2     3  4     5     6     7     8   9      10\n",
       "ãƒˆãƒ¼ã‚¯ãƒ³åˆ—  [CLS]    å¤§è°·     ç¿”   ##å¹³  ã¯    å²©æ‰‹     çœŒ    æ°´æ²¢     å¸‚  å‡ºèº«  [SEP]\n",
       "ãƒ©ãƒ™ãƒ«åˆ—       -  B-äººå  I-äººå  I-äººå  O  B-åœ°å  I-åœ°å  I-åœ°å  I-åœ°å   O      -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def output_tokens_and_labels(\n",
    "    text: str,\n",
    "    entities: list[dict[str, list[int] | str]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³ã®listã¨ãƒ©ãƒ™ãƒ«ã®listã‚’å‡ºåŠ›\"\"\"\n",
    "    # æ–‡å­—åˆ—ã®listã¨ãƒˆãƒ¼ã‚¯ãƒ³ã®listã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ã¨ã‚‹\n",
    "    characters = list(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "    \n",
    "    # \"O\"ã®ãƒ©ãƒ™ãƒ«ã§åˆæœŸåŒ–ã—ãŸãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆã™ã‚‹\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for entity in entities: # å„å›ºæœ‰è¡¨ç¾ã§å‡¦ç†ã™ã‚‹\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        start = char_to_token_indices[entity_span[0]][0]\n",
    "        end = char_to_token_indices[entity_span[1]-1][0]\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã«\"B-\"ã®ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã™ã‚‹\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å¤–ã®ä½ç½®ã«\"I-\"ã®ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã™ã‚‹\n",
    "        for idx in range(start + 1, end + 1):\n",
    "            labels[idx] = f\"I-{entity_type}\"\n",
    "    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã«ã¯ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã—ãªã„\n",
    "    labels[0] = \"-\" # é–‹å§‹\n",
    "    labels[-1] = \"-\" # çµ‚äº†\n",
    "    return tokens, labels\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒ©ãƒ™ãƒ«ã®listã‚’å‡ºåŠ›ã™ã‚‹\n",
    "tokens, labels = output_tokens_and_labels(text, entities, tokenizer)\n",
    "# DataFrameã®å½¢å¼ã§è¡¨ç¤ºã™ã‚‹\n",
    "df = pd.DataFrame({\"ãƒˆãƒ¼ã‚¯ãƒ³åˆ—\": tokens, \"ãƒ©ãƒ™ãƒ«åˆ—\": labels})\n",
    "df.index.name = \"ä½ç½®\"\n",
    "display(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è©•ä¾¡æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seqevalãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ãŸè©•ä¾¡ã‚¹ã‚³ã‚¢ã®ç®—å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "def create_character_labels(\n",
    "    text: str, entities: list[dict[str, list[int] | str]]\n",
    ") -> list[str]:\n",
    "    \"\"\"æ–‡å­—ãƒ™ãƒ¼ã‚¹ã§ãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆ\"\"\"\n",
    "    # \"O\"ã®ãƒ©ãƒ™ãƒ«ã§åˆæœŸåŒ–ã—ãŸãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆã™ã‚‹\n",
    "    labels = [\"O\"] * len(text)\n",
    "    for entity in entities: # å„å›ºæœ‰è¡¨ç¾ã‚’å‡¦ç†ã™ã‚‹\n",
    "        entity_span, entity_type = entity[\"span\"], entity[\"type\"]\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹æ–‡å­—ã®ä½ç½®ã«\"B-\"ã®ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã™ã‚‹\n",
    "        labels[entity_span[0]] = f\"B-{entity_type}\"\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹æ–‡å­—ä»¥å¤–ã®ä½ç½®ã«\"I-\"ãƒ©ãƒ™ãƒ«ã‚’è¨­å®šã™ã‚‹\n",
    "        for i in range(entity_span[0] + 1, entity_span[1]):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "    \n",
    "def convert_results_to_labels(\n",
    "    results: list[dict[str, Any]]\n",
    ") -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆ\"\"\"\n",
    "    true_labels, pred_labels = [], []\n",
    "    for result in results: # å„äº‹ä¾‹ã‚’å‡¦ç†ã™ã‚‹\n",
    "        # æ–‡å­—ãƒ™ãƒ¼ã‚¹ã§ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦listã«åŠ ãˆã‚‹\n",
    "        true_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"entities\"])\n",
    "        )\n",
    "        pred_labels.append(\n",
    "            create_character_labels(result[\"text\"], result[\"pred_entities\"])\n",
    "        )\n",
    "    return true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          äººå       1.00      1.00      1.00         1\n",
      "          åœ°å       0.00      0.00      0.00         1\n",
      "         æ–½è¨­å       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.33      0.50      0.40         2\n",
      "   macro avg       0.33      0.33      0.33         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = [\n",
    "    {\n",
    "        \"text\": \"å¤§è°·ç¿”å¹³ã¯å²©æ‰‹çœŒæ°´æ²¢å¸‚å‡ºèº«\",\n",
    "        \"entities\": [\n",
    "            {\"name\": \"å¤§è°·ç¿”å¹³\", \"span\": [0,4], \"type\": \"äººå\"},\n",
    "            {\"name\": \"å²©æ‰‹çœŒæ°´æ²¢å¸‚\", \"span\": [5, 11], \"type\": \"åœ°å\"}\n",
    "        ],\n",
    "        \"pred_entities\": [\n",
    "            {\"name\": \"å¤§è°·ç¿”å¹³\", \"span\": [0,4], \"type\": \"äººå\"},\n",
    "            {\"name\": \"å²©æ‰‹çœŒ\", \"span\": [5,8], \"type\": \"åœ°å\"},\n",
    "            {\"name\": \"æ°´æ²¢å¸‚\", \"span\": [8,11], \"type\": \"æ–½è¨­å\"}\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ã®listã‚’ä½œæˆ\n",
    "true_labels, pred_labels = convert_results_to_labels(results)\n",
    "# è©•ä¾¡çµæœã‚’å–å¾—ã—ã¦è¡¨ç¤º\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.3333333333333333, 'recall': 0.5, 'f1-score': 0.4}\n",
      "{'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_scores(\n",
    "    true_labels: list[list[str]], pred_labels: list[list[str]], average: str\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"é©åˆç‡ã€å†ç¾ç‡ã€Få€¤ã‚’ç®—å‡º\"\"\"\n",
    "    scores = {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average=average),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average=average),\n",
    "        \"f1-score\": f1_score(true_labels, pred_labels, average=average),\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# é©åˆç‡ã€å†ç¾ç‡ã€Få€¤ã®ãƒã‚¤ã‚¯ãƒ­å¹³å‡ã‚’ç®—å‡ºã™ã‚‹\n",
    "print(compute_scores(true_labels, pred_labels, \"micro\"))\n",
    "# é©åˆç‡ã€å†ç¾ç‡ã€Få€¤ã®ãƒã‚¯ãƒ­å¹³å‡ã‚’ç®—å‡ºã™ã‚‹\n",
    "print(compute_scores(true_labels, pred_labels, \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å›ºæœ‰è¡¨ç¾èªè­˜ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O',\n",
      " 1: 'B-ãã®ä»–ã®çµ„ç¹”å',\n",
      " 2: 'I-ãã®ä»–ã®çµ„ç¹”å',\n",
      " 3: 'B-ã‚¤ãƒ™ãƒ³ãƒˆå',\n",
      " 4: 'I-ã‚¤ãƒ™ãƒ³ãƒˆå',\n",
      " 5: 'B-äººå',\n",
      " 6: 'I-äººå',\n",
      " 7: 'B-åœ°å',\n",
      " 8: 'I-åœ°å',\n",
      " 9: 'B-æ”¿æ²»çš„çµ„ç¹”å',\n",
      " 10: 'I-æ”¿æ²»çš„çµ„ç¹”å',\n",
      " 11: 'B-æ–½è¨­å',\n",
      " 12: 'I-æ–½è¨­å',\n",
      " 13: 'B-æ³•äººå',\n",
      " 14: 'I-æ³•äººå',\n",
      " 15: 'B-è£½å“å',\n",
      " 16: 'I-è£½å“å'}\n"
     ]
    }
   ],
   "source": [
    "# ãƒ©ãƒ™ãƒ«ã¨IDã‚’å¯¾å¿œä»˜ã‘ã‚‹dictã®ä½œæˆ\n",
    "import torch\n",
    "\n",
    "def create_label2id(\n",
    "    entities_list: list[list[dict[str, str | int]]]\n",
    ") -> dict[str, int]:\n",
    "    \"\"\"ãƒ©ãƒ™ãƒ«ã¨IDã‚’ç´ã¥ã‘ã‚‹dictã‚’ä½œæˆ\"\"\"\n",
    "    # \"O\"ã®IDã«ã¯0ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "    label2id = {\"O\": 0}\n",
    "    # å›ºæœ‰è¡¨ç¾ã‚¿ã‚¤ãƒ—ã®setã‚’ç²å¾—ã—ã¦ä¸¦ã³æ›¿ãˆã‚‹\n",
    "    entity_types = set(\n",
    "        [e[\"type\"] for entities in entities_list for e in entities]\n",
    "    )\n",
    "    entity_types = sorted(entity_types)\n",
    "    for i, entity_type in enumerate(entity_types):\n",
    "        # \"B-\"ã®IDã«ã¯å¥‡æ•°ç•ªå·ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "        label2id[f\"B-{entity_type}\"] = i * 2 + 1\n",
    "        # \"I-\"ã®IDã«ã¯å¶æ•°ç•ªå·ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "        label2id[f\"I-{entity_type}\"] = i * 2 + 2\n",
    "    return label2id\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã¨IDã‚’ç´ã¥ã‘ã‚‹dictã‚’ä½œæˆã™ã‚‹\n",
    "label2id = create_label2id(dataset[\"train\"][\"entities\"])\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "pprint(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4274/4274 [00:03<00:00, 1155.79ex/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [00:00<00:00, 1397.92ex/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "def preprocess_data(\n",
    "    data: dict[str, Any],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    label2id: dict[int, str],\n",
    ") -> BatchEncoding:\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\"\"\"\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†\n",
    "    inputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "    \n",
    "    # æ–‡å­—ã®listã¨ãƒˆãƒ¼ã‚¯ãƒ³ã®listã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ã¨ã‚‹\n",
    "    characters = list(data[\"text\"])\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "    char_to_token_indices, _ = get_alignments(characters, tokens)\n",
    "    \n",
    "    # \"O\"ã®IDã®listã‚’ä½œæˆã™ã‚‹\n",
    "    labels = torch.zeros_like(inputs[\"input_ids\"])\n",
    "    for entity in data[\"entities\"]: # å„å›ºæœ‰è¡¨ç¾ã‚’å‡¦ç†ã™ã‚‹\n",
    "        start_token_indices = char_to_token_indices[entity[\"span\"][0]]\n",
    "        end_token_indices = char_to_token_indices[\n",
    "            entity[\"span\"][1] - 1\n",
    "        ]\n",
    "        # æ–‡å­—ã«å¯¾ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹\n",
    "        if (\n",
    "            len(start_token_indices)==0\n",
    "            or len(end_token_indices)==0\n",
    "        ):\n",
    "            continue\n",
    "        start, end = start_token_indices[0], end_token_indices[0]\n",
    "        entity_type = entity[\"type\"]\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã«\"B-\"ã®IDã‚’è¨­å®šã™ã‚‹\n",
    "        labels[start] = label2id[f\"B-{entity_type}\"]\n",
    "        # å›ºæœ‰è¡¨ç¾ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å¤–ã®ä½ç½®ã«\"I-\"ã®IDã‚’è¨­å®šã™ã‚‹\n",
    "        if start != end:\n",
    "            labels[start + 1: end + 1] = label2id[f\"I-{entity_type}\"]\n",
    "    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã®IDã¯-100ã¨ã™ã‚‹\n",
    "    labels[torch.where(inputs[\"special_tokens_mask\"])] = -100\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "    \n",
    "# è¨“ç·´ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’è¡Œã†\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "# æ¤œè¨¼ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’è¡Œã†\n",
    "validation_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_data,\n",
    "    fn_kwargs={\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"label2id\": label2id,\n",
    "    },\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:803: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, label2id=label2id, id2label=id2label\n",
    ")\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªä¸Šã«éš£æ¥ã™ã‚‹å½¢ã§é…ç½®\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.contiguous()\n",
    "# collateé–¢æ•°ã«DataCollatorForTokenClassificationã‚’ç”¨ã„ã‚‹\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1678/359011269.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 01:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.098389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.091702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.091689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.100583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.104642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=670, training_loss=0.15430139712433316, metrics={'train_runtime': 102.5866, 'train_samples_per_second': 208.312, 'train_steps_per_second': 6.531, 'total_flos': 1070012411245680.0, 'train_loss': 0.15430139712433316, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "# ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’42ã«å›ºå®šã™ã‚‹\n",
    "set_seed(42)\n",
    "\n",
    "# Trainerã«æ¸¡ã™å¼•æ•°ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/output_bert_ner\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Trainerã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# è¨“ç·´ã™ã‚‹\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å›ºæœ‰è¡¨ç¾ã®äºˆæ¸¬ãƒ»æŠ½å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…¥åŠ›: {'input_ids': [[0, 1], [2, 3]], 'labels': [[1, 2], [3, 4]]}\n",
      "å‡ºåŠ›: [{'input_ids': [0, 1], 'labels': [1, 2]}, {'input_ids': [2, 3], 'labels': [3, 4]}]\n"
     ]
    }
   ],
   "source": [
    "def convert_list_dict_to_dict_list(\n",
    "    list_dict: dict[str, list]\n",
    ") -> list[dict[str, list]]:\n",
    "    \"\"\"ãƒŸãƒ‹ãƒãƒƒãƒã®ãƒ‡ãƒ¼ã‚¿ã‚’äº‹ä¾‹å˜ä½ã®listã«å¤‰æ›\"\"\"\n",
    "    dict_list = []\n",
    "    # dictã®ã‚­ãƒ¼ã®listã‚’ä½œæˆã™ã‚‹\n",
    "    keys = list(list_dict.keys())\n",
    "    for idx in range(len(list_dict[keys[0]])): # å„äº‹ä¾‹ã§å‡¦ç†ã™ã‚‹\n",
    "        # dictã®å„ã‚­ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã—ã¦listã«è¿½åŠ ã™ã‚‹\n",
    "        dict_list.append({key: list_dict[key][idx] for key in keys})\n",
    "    return dict_list\n",
    "\n",
    "# ãƒŸãƒ‹ãƒãƒƒãƒã®ãƒ‡ãƒ¼ã‚¿ã‚’äº‹ä¾‹å˜ä½ã®listã«å¤‰æ›ã™ã‚‹\n",
    "list_dict = {\n",
    "    \"input_ids\": [[0, 1], [2, 3]],\n",
    "    \"labels\": [[1, 2], [3, 4]],\n",
    "}\n",
    "dict_list = convert_list_dict_to_dict_list(list_dict)\n",
    "print(f\"å…¥åŠ›: {list_dict}\")\n",
    "print(f\"å‡ºåŠ›: {dict_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 39.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 15, 16, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 13, 14, 14, 14, 13, 0, 0, 0, 0, 0, 0, 15, 16, 15, 13, 14, 14, 14, 14, 13, 13, 13, 14, 14, 0, 0, 0, 13, 13, 14, 14, 14, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 15, 16, 16, 0, 13, 14, 14, 14, 14, 15, 0, 0, 15, 15, 15, 16, 16, 0, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "def run_prediction(\n",
    "    dataloader: DataLoader, \n",
    "    model: PreTrainedModel\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"äºˆæ¸¬ã‚¹ã‚³ã‚¢ã«åŸºã¥ãå›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬\"\"\"\n",
    "    predictions = []\n",
    "    for batch in tqdm(dataloader): # å„ãƒŸãƒ‹ãƒãƒƒãƒã‚’å‡¦ç†ã™ã‚‹\n",
    "        inputs = {\n",
    "            k: v.to(model.device)\n",
    "            for k, v in batch.items()\n",
    "            if k != \"special_tokens_mask\"\n",
    "        }\n",
    "        # äºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’å–å¾—ã™ã‚‹\n",
    "        logits = model(**inputs).logits\n",
    "        # æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„IDã‚’å–å¾—ã™ã‚‹\n",
    "        batch[\"pred_label_ids\"] = logits.argmax(-1)\n",
    "        batch = {k: v.cpu().tolist() for k, v in batch.items()}\n",
    "        # ãƒŸãƒ‹ãƒãƒƒãƒã®ãƒ‡ãƒ¼ã‚¿ã‚’äº‹ä¾‹å˜ä½ã®listã«å¤‰æ›ã™ã‚‹\n",
    "        predictions += convert_list_dict_to_dict_list(batch)\n",
    "    return predictions\n",
    "\n",
    "# ãƒŸãƒ‹ãƒãƒƒãƒã®ä½œæˆã«DataLoaderã‚’ç”¨ã„ã‚‹\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "# å›ºæœ‰è¡¨ç¾ãƒ©ãƒ™ãƒ«ã‚’äºˆæ¸¬ã™ã‚‹\n",
    "predictions = run_prediction(validation_dataloader, model)\n",
    "print(predictions[0][\"pred_label_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å›ºæœ‰è¡¨ç¾ã®æŠ½å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curid': '1662110',\n",
      " 'entities': [{'name': 'å¾©æ´»ç¯‡', 'span': [1, 4], 'type': 'è£½å“å'},\n",
      "              {'name': 'ã‚°ãƒªãƒ¼ãƒ³ãƒãƒ‹ãƒ¼', 'span': [6, 13], 'type': 'æ³•äººå'}],\n",
      " 'pred_entities': [{'name': 'å¾©æ´»ç¯‡', 'span': [1, 4], 'type': 'è£½å“å'},\n",
      "                   {'name': 'ã‚°ãƒªãƒ¼ãƒ³ãƒãƒ‹ãƒ¼', 'span': [6, 13], 'type': 'æ³•äººå'}],\n",
      " 'text': 'ã€Œå¾©æ´»ç¯‡ã€ã¯ã‚°ãƒªãƒ¼ãƒ³ãƒãƒ‹ãƒ¼ã‹ã‚‰ã®ç™ºå£²ã¨ãªã£ã¦ã„ã‚‹ã€‚'}\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "\n",
    "def extract_entities(\n",
    "    predictions: list[dict[str, Any]],\n",
    "    dataset: list[dict[str, Any]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    id2label: dict[int, str],\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡º\"\"\"\n",
    "    results = []\n",
    "    for prediction, data in zip(predictions, dataset):\n",
    "        # æ–‡å­—åˆ—ã®listã‚’å–å¾—ã™ã‚‹\n",
    "        characters = list(data[\"text\"])\n",
    "        \n",
    "        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã„ãŸãƒˆãƒ¼ã‚¯ãƒ³ã®listã¨äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã®listã‚’å–å¾—ã™ã‚‹\n",
    "        tokens, pred_labels = [], []\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(\n",
    "            prediction[\"input_ids\"]\n",
    "        )\n",
    "        for token, label_id in zip(\n",
    "            all_tokens, prediction[\"pred_label_ids\"]\n",
    "        ):\n",
    "            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»¥å¤–ã‚’listã«è¿½åŠ ã™ã‚‹\n",
    "            if token not in tokenizer.all_special_tokens:\n",
    "                tokens.append(token)\n",
    "                pred_labels.append(id2label[label_id])\n",
    "                \n",
    "        # æ–‡å­—ã®listã¨ãƒˆãƒ¼ã‚¯ãƒ³ã®listã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’å–ã‚‹\n",
    "        _, token_to_char_indices = get_alignments(characters, tokens)\n",
    "        \n",
    "        # äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã®listã‹ã‚‰å›ºæœ‰è¡¨ç¾ã‚¿ã‚¤ãƒ—ã¨ã€\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã®é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã‚’å–å¾—ã—ã¦ã€\n",
    "        # ãã‚Œã‚‰ã‚’æ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜å½¢å¼ã«ã™ã‚‹\n",
    "        pred_entities = []\n",
    "        for entity in get_entities(pred_labels):\n",
    "            entity_type, token_start, token_end = entity\n",
    "            # æ–‡å­—å˜ä½ã®é–‹å§‹ä½ç½®ã‚’å–å¾—ã™ã‚‹\n",
    "            char_start = token_to_char_indices[token_start][0]\n",
    "            # æ–‡å­—å˜ä½ã®çµ‚äº†ä½ç½®ã‚’å–å¾—ã™ã‚‹\n",
    "            char_end = token_to_char_indices[token_end][-1] + 1\n",
    "            pred_entity = {\n",
    "                \"name\": \"\".join(characters[char_start:char_end]),\n",
    "                \"span\": [char_start, char_end],\n",
    "                \"type\": entity_type,\n",
    "            }\n",
    "            pred_entities.append(pred_entity)\n",
    "        data[\"pred_entities\"] = pred_entities\n",
    "        results.append(data)\n",
    "    return results\n",
    "\n",
    "# å›ºæœ‰è¡¨ç¾ã‚’æŠ½å‡ºã™ã‚‹\n",
    "results = extract_entities(\n",
    "    predictions, dataset[\"validation\"], tokenizer, id2label\n",
    ")\n",
    "pprint(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
